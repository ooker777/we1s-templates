{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize JSON Documents\n",
    "\n",
    "This notebook will generate tokens counts for each of your documents and save them to a `bag_of_words` field in each document. This can speed up processing for downstream tasks.\n",
    "\n",
    "### INFO\n",
    "\n",
    "__author__    = 'Scott Kleinman'  \n",
    "__copyright__ = 'copyright 2020, The WE1S Project'  \n",
    "__license__   = 'MIT'  \n",
    "__version__   = '2.0'  \n",
    "__email__     = 'scott.kleinman@csun.edu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "If your data already has a `features` table and you would like a `bag_of_words` field to be generated, set `bagify_features=True`. For more information on features tables, see the <a href=\"README.md\" target=\"_blank\">README</a> file.\n",
    "\n",
    "The default tokenization method strips all non-alphanumeric characters and splits the text into tokens on white space. If you would like to use the WE1S tokenizer, set `method='we1s'`. Note that this method takes longer. The WE1S tokenizer leverages <a href=\"https://spacy.io/\" target=\"_blank\">spaCy</a> and its the language models. The default language model is `en_core_web_sm`, but this can be changed. However, you will have to download another model into your environment. See the <a href=\"README.md\" target=\"_blank\">README</a> file for instructions.\n",
    "\n",
    "Errors will be logged to the path you set for the `log_file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagify_features  = True\n",
    "method           = 'default'\n",
    "language_model   = 'en_core_web_sm'\n",
    "log_file         = 'tokenizer_log.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Get path to project_dir\n",
    "current_dir            = %pwd\n",
    "project_dir            = str(Path(current_dir).parent.parent)\n",
    "json_dir               = project_dir + '/project_data/json'\n",
    "tokenizer_script_path  = 'scripts/import_tokenizer.py'\n",
    "\n",
    "# Run the tokenization script\n",
    "%run {tokenizer_script_path}\n",
    "tokenizer = ImportTokenizer(json_dir, language_model='en_core_web_sm',\n",
    "                            log_file='tokenizer_log.txt')\n",
    "tokenizer.start(bagify_features=bagify_features, method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
