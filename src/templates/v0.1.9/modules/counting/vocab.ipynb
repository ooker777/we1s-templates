{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab\n",
    "\n",
    "This builds a single json file (the vocab file) containing term counts for all the documents in a json directory.\n",
    "\n",
    "The vocab file can be loaded into a `Vocab` object called `vocab`. You can view the file's contents by calling `vocab.vocab`. However, since this is a large list, it is recommended that you view a slice like `vocab.vocab[0:100]` (to view the first 100 terms), or you may freeze the notebook.\n",
    "\n",
    "The `Vocab` object also has a number of methods for obtaining various types of information from the vocab file. These methods are listed below: \n",
    "\n",
    "- `vocab.get_filenames()`: Returns a list of filenames in the vocab.\n",
    "- `vocab.get_names()`: Returns a list of names of json documents in the vocab.\n",
    "- `vocab.get_document(value)`: Returns a dict containing a single document. The value can be either be a filename or a `name` field value.\n",
    "- `vocab.get_documents(value)`: Returns a list of dicts containing document. The value can be either be a list of filenames or a list of `name` field values.\n",
    "- `vocab.get_num_docs()`: Returns the number of documents in the vocab.\n",
    "- `vocab.get_num_terms(documents=None)`:  Returns the number of terms in the entire vocab or a list of documents. If using a list of documents, call `vocab.get_num_terms(documents=['document1', 'document2'])`. If you are unsure of the names of your documents, you can get a list with `vocab.get_names()`.\n",
    "- `vocab.get_num_tokens(documents=None)`:  Returns the total number of tokens in the entire vocab or a list of documents. If using a list of documents, call `vocab.get_num_tokens(documents=['document1', 'document2'])`. If you are unsure of the names of your documents, you can get a list with `vocab.get_names()`.\n",
    "- `vocab.get_terms(documents=None, sortby=['TERM', 'COUNT'], ascending=[True, False], as_dict=False)`: Returns a dataframe containing the terms and counts in the vocab or a list of documents specified by filenames or `name` field values. By default, the data is sorted in ascending order of terms and descending order of counts. These can be modified using the `sortby` and `ascending` parameters. If you choose to include only one `sortby` criterion in the list make sure that the `ascending` parameter also has one value (and vice versa). `Setting `as_dict=True` will return a plain dict.\n",
    "\n",
    "### INFO\n",
    "\n",
    "__author__    = 'Scott Kleinman'  \n",
    "__copyright__ = 'copyright 2020, The WE1S Project'  \n",
    "__license__   = 'MIT'  \n",
    "__version__   = '2.0'  \n",
    "__email__     = 'scott.kleinman@csun.edu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Configuration (only needs to be changed in rare circumstances)\n",
    "json_dir = '/project_data/json'\n",
    "vocab_file = '/project_data/vocab.json'\n",
    "\n",
    "# Define paths\n",
    "current_dir = %pwd\n",
    "current_pathobj = Path(current_dir)\n",
    "project_dir = str(current_pathobj.parent.parent)\n",
    "json_dir = project_dir + json_dir\n",
    "vocab_file = project_dir + vocab_file\n",
    "\n",
    "# Import scripts\n",
    "%run scripts/vocab.py\n",
    "\n",
    "# Display the project directory\n",
    "display(HTML('<p><strong>Project Directory:</strong> ' + project_dir + '</p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Vocab File\n",
    "\n",
    "You only need to run this cell once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocab file\n",
    "build_vocab(json_dir, vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the `Vocab` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vocab object\n",
    "vocab = Vocab(vocab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Stuff\n",
    "\n",
    "You can now do stuff by calling the `Vocab` object's methods. The example below gets a table of term counts for the entire vocabulary. For convenience, here is a copy of the available methods (described in the first cell of this notebook):\n",
    "\n",
    "- `vocab.get_filenames()`: Returns a list of filenames in the vocab.\n",
    "- `vocab.get_names()`: Returns a list of names of json documents in the vocab.\n",
    "- `vocab.get_document(value)`: Returns a dict containing a single document. The value can be either be a filename or a `name` field value.\n",
    "- `vocab.get_documents(value)`: Returns a list of dicts containing document. The value can be either be a list of filenames or a list of `name` field values.\n",
    "- `vocab.get_num_docs()`: Returns the number of documents in the vocab.\n",
    "- `vocab.get_num_terms(documents=None)`:  Returns the number of terms in the entire vocab or a list of documents. If using a list of documents, call `vocab.get_num_terms(documents=['document1', 'document2'])`. If you are unsure of the names of your documents, you can get a list with `vocab.get_names()`.\n",
    "- `vocab.get_num_tokens(documents=None)`:  Returns the total number of tokens in the entire vocab or a list of documents. If using a list of documents, call `vocab.get_num_tokens(documents=['document1', 'document2'])`. If you are unsure of the names of your documents, you can get a list with `vocab.get_names()`.\n",
    "- `vocab.get_terms(documents=None, sortby=['TERM', 'COUNT'], ascending=[True, False], as_dict=False)`: Returns a dataframe containing the terms and counts in the vocab or a list of documents specified by filenames or `name` field values. By default, the data is sorted in ascending order of terms and descending order of counts. These can be modified using the `sortby` and `ascending` parameters. If you choose to include only one `sortby` criterion in the list make sure that the `ascending` parameter also has one value (and vice versa). `Setting `as_dict=True` will return a plain dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Term Counts Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = vocab.get_terms(sortby=['COUNT', 'TERM'], ascending=[False, True])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Query on the Term Counts Table\n",
    "\n",
    "This cell provides a means of querying the table produced in the previous cell (you will get an error if you do not first run that cell). It uses the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html\" target=\"_blank\">pandas query method</a>, and user should consult the pandas documentation for information about how to construct specific queries.\n",
    "\n",
    "The example below filters the table so that it contains only terms not in the list of terms provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = table.query('TERM not in [\"the\", \"a\", \"and\", \"of\", \"to\", \"in\", \"is\", \"for\", \"that\", \"was\", \"at\", \"i\"]')\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Stop Words\n",
    "\n",
    "The cells below provide an example of how you to use a list of stop words to filter the table. You must have already run the **Get Term Counts** table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Stop Word List\n",
    "\n",
    "Add words to the list below to create a stop word list. If you want to load the text from a text file (one word per line), replace the list with the path to the file (e.g. `stopwords = 'stoplist.txt'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['the', 'and']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Download the A List of Stop Words\n",
    "\n",
    "This cell provides an example of how you might download a list of stop words (in this case the standard WE1S stop word list). It also shows how to add additional stop words to the list such as \"'s\" to the stop word list so that the query in the next cell can also filter the possessive \"'s\".\n",
    "\n",
    "If the stop word file is downloaded successfully, the first five stop words are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stopwords\n",
    "import requests\n",
    "stoplist_url = 'https://raw.githubusercontent.com/whatevery1says/preprocessing/master/libs/vectors/we1s_standard_stoplist.txt'\n",
    "response = requests.get(stoplist_url)\n",
    "stopwords = response.text.split('\\n')\n",
    "\n",
    "# We'll filter possessive 's as well\n",
    "stopwords.append(\"'s\")\n",
    "\n",
    "# Display the first five words of the stop word list\n",
    "stopwords[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the Query to Filter the Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = table.query('TERM not in ' + str(stopwords))\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Punctuation Marks\n",
    "\n",
    "There are many ways that you could filter punctuation from your table. The method below removes any term that contains a character not defined by Unicode as a \"word\" character, which includes punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = table[table.TERM.str.contains('\\w', regex= True, na=False)]\n",
    "query"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
