{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "This notebook enables you to import data to a project directory or a MongoDB database. Valid data sources are:\n",
    "\n",
    "- A zip archive of plain text files with an accompanying csv-formatted metadata file.\n",
    "- A zip archive of json files.\n",
    "- A zipped Frictionless Data data package containing json files.\n",
    "\n",
    "Metadata fields can be mapped onto those required by the WhatEvery1Says Workspace.\n",
    "\n",
    "By default, the notebook will import to the project directory in which it is located.\n",
    "\n",
    "It is also possible to import data directly from a MongoDB database to a project directory. For this functionality, see the **Import from MongoDB** cell below.\n",
    "\n",
    "## Info\n",
    "\n",
    "__authors__    = 'Scott Kleinman'  \n",
    "__copyright__ = 'copyright 2020, The WE1S Project'  \n",
    "__license__   = 'GPL'  \n",
    "__version__   = '2.5'  \n",
    "__email__     = 'scott.kleinman@csun.edu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python imports\n",
    "from pathlib import Path\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Get path to project_dir\n",
    "current_dir            = %pwd\n",
    "project_dir            = str(Path(current_dir).parent.parent)\n",
    "json_dir               = project_dir + '/project_data/json'\n",
    "config_path            = project_dir + '/config/config.py'\n",
    "import_script_path     = 'scripts/import.py'\n",
    "tokenizer_script_path  = 'scripts/import_tokenizer.py'\n",
    "\n",
    "# Import the project configuration and classes\n",
    "%run {config_path}\n",
    "%run {import_script_path}\n",
    "display_setup_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Configuration options are explained briefly below. For more information, please see this module's <a href=\"README.md\" target=\"_blank\">README</a> file.\n",
    "\n",
    "- `zip_file`: The name of the zip archive containing your data. By default, the archive is called `import.zip`, but you can modify the filename. If the data is in plain text format, you must also prepare a `metadata.csv` file. Does not apply when importing from MongoDB (you can set it to `None`).\n",
    "- `metadata.csv`: The name of your metadata file if you are importing plain text data. By default, it is called `metadata.csv`, but you can change the name. <span style=\"color:red;\">Important:</span> The metadata file must have `filename`, `pub_date`, `title`, and `author` as its first four headers. You can include additional metadata fields _after_ the `author` field. Does not apply when importing directly from JSON files or from MongoDB (you can set it to `None`).\n",
    "- `remove_existing_json`: Empty the json folder before importing. The default is `False`, so it is possible to add additional data on multiple runs.\n",
    "- `delete_imports_dir`: If set to `True`, the folder containing your `zip_file` and `metadata.csv` file will be deleted when the import is complete. Does not apply when importing from MongoDB (you can set it to `None`).\n",
    "- `delete_text_dir`: If set to `True`, the folder containing your imported plain text files will be deleted after they are converted to json format. Does not apply when importing directly from JSON files or from MongoDB (you can set it to `None`).\n",
    "- `data_dirs`: If you are importing data already in json format, you can specify a list of paths in your zip archive or Frictionless Data data package where the json files are located. Does not apply when importing from MongoDB (you can set it to `None`).\n",
    "- `title_field`: If you are importing data already in json format that does not contain a field named `title` you can map an existing field to this key by providing the name of the existing field here.\n",
    "- `author_field`: If you are importing data already in json format that does not contain a field named `author` you can map an existing field to this key by providing the name of the existing field here.\n",
    "- `pub_date_field`: If you are importing data already in json format that does not contain a field named `pub_date` you can map an existing field to this key by providing the name of the existing field here.\n",
    "- `content_field`: If you are importing data already in json format that does not contain a field named `content` you can map an existing field to this key by providing the name of the existing field here.\n",
    "- `dedupe`: If set to `True`, the script will check for duplicate files within the project that may have been created by importing data from multiple zip archives. Duplicate files will be given the extension `.dupe`. This option also changes the extension of json files containing empty `content` fields to `.empty`. <span style=\"color:red;\">Warning:</span> For very large projects (~100,000 or more documents), duplicate detection may take up to several hours to run and, depending on other traffic on the server, may cause a server error.\n",
    "- `random_sample`: If you wish to import a random sample of the data in your `zip_file`, specify the number of documents you wish to import.\n",
    "- `random_seed`: Specify a number to initialize the random sampling. This ensures reproducibility if you have to run the import multiple times. In most cases, the setting can be left as `1`.\n",
    "- `required_phrase`: A word or phrase which will be used to filter the imported data. Only documents that contain the `required_phrase` value will be imported to your project.\n",
    "- `log_file`: The path to the file where errors and deduping results are logged. The default is `import_log.txt` in the same folder as this notebook.\n",
    "\n",
    "If you are importing your data directly to MongoDB, rather than a project folder, configure your MongoDB `client`, your database as `db`, and the name of your `collection`. For the `client` setting you can simply enter `MONGODB_CLIENT` to use your project's configuration. If importing from MongoDB, the `query` setting should be a valid MongoDB query. Since MongoDB syntax can be difficult &mdash; especially for complex queries &mdash; you may wish to use the <a href=\"query-builder/index.html\" target=\"_blank\">WE1S QueryBuilder</a> to construct your query and then paste it into the configuration cell. For information on customizing the QueryBuilder for your data, see the <a href=\"README.md\" target=\"_blank\">README</a> file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Configuration\n",
    "zip_file              = 'import.zip' # The name of the zip archive containing your data files\n",
    "metadata_file         = 'metadata.csv' # The name of the meadata file (only required for plain text data)\n",
    "remove_existing_json  = False # Clear an existing json folder before importing\n",
    "delete_imports_dir    = False # Delete the imports folder after the data has been extracted\n",
    "delete_text_dir       = False # Delete the plain text files folder after the data has been imported\n",
    "data_dirs             = None # For zipped json files and data packages, list of directories in which data is located\n",
    "title_field           = None\n",
    "author_field          = None\n",
    "pub_date_field        = None\n",
    "content_field         = None\n",
    "dedupe                = False  \n",
    "random_sample         = None\n",
    "random_seed           = 1\n",
    "required_phrase       = None\n",
    "save_mode             = 'project' # Set to 'db' to import data directly to MongoDB\n",
    "logfile               = 'import_log.txt' # The name of the error log file\n",
    "\n",
    "# MongoDB Configuration (required only if importing from MongoDB or saving imports to MongoDB)\n",
    "client                = 'mongodb://mongo:27017'\n",
    "db                    = ''\n",
    "collection            = ''\n",
    "query                 = {} # The query to perform if importing from MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Workspace for File Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Import object\n",
    "task = Import(zip_file=zip_file, metadata=metadata_file, delete_imports_dir=delete_imports_dir,\n",
    "              delete_text_dir=delete_text_dir, title_field=title_field, author_field=author_field,\n",
    "              pub_date_field=pub_date_field, content_field=content_field, dedupe=dedupe,\n",
    "              random_sample=random_sample, random_seed=random_seed, required_phrase=required_phrase,\n",
    "              logfile=logfile, client=client, db=db, collection=collection, project_dir=project_dir,\n",
    "              json_dir=json_dir, save_mode=save_mode, environment='jupyter')\n",
    "\n",
    "# Create the import directories\n",
    "task.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the import\n",
    "task.start_import(remove_existing_json=remove_existing_json)\n",
    "\n",
    "# Tokenization message\n",
    "display(HTML('<p>If you would like tokenize your imported data, proceed to the <strong>Tokenize the Data</strong> section below.</p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import from MongoDB\n",
    "\n",
    "Make sure that you have configured your database information and query in the **Configuration** cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Import object\n",
    "task = MongoDBImport(query, client=client, db=db, collection=collection, project_dir=project_dir, json_dir=json_dir,\n",
    "                     title_field=title_field, author_field=author_field, pub_date_field=pub_date_field,\n",
    "                     content_field=content_field, dedupe=dedupe, random_sample=random_sample, random_seed=random_seed,\n",
    "                     required_phrase=required_phrase, logfile=logfile, environment='jupyter')\n",
    "\n",
    "# Start the import\n",
    "task.start_import(remove_existing_json=remove_existing_json)\n",
    "\n",
    "# Tokenization message\n",
    "display(HTML('<p>If you would like tokenize your imported data, proceed to the <strong>Tokenize the Data</strong> section below.</p>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data\n",
    "\n",
    "The cell below will generate tokens counts for each of your documents and save them to a `bag_of_words` field in each document. This can speed up processing for downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "You do not have to reconfigure the `json_dir` if you have already run the first cell of this notebook. Errors will be logged to the path you set for the `log_file`.\n",
    "\n",
    "If you would like to save your tokens as a bag of words, set `bagify_features=True`. If your data has the tokens in a `features` table tokens will be counted from that table; otherwise, the `content` field will be tokenized first. If your data does not have a `features` table, and you would like to save one to your json documents, set `save_features_table=True` and `method='we1s'`. For more information on features tables, see the <a href=\"README.md\" target=\"_blank\">README</a> file.\n",
    "\n",
    "The default tokenization method strips all non-alphanumeric characters and splits the text into tokens on white space. If you would like to use the WE1S tokenizer, set `method='we1s'`. Note that this method takes longer. The WE1S tokenizer leverages <a href=\"https://spacy.io/\" target=\"_blank\">spaCy</a> and its the language models. The default language model is `en_core_web_sm`, but this can be changed. However, you will have to download another model into your environment. See the <a href=\"README.md\" target=\"_blank\">README</a> file for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir             = json_dir\n",
    "log_file             = 'tokenizer_log.txt'\n",
    "bagify_features      = True\n",
    "save_features_table  = False\n",
    "method               = 'we1s'\n",
    "language_model       = 'en_core_web_sm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run {tokenizer_script_path}\n",
    "tokenizer = ImportTokenizer(json_dir, language_model='en_core_web_sm',\n",
    "                            log_file='tokenizer_log.txt')\n",
    "tokenizer.start(bagify_features=bagify_features, save_features_table=save_features_table, method=method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
