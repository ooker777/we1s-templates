{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The WE1S Workspace is a containerized environment for importing, managing, and analyzing textual data using a variety of built-in analysis and visualization tools. It is designed to be well-documented and easy to use, but extendible, code-based environment. In the WE1S Workspace, users interact with their data inside a project created from a template. Version Compatibility This documentation is compatible with v1.0.0 of the WE1S Workspace and v1.0.0 of the WE1S project template. Projects are managed primarily through a Jupyter notebooks-based environment. Jupyter notebooks are a web-based interface for running code inside your web browser. If you are unfamiliar with Jupyter notebooks or Python, please read the the How Jupyter Notebooks Work and Introduction to Python sections of this guide before moving on to Creating a Project . These tutorials provide basic introductions to how to use Jupyter notebooks and some basic Python terminology useful for navigating the WE1S Workspace. This website is designed to get you up and running once you have installed the Workspace on your computer. Important The search function on this site requires a server environment. If you copy the getting_started folder to a location outside the Workspace, you should still be able to read this site, but the search function will not work. For more information about the WhatEvery1Says (WE1S) Project can be found on the WE1S website: https://we1s.ucsb.edu/ . Last update: 2021-03-07","title":"Introduction"},{"location":"#introduction","text":"The WE1S Workspace is a containerized environment for importing, managing, and analyzing textual data using a variety of built-in analysis and visualization tools. It is designed to be well-documented and easy to use, but extendible, code-based environment. In the WE1S Workspace, users interact with their data inside a project created from a template. Version Compatibility This documentation is compatible with v1.0.0 of the WE1S Workspace and v1.0.0 of the WE1S project template. Projects are managed primarily through a Jupyter notebooks-based environment. Jupyter notebooks are a web-based interface for running code inside your web browser. If you are unfamiliar with Jupyter notebooks or Python, please read the the How Jupyter Notebooks Work and Introduction to Python sections of this guide before moving on to Creating a Project . These tutorials provide basic introductions to how to use Jupyter notebooks and some basic Python terminology useful for navigating the WE1S Workspace. This website is designed to get you up and running once you have installed the Workspace on your computer. Important The search function on this site requires a server environment. If you copy the getting_started folder to a location outside the Workspace, you should still be able to read this site, but the search function will not work. For more information about the WhatEvery1Says (WE1S) Project can be found on the WE1S website: https://we1s.ucsb.edu/ . Last update: 2021-03-07","title":"Introduction"},{"location":"about/","text":"About the WhatEvery1Says (WE1S) Project For more information about the WhatEvery1Says (WE1S) Project, go to https://we1s.ucsb.edu/ . The WE1S GitHub site is https://github.com/whatevery1says .","title":"About the WhatEvery1Says (WE1S) Project"},{"location":"about/#about-the-whatevery1says-we1s-project","text":"For more information about the WhatEvery1Says (WE1S) Project, go to https://we1s.ucsb.edu/ . The WE1S GitHub site is https://github.com/whatevery1says .","title":"About the WhatEvery1Says (WE1S) Project"},{"location":"creating-a-project/","text":"Creating a Project Begin by double-clicking on the new_project.ipynb notebook. Follow the instructions to create a new project, and then click the link to navigate to the project folder. Note It is good practice to close and halt the new_project.ipynb notebook after you have opened the project folder in a new browser tab. A project is a folder containing copies of the WE1S project template files and folders. Here is a brief description of each part of the project: The README.md File This is a Markdown file containing details about the version of the WE1S template used to create the project and any metadata about the project which you added in new_project.ipynb . It is meant to be a human-readable guide to the content of the project. The datapackage.json File The datapackage.json file is a manifest of your project's resources which is compliant with the WE1S manifest schema and the Frictionless Data project specification. The purpose of this file is to enable easier interoperability between your data and tools outside the WE1S Workspace. The datapackage.json file is is a JSON file containing metadata about the project and a complete list of the file paths to all resources in the project. If you export your project, the export module will detect any files you have added and add their paths to datapackage.json . The config.py File The config.py file inside the config folder is a Python file that contains information about the Workspace's server environment and the resources installed with the project template. It is used to restore the project to a virgin state if you run the models/clear_caches.ipynb notebook. The modules Folder The modules folder contains all the project's Jupyter notebooks (and supporting scripts and files). Each module focuses on a particular task: e.g., creating a topic model or visualizing it. Some modules can be used at any point in your workflow. Others need to be implemented in a certain order. For instance, the dfr_browser , topic_bubbles , and pyldavis modules create visualizations of topic models, so they will naturally not work until you have run the topic_modeling module. The project_data Folder The project_data contains all your project's primary data. It is empty when the project is first created until you import your data using the import module. Note Sample projects providing example of the contents of each of these components of the Workspace can be found in the examples folder on GitHub. Importing Data to Your Project Before you do anything else, you must import some data to your project. The WE1S accepts data in four formats: A zip archive of plain text ( .txt ) files accompanied by a CSV file containing metadata. A zip archive of JSON files combining the textual content and metadata. A Frictionless Data data package containing paths to all your data files. A query of records in a MongoDB database. To import your data, navigate to modules/import/import.ipynb . This notebook creates a new folder, project_data/json and copies your data from its source into this folder, converting it into JSON format, if necessary. Important Most tools in the WE1S Workspace use the JSON files in the project_data/json folder. These tools assume that the files are compliant with the WE1S manifest schema. The import module provides some functions for converting your metadata fields to the expected format; however, it cannot cover every scenario. You may need to perform some preprocessing on your data prior to import. Note The import module automatically coerces your textual data to UTF-8 character encoding . What Next? Once you have imported your data, you can perform a number of procedures with the other modules. The WE1S project primarily employs topic modeling in its research methodlogy, so this technique is prominent in the Workspace in its current version. Many of the modules depend on your first having run a topic model on your data. This makes the topic_modeling module a good place to start. The metadata module contains some analysis and visualization tools that do not require a pre-existing topic model.","title":"Creating a Project"},{"location":"creating-a-project/#creating-a-project","text":"Begin by double-clicking on the new_project.ipynb notebook. Follow the instructions to create a new project, and then click the link to navigate to the project folder. Note It is good practice to close and halt the new_project.ipynb notebook after you have opened the project folder in a new browser tab. A project is a folder containing copies of the WE1S project template files and folders. Here is a brief description of each part of the project:","title":"Creating a Project"},{"location":"creating-a-project/#the-readmemd-file","text":"This is a Markdown file containing details about the version of the WE1S template used to create the project and any metadata about the project which you added in new_project.ipynb . It is meant to be a human-readable guide to the content of the project.","title":"The README.md File"},{"location":"creating-a-project/#the-datapackagejson-file","text":"The datapackage.json file is a manifest of your project's resources which is compliant with the WE1S manifest schema and the Frictionless Data project specification. The purpose of this file is to enable easier interoperability between your data and tools outside the WE1S Workspace. The datapackage.json file is is a JSON file containing metadata about the project and a complete list of the file paths to all resources in the project. If you export your project, the export module will detect any files you have added and add their paths to datapackage.json .","title":"The datapackage.json File"},{"location":"creating-a-project/#the-configpy-file","text":"The config.py file inside the config folder is a Python file that contains information about the Workspace's server environment and the resources installed with the project template. It is used to restore the project to a virgin state if you run the models/clear_caches.ipynb notebook.","title":"The config.py File"},{"location":"creating-a-project/#the-modules-folder","text":"The modules folder contains all the project's Jupyter notebooks (and supporting scripts and files). Each module focuses on a particular task: e.g., creating a topic model or visualizing it. Some modules can be used at any point in your workflow. Others need to be implemented in a certain order. For instance, the dfr_browser , topic_bubbles , and pyldavis modules create visualizations of topic models, so they will naturally not work until you have run the topic_modeling module.","title":"The modules Folder"},{"location":"creating-a-project/#the-project_data-folder","text":"The project_data contains all your project's primary data. It is empty when the project is first created until you import your data using the import module. Note Sample projects providing example of the contents of each of these components of the Workspace can be found in the examples folder on GitHub.","title":"The project_data Folder"},{"location":"creating-a-project/#importing-data-to-your-project","text":"Before you do anything else, you must import some data to your project. The WE1S accepts data in four formats: A zip archive of plain text ( .txt ) files accompanied by a CSV file containing metadata. A zip archive of JSON files combining the textual content and metadata. A Frictionless Data data package containing paths to all your data files. A query of records in a MongoDB database. To import your data, navigate to modules/import/import.ipynb . This notebook creates a new folder, project_data/json and copies your data from its source into this folder, converting it into JSON format, if necessary. Important Most tools in the WE1S Workspace use the JSON files in the project_data/json folder. These tools assume that the files are compliant with the WE1S manifest schema. The import module provides some functions for converting your metadata fields to the expected format; however, it cannot cover every scenario. You may need to perform some preprocessing on your data prior to import. Note The import module automatically coerces your textual data to UTF-8 character encoding .","title":"Importing Data to Your Project"},{"location":"creating-a-project/#what-next","text":"Once you have imported your data, you can perform a number of procedures with the other modules. The WE1S project primarily employs topic modeling in its research methodlogy, so this technique is prominent in the Workspace in its current version. Many of the modules depend on your first having run a topic model on your data. This makes the topic_modeling module a good place to start. The metadata module contains some analysis and visualization tools that do not require a pre-existing topic model.","title":"What Next?"},{"location":"glossary/","text":"Glossary \u201cCollection\u201d \u2014 A specific set of of texts (and data about them) that a user is working on inside a project. A collection can be the entirety of a corpus; but it can also be a particular subset of a corpus: e.g., only newspaper articles containing both the words humanities and science . \u201cComputer environment\u201d \u2014 The whole computing platform containing (in replicable containers ) the software and tools offered by WE1S. Users can download and install the environment on their own computers. \u201cCorpus / Corpora\u201d \u2014 The total set of texts (and data about them) that a user has collected or is working on. (Compare Collection .) \u201cData\u201d \u2014 Representations of a text collection in derived forms that are not readable as plain text. For example, data that the WE1S Workspace generates from text collections include: bags-of-words or term frequencies, ngram counts, etc. \u201cModule\u201d \u2014 A specific bundle of one or more Jupyter notebooks (and supporting scripts and files) available inside a project folder. Each module focuses on a particular task: e.g., creating a topic model or visualizing it. Each module contains a README.md file with a user guide for that module. \u201cProject\u201d \u2014 The folder location and file structure created by running the create_new_project.ipynb notebook. A project is where users work on collections of texts and data using the Workspace\u2019s modules. \u201cMetadata\u201d \u2014 Secondary data about the data being worked on in the WE1S Workspace. Metadata includes such citation information about collections of texts as author, publication, date, etc. But it can also include other kinds of labels or tags created by a user to facilitate addressing research questions. For example, the WE1S Project labeled publication sources in some of the collections it topic modeled based on geographical region, kind of publication, self-identified association with particular social groups, etc. \u201cTemplate\u201d \u2014 When a new project is created, folders for each module containing notebooks and supporting scripts or other resources are copied from a central location to your project folder. This is known as the project \u201ctemplate\u201d, and the individual files are called the \u201ctemplate\u201d files. Templates have version numbers so that it is clear what version of the template was used to produce a project even if the template files are updated after the project was created. \u201cWorkspace\u201d \u2014 The Jupyter notebook system within the WE1S computing environment for collecting, managing, analyzing, topic modeling, visualizing, and other operations on texts. When initially downloaded as part of the computing environment, the Workspace includes a Jupyter notebook for initiating a project and installing the necessary modules for managing project workflow.","title":"Glossary"},{"location":"glossary/#glossary","text":"\u201cCollection\u201d \u2014 A specific set of of texts (and data about them) that a user is working on inside a project. A collection can be the entirety of a corpus; but it can also be a particular subset of a corpus: e.g., only newspaper articles containing both the words humanities and science . \u201cComputer environment\u201d \u2014 The whole computing platform containing (in replicable containers ) the software and tools offered by WE1S. Users can download and install the environment on their own computers. \u201cCorpus / Corpora\u201d \u2014 The total set of texts (and data about them) that a user has collected or is working on. (Compare Collection .) \u201cData\u201d \u2014 Representations of a text collection in derived forms that are not readable as plain text. For example, data that the WE1S Workspace generates from text collections include: bags-of-words or term frequencies, ngram counts, etc. \u201cModule\u201d \u2014 A specific bundle of one or more Jupyter notebooks (and supporting scripts and files) available inside a project folder. Each module focuses on a particular task: e.g., creating a topic model or visualizing it. Each module contains a README.md file with a user guide for that module. \u201cProject\u201d \u2014 The folder location and file structure created by running the create_new_project.ipynb notebook. A project is where users work on collections of texts and data using the Workspace\u2019s modules. \u201cMetadata\u201d \u2014 Secondary data about the data being worked on in the WE1S Workspace. Metadata includes such citation information about collections of texts as author, publication, date, etc. But it can also include other kinds of labels or tags created by a user to facilitate addressing research questions. For example, the WE1S Project labeled publication sources in some of the collections it topic modeled based on geographical region, kind of publication, self-identified association with particular social groups, etc. \u201cTemplate\u201d \u2014 When a new project is created, folders for each module containing notebooks and supporting scripts or other resources are copied from a central location to your project folder. This is known as the project \u201ctemplate\u201d, and the individual files are called the \u201ctemplate\u201d files. Templates have version numbers so that it is clear what version of the template was used to produce a project even if the template files are updated after the project was created. \u201cWorkspace\u201d \u2014 The Jupyter notebook system within the WE1S computing environment for collecting, managing, analyzing, topic modeling, visualizing, and other operations on texts. When initially downloaded as part of the computing environment, the Workspace includes a Jupyter notebook for initiating a project and installing the necessary modules for managing project workflow.","title":"Glossary"},{"location":"how-jupyter-notebooks-work/","text":"How Jupyter Notebooks Work In the WE1S Workspace, users interact with their data using a Jupyter notebooks-based environment. Jupyter notebooks are a web-based interface for running code inside your web browser. There are two varieties of Jupyter notebooks: the classic Jupyter notebooks interface and the newer Jupyter Lab interface. The WE1S Workspace works with both, but the Jupyter Lab interface is opened by default. A \"notebook\" is like a web page with form fields (called \"cells\") into which you type your Python code. You can then run the code and get feedback. There are a number different notebook implementations, but Jupyter notebooks (formerly called iPython notebooks) are by far the most popular. When you open a specific notebook, you enter (or paste) your code in the cells and then click the Run button or type Shift + Enter to run your code. The WE1S Workspace provides notebooks in which the cells are pre-populated with the code necessary to perform specific tasks. Jupyter notebooks also contain documentation cells (written in a combination of Markdown and HTML) that describe the purpose of the cell's code. In some cases, you will be asked to enter configuration information, and this will require you to write some very basic Python code. If you are new to Python, this section provides instructions that will help you configure your cells. If you are more experienced with Python, you can modify the code in the WE1S Workspace to suit your needs. This provides a very flexible research environment suitable for beginners and more advanced users. Here is a small sample. The cell below contains some code that displays a message, waits 10 seconds, and then displays another message. Sample Jupyter notebook cell. Notice the small In []: to the left of the cell. This tells you that the cell contains some input code. When you run the cell, it will display In [ * ]: . When the code has finished running, it will display In [ 1 ]: The next cell will display In [ 2 ]: , and so on. When the code finishes, the output of whatever the code is doing will display right below the cell. Also notice that there is a small white circle next to the words \"Python 3\" at the top right of the screen. When you are running a cell, the circle will turn grey. It will turn white again when the circle is finished. If you make a mistake when running a cell \u2014 or if there is a bug in the code we haven't caught or an issue we haven't anticipated \u2014 an error message will be displayed in the output section. Note that for longer code blocks, you may have to scroll to see the error, so, if something doesn't seem to be working, look for an error. The meaning of Python error messages can be somewhat opaque until you get used to them, so we recommend copying the error you get, pasting it into a Google search bar, and starting there. You can clear the output by selecting Cell > Current Outputs > Clear or `Cell > All Output > Clear in the menu at the top of the notebook. There are lots of other menu items which allow you to add and delete cells, as well as to run cells. For a full tutorial on Jupyter notebooks, see Quinn Dombrowski, Tassie Gniady, and David Kloster's Introduction to Jupyter Notebooks . Using Jupyter Lab In the Classic notebook environment, you typically start in a browser tab showing your file and folder hierarchy. By clicking the New button at the top right corner, you can open new tabs containing notebooks, text files, or terminals for running processes from the command line. By contrast, Jupyter Lab operates in a single browser tab with the file/folder hierarchy in a sidebar and subtabs containing notebooks, files, and terminals. To open a new subtab, you click the appropriate icon for a notebook, text file, or terminal. One of the other main differences is that many functions are now located in the contextual menu opened by right-clicking on the screen. A screenshot of Jupyter Lab illustrates the appearance of the interface. Screenshot of Jupyter Lab. It is always possible to switch the Classic interface from within Jupyter Lab by selecting \"Launch Classic Notebook\" from the Help menu. For more information, see the Jupyter Lab interface . Hint Classic notebooks will always have the path tree in the url in the browser's address bar. By changing tree to lab , and vice versa, you can switch between the two interfaces.","title":"How Jupyter Notebooks Work"},{"location":"how-jupyter-notebooks-work/#how-jupyter-notebooks-work","text":"In the WE1S Workspace, users interact with their data using a Jupyter notebooks-based environment. Jupyter notebooks are a web-based interface for running code inside your web browser. There are two varieties of Jupyter notebooks: the classic Jupyter notebooks interface and the newer Jupyter Lab interface. The WE1S Workspace works with both, but the Jupyter Lab interface is opened by default. A \"notebook\" is like a web page with form fields (called \"cells\") into which you type your Python code. You can then run the code and get feedback. There are a number different notebook implementations, but Jupyter notebooks (formerly called iPython notebooks) are by far the most popular. When you open a specific notebook, you enter (or paste) your code in the cells and then click the Run button or type Shift + Enter to run your code. The WE1S Workspace provides notebooks in which the cells are pre-populated with the code necessary to perform specific tasks. Jupyter notebooks also contain documentation cells (written in a combination of Markdown and HTML) that describe the purpose of the cell's code. In some cases, you will be asked to enter configuration information, and this will require you to write some very basic Python code. If you are new to Python, this section provides instructions that will help you configure your cells. If you are more experienced with Python, you can modify the code in the WE1S Workspace to suit your needs. This provides a very flexible research environment suitable for beginners and more advanced users. Here is a small sample. The cell below contains some code that displays a message, waits 10 seconds, and then displays another message. Sample Jupyter notebook cell. Notice the small In []: to the left of the cell. This tells you that the cell contains some input code. When you run the cell, it will display In [ * ]: . When the code has finished running, it will display In [ 1 ]: The next cell will display In [ 2 ]: , and so on. When the code finishes, the output of whatever the code is doing will display right below the cell. Also notice that there is a small white circle next to the words \"Python 3\" at the top right of the screen. When you are running a cell, the circle will turn grey. It will turn white again when the circle is finished. If you make a mistake when running a cell \u2014 or if there is a bug in the code we haven't caught or an issue we haven't anticipated \u2014 an error message will be displayed in the output section. Note that for longer code blocks, you may have to scroll to see the error, so, if something doesn't seem to be working, look for an error. The meaning of Python error messages can be somewhat opaque until you get used to them, so we recommend copying the error you get, pasting it into a Google search bar, and starting there. You can clear the output by selecting Cell > Current Outputs > Clear or `Cell > All Output > Clear in the menu at the top of the notebook. There are lots of other menu items which allow you to add and delete cells, as well as to run cells. For a full tutorial on Jupyter notebooks, see Quinn Dombrowski, Tassie Gniady, and David Kloster's Introduction to Jupyter Notebooks .","title":"How Jupyter Notebooks Work"},{"location":"how-jupyter-notebooks-work/#using-jupyter-lab","text":"In the Classic notebook environment, you typically start in a browser tab showing your file and folder hierarchy. By clicking the New button at the top right corner, you can open new tabs containing notebooks, text files, or terminals for running processes from the command line. By contrast, Jupyter Lab operates in a single browser tab with the file/folder hierarchy in a sidebar and subtabs containing notebooks, files, and terminals. To open a new subtab, you click the appropriate icon for a notebook, text file, or terminal. One of the other main differences is that many functions are now located in the contextual menu opened by right-clicking on the screen. A screenshot of Jupyter Lab illustrates the appearance of the interface. Screenshot of Jupyter Lab. It is always possible to switch the Classic interface from within Jupyter Lab by selecting \"Launch Classic Notebook\" from the Help menu. For more information, see the Jupyter Lab interface . Hint Classic notebooks will always have the path tree in the url in the browser's address bar. By changing tree to lab , and vice versa, you can switch between the two interfaces.","title":"Using Jupyter Lab"},{"location":"introduction-to-python/","text":"Introduction to Python Python is an easy-to-learn programming language that is very popular in the digital humanities, in part because it has a large user community that has developed tools for such tasks as natural language processing and machine learning. You should be aware that there are two common versions of the language: Python 2 and Python 3. Although tools developed in Python 2 are still common, it is no longer supported as of January 2020. All of the code in the WE1S is written in Python 3.7. The differences are fairly minor, but, if you need to Google how do something in Python, be sure that you are getting an answer that is compatible with version 3.7. Basic Python Concepts File Paths A file path specifies the unique location of a file on a file system. File paths are important to understand when navigating through your file system using your computer's command line and when using a programming language (for more on Mac command lines, we recommend Miriam Posner's Get to know your terminal ); for more on Windows command lines, we recommend How to use the Windows command line ). There are 2 common kinds of file paths: absolute and relative. An absolute file path points to the location of a file by displaying the full directory tree hierarchy in which path components, separated by a delimiting character (usually a slash, / or \\ , depending on your operating system), represent each directory under which the file is stored. For example, on a Mac or Linux machine, the absolute file path of a text file stored in a folder on a user's Desktop looks something like this: / Users / user_name / Desktop / folder_name / file_name . txt On a Windows machine, the absolute file path of a text file stored on a user's Desktop looks like this: C : \\ Users \\ user_name \\ Desktop \\ folder_name \\ file_name . txt A relative file path , in comparison, points to the location of a file relative to \"where\" the user is in the file system. So, if you used your command line to navigate to the folder on your Desktop described above and then looked at the text file, the relative file path would simply be: file_name . txt If you then used your command line to move back up to your Desktop and looked for the text file, the relative file path would be: folder_name / file_name . txt (Mac or Linux) or folder_name \\ file_name . txt (Windows). Note The WE1S Workspace runs on a Linux machine, so any file path you see or need to enter will be of the Mac/Linux variety using a forward / . Many file paths will be automatically configured for you in the WE1S Workspace, but you will need to enter file paths from time to time. The instructions in each notebook will tell you whether to enter an absolute or relative file path. If you're not used to working with them, file paths can be vexing because entering in the wrong file path will cause errors in otherwise \"working\" code. In general, if you see a FileNotFoundError output to a cell in your notebook, your issue could be related to file path(s) you have configured. Python Packages In Python, you use \"functions\" to perform operations. For example, the print () function prints whatever message inside the parentheses to the screen. Python has a set, or \"library\", of functions (known as \"the standard library\") like print () , which can be \"called\" out of the box. It is also possible to import libraries of functions, which are called either \"packages\" or \"modules\", into your code. Some Python packages like time have to be imported; they are not available by default in order to save memory. Other Python packages are designed by third-party contributors. These generally have to be installed on the machine running the code and then imported into the notebook. The WE1S Workspace comes with all required packages pre-installed. In the notebooks, a cell (generally at the beginning of the notebook) is pre-configured to import them into the notebook. You will see this where the cell contains lines like import PACKAGE_NAME (where PACKAGE_NAME represents the name of the package) or from PACKAGE_NAME import FUNCTION_NAME . You will not need to modify this setup, but it is useful to be familiar with the terminology and aware of the purpose of this code. Data Types In most programming languages, content is categorised into types of data, each of which has its own set of behaviours. In particular, some functions can only operate with certain data types. In Python, some of the most common types of content are integers , floats , strings , lists , dictionaries (also known as \"dicts\"), and Booleans (True or False). The data type of any piece of data is defined using delimiter symbols: strings : enclosed in apostrophes or quotation marks (e.g. 'John' or \"John\" ). integers and floats : no delimiters (e.g. 100 , 3.14 ) lists : enclosed in square brackets (e.g. [ 1 , 2 ] or [ 'Jack' , 'Jill' ] ). dicts : enclosed in curly brackets (e.g. { 'Jack' : 1 } ). Booleans : True or False (capitalised with no delimiters) We'll look at each of these datatypes more closely below. Strings contain alphanumeric data \u2014 that is, either numbers or letters, as well as punctuation marks. Strings are delimited either by single or double quotation marks. So 'hello' and \"hello\" are exactly the same. In the WE1S Workspace, single quotation marks are used wherever possible (and also because that is a recommendation of a prominent Python code style guide). Quotation marks inside of apostrophe delimiters will be interpreted as part of the string, as will apostrophes inside quotation mark delimiters. For example, you could encode the sentence He said, \"We are ready.\" as 'He said, \"We are ready.\"' There are a number of gotchas when dealing with strings. First, curly quotes or smart quotes are not interpreted as quotation marks by Python; they will be treated as characters in the string. So be warned. If you try to copy code from a document with curly quotes, you are likely to get an error. Some strings may contain apostrophes or quotation marks that are part of the string, not delimiter symbols. Take, for example, the sentence We 're ready. . Coding this as 'We' re ready . ' will generate an error because Python will interpret the second apostrophe as a delimiter, ending the string. An obvious way around this problem is to use double quotation marks as the delimiters: \"We're ready.\" . That works. But what if you wanted to use the sentence He said , \"We're ready.\" ? For this sentence you will probably want to use a method called \"escaping\" the ambiguous punctuation mark. In Python, characters like this are normally escaped with a preceding backslash. Here is any easy way to fix the problem: 'He said, \"We \\' re ready.\"' . The backslash before the apostrophe tells Python not to interpret it as a string delimiter. Note In the WE1S Workspace we try to use single quotation marks as consistently as possible, switching to double quotation marks only if there is a reason to switch. This conforms with Python's PEP 8 recommendations for code style. Various types of numbers are recognised by Python. The most important types are floats, which can have decimal points, and integers, which cannot. The standard library contains functions to convert one to the other. Their different behaviours mostly become important when you are performing mathematical operations. Mathematical operators are symbols like + and - that tell Python what operations to perform. As you can imagine, they indicate \"add\" and \"subtract\" respectively. Other mathematical operators are * (multiply) and / (divide). It is not necessary to put spaces around the operators, but it is generally good coding style. For a fuller (but still digestible) description of mathematical operators, see Basic Math Operators in Python . Lists are comma-separated items belonging to other data types such as numbers or strings, as in the examples above. You can also have lists of dictionaries, or even lists of lists. Items in lists are stored in a strict order, and each item has an index number. Like most programming languages, Python uses zero-indexing, so the first item in the list will have the index number 0. You can reference items in a list by giving its number in square brackets. For instance, if you had a list called my_list , you could get the first item in the list with my_list [ 0 ] . Dictionaries (also called dicts ) are like lists, except that each item is a key-value pair (separated by a colon). A common use of dictionaries is to contain word counts. Consider this dictionary: { 'Jack' : 1 , 'Jill' : 2 } . This might be used to indicate that the word \"Jack\" occurs once and the word \"Jill\" occurs twice. Items in dictionaries are not stored in order, so they must be referenced by their keys. In order to illustrate this, we need to save our dict to a variable a concept that will be explained in full below. We'll call our variable word_counts . We can then find the number of times \"Jill\" occurs by calling word_counts [ 'Jill' ] . Try running the cell below to demonstrate this. Then experiment with creating your own dicts and re-run the cell. Booleans are binary values, either True or False . Important The Boolean values True and False must be capitalized, and they must not be contained in quotation marks (i.e. not 'True' or 'False' ). Variables A variable is a container for some data of any type. This container is referenced by a label or variable name. The content, or value , of a variable is assigned to the variable name using the = sign. For instance, firstname = 'John' would assign the string 'John' to the variable firstname . If you also assigned lastname = 'Smith' , then firstname + ' ' + lastname would produce the string 'John Smith' . In some cases, you have to create the variable before you assign a value to it. For instance, you might create an empty list like my_list = [] . Once you have assigned a value to a variable name, you can reference it by the variable name in your subsequent code. This is what we did with the word_counts variable in the discussion of dictionaries above. You can name variables whatever you want, although they cannot have spaces or punctuation marks. It is good coding style to make variable names short and descriptive of the values they are storing. As in the case of word_counts , it is conventional to use the underscore _ where you might use a space in ordinary English. We have tried to follow these guidelines as much as possible in the development of configuration variables for the WE1S Workspace. Functions A function is a command to do something with one of the data types and send back (or \"return\") a result. When functions form part of larger libraries, they are often called \"methods\". Since print () and str () are part of the standard library, they are often referred to as the print () and str () methods. Functions are called by using the name of the function (e.g. print ) followed by parentheses containing the data to be passed to the function. So print ( 'Hello world.' ) passes \"Hello world.\" to the print () function. The value in parentheses is called the argument or parameter . You can use a variable as the argument of a function. For instance, if you defined my_var = 1 , you could then use str ( my_var ) to convert 1 to a '1' . Another example is the append () function, which adds an item to the end of a list. For instance, you could add the number 1 to the my_list created above: my_list . append ( 1 ) . Code that is part of a function is indented by four spaces (a tab also works within the notebook environment). This also applies to certain function-like operations such as for loops or if ... then clauses. If you get an error around a function or either of these key words in the code, you may have accidentally changed the indentation. In the WE1S Workspace, many functions are pre-written and called from a single line of code that passes your configuration variables to the function. Complex functions are generally stored in separate files (typically in a scripts folder) and imported into the notebook in order to reduce clutter. Experienced Python users can open and modify the functions files. Comments Comments are blocks of text in the code that do not get executed as code. They exist to provide explanations for human readers to understand what the code is doing. In Python, comments are indicated by a preceding # . Anything with this preceding hash will be ignored. In the WE1S Workspace, code is heavily commented to help you see what its purpose is. A common trick is to \"comment out\" blocks of code so that they will not function by placing # at the beginning of the line. The code can then be \"uncommented\" by removing the # . A shortcut for commenting out and uncommenting code in the Jupyter notebooks environment is the key combination Ctrl + / . Use this for toggling commented code on and off. JSON In the WE1S Workspace, data is typically accessed in a format called JSON. JSON is an acronym for Javascript Object Notation, and, as the name suggests, it derives originally from the Javascript programming language. JSON looks and behaves pretty much like a Python dict with one major gotcha: keys and values have to be in double quotation marks. (There are a few other differences as well, but this is the most obvious). So, if we want to work with JSON data in Python, we need to parse it into a Python dict. Likewise, if we want to, for instance, send the material in a Python dict to a web brower or save it to a file, we first convert it to JSON format, the data type of which is a string. Python has a json library that performs this task, and the WE1S Workspace uses it internally when it accesses project data stored as JSON files.","title":"Introduction to Python"},{"location":"introduction-to-python/#introduction-to-python","text":"Python is an easy-to-learn programming language that is very popular in the digital humanities, in part because it has a large user community that has developed tools for such tasks as natural language processing and machine learning. You should be aware that there are two common versions of the language: Python 2 and Python 3. Although tools developed in Python 2 are still common, it is no longer supported as of January 2020. All of the code in the WE1S is written in Python 3.7. The differences are fairly minor, but, if you need to Google how do something in Python, be sure that you are getting an answer that is compatible with version 3.7.","title":"Introduction to Python"},{"location":"introduction-to-python/#basic-python-concepts","text":"","title":"Basic Python Concepts"},{"location":"introduction-to-python/#file-paths","text":"A file path specifies the unique location of a file on a file system. File paths are important to understand when navigating through your file system using your computer's command line and when using a programming language (for more on Mac command lines, we recommend Miriam Posner's Get to know your terminal ); for more on Windows command lines, we recommend How to use the Windows command line ). There are 2 common kinds of file paths: absolute and relative. An absolute file path points to the location of a file by displaying the full directory tree hierarchy in which path components, separated by a delimiting character (usually a slash, / or \\ , depending on your operating system), represent each directory under which the file is stored. For example, on a Mac or Linux machine, the absolute file path of a text file stored in a folder on a user's Desktop looks something like this: / Users / user_name / Desktop / folder_name / file_name . txt On a Windows machine, the absolute file path of a text file stored on a user's Desktop looks like this: C : \\ Users \\ user_name \\ Desktop \\ folder_name \\ file_name . txt A relative file path , in comparison, points to the location of a file relative to \"where\" the user is in the file system. So, if you used your command line to navigate to the folder on your Desktop described above and then looked at the text file, the relative file path would simply be: file_name . txt If you then used your command line to move back up to your Desktop and looked for the text file, the relative file path would be: folder_name / file_name . txt (Mac or Linux) or folder_name \\ file_name . txt (Windows). Note The WE1S Workspace runs on a Linux machine, so any file path you see or need to enter will be of the Mac/Linux variety using a forward / . Many file paths will be automatically configured for you in the WE1S Workspace, but you will need to enter file paths from time to time. The instructions in each notebook will tell you whether to enter an absolute or relative file path. If you're not used to working with them, file paths can be vexing because entering in the wrong file path will cause errors in otherwise \"working\" code. In general, if you see a FileNotFoundError output to a cell in your notebook, your issue could be related to file path(s) you have configured.","title":"File Paths"},{"location":"introduction-to-python/#python-packages","text":"In Python, you use \"functions\" to perform operations. For example, the print () function prints whatever message inside the parentheses to the screen. Python has a set, or \"library\", of functions (known as \"the standard library\") like print () , which can be \"called\" out of the box. It is also possible to import libraries of functions, which are called either \"packages\" or \"modules\", into your code. Some Python packages like time have to be imported; they are not available by default in order to save memory. Other Python packages are designed by third-party contributors. These generally have to be installed on the machine running the code and then imported into the notebook. The WE1S Workspace comes with all required packages pre-installed. In the notebooks, a cell (generally at the beginning of the notebook) is pre-configured to import them into the notebook. You will see this where the cell contains lines like import PACKAGE_NAME (where PACKAGE_NAME represents the name of the package) or from PACKAGE_NAME import FUNCTION_NAME . You will not need to modify this setup, but it is useful to be familiar with the terminology and aware of the purpose of this code.","title":"Python Packages"},{"location":"introduction-to-python/#data-types","text":"In most programming languages, content is categorised into types of data, each of which has its own set of behaviours. In particular, some functions can only operate with certain data types. In Python, some of the most common types of content are integers , floats , strings , lists , dictionaries (also known as \"dicts\"), and Booleans (True or False). The data type of any piece of data is defined using delimiter symbols: strings : enclosed in apostrophes or quotation marks (e.g. 'John' or \"John\" ). integers and floats : no delimiters (e.g. 100 , 3.14 ) lists : enclosed in square brackets (e.g. [ 1 , 2 ] or [ 'Jack' , 'Jill' ] ). dicts : enclosed in curly brackets (e.g. { 'Jack' : 1 } ). Booleans : True or False (capitalised with no delimiters) We'll look at each of these datatypes more closely below. Strings contain alphanumeric data \u2014 that is, either numbers or letters, as well as punctuation marks. Strings are delimited either by single or double quotation marks. So 'hello' and \"hello\" are exactly the same. In the WE1S Workspace, single quotation marks are used wherever possible (and also because that is a recommendation of a prominent Python code style guide). Quotation marks inside of apostrophe delimiters will be interpreted as part of the string, as will apostrophes inside quotation mark delimiters. For example, you could encode the sentence He said, \"We are ready.\" as 'He said, \"We are ready.\"' There are a number of gotchas when dealing with strings. First, curly quotes or smart quotes are not interpreted as quotation marks by Python; they will be treated as characters in the string. So be warned. If you try to copy code from a document with curly quotes, you are likely to get an error. Some strings may contain apostrophes or quotation marks that are part of the string, not delimiter symbols. Take, for example, the sentence We 're ready. . Coding this as 'We' re ready . ' will generate an error because Python will interpret the second apostrophe as a delimiter, ending the string. An obvious way around this problem is to use double quotation marks as the delimiters: \"We're ready.\" . That works. But what if you wanted to use the sentence He said , \"We're ready.\" ? For this sentence you will probably want to use a method called \"escaping\" the ambiguous punctuation mark. In Python, characters like this are normally escaped with a preceding backslash. Here is any easy way to fix the problem: 'He said, \"We \\' re ready.\"' . The backslash before the apostrophe tells Python not to interpret it as a string delimiter. Note In the WE1S Workspace we try to use single quotation marks as consistently as possible, switching to double quotation marks only if there is a reason to switch. This conforms with Python's PEP 8 recommendations for code style. Various types of numbers are recognised by Python. The most important types are floats, which can have decimal points, and integers, which cannot. The standard library contains functions to convert one to the other. Their different behaviours mostly become important when you are performing mathematical operations. Mathematical operators are symbols like + and - that tell Python what operations to perform. As you can imagine, they indicate \"add\" and \"subtract\" respectively. Other mathematical operators are * (multiply) and / (divide). It is not necessary to put spaces around the operators, but it is generally good coding style. For a fuller (but still digestible) description of mathematical operators, see Basic Math Operators in Python . Lists are comma-separated items belonging to other data types such as numbers or strings, as in the examples above. You can also have lists of dictionaries, or even lists of lists. Items in lists are stored in a strict order, and each item has an index number. Like most programming languages, Python uses zero-indexing, so the first item in the list will have the index number 0. You can reference items in a list by giving its number in square brackets. For instance, if you had a list called my_list , you could get the first item in the list with my_list [ 0 ] . Dictionaries (also called dicts ) are like lists, except that each item is a key-value pair (separated by a colon). A common use of dictionaries is to contain word counts. Consider this dictionary: { 'Jack' : 1 , 'Jill' : 2 } . This might be used to indicate that the word \"Jack\" occurs once and the word \"Jill\" occurs twice. Items in dictionaries are not stored in order, so they must be referenced by their keys. In order to illustrate this, we need to save our dict to a variable a concept that will be explained in full below. We'll call our variable word_counts . We can then find the number of times \"Jill\" occurs by calling word_counts [ 'Jill' ] . Try running the cell below to demonstrate this. Then experiment with creating your own dicts and re-run the cell. Booleans are binary values, either True or False . Important The Boolean values True and False must be capitalized, and they must not be contained in quotation marks (i.e. not 'True' or 'False' ).","title":"Data Types"},{"location":"introduction-to-python/#variables","text":"A variable is a container for some data of any type. This container is referenced by a label or variable name. The content, or value , of a variable is assigned to the variable name using the = sign. For instance, firstname = 'John' would assign the string 'John' to the variable firstname . If you also assigned lastname = 'Smith' , then firstname + ' ' + lastname would produce the string 'John Smith' . In some cases, you have to create the variable before you assign a value to it. For instance, you might create an empty list like my_list = [] . Once you have assigned a value to a variable name, you can reference it by the variable name in your subsequent code. This is what we did with the word_counts variable in the discussion of dictionaries above. You can name variables whatever you want, although they cannot have spaces or punctuation marks. It is good coding style to make variable names short and descriptive of the values they are storing. As in the case of word_counts , it is conventional to use the underscore _ where you might use a space in ordinary English. We have tried to follow these guidelines as much as possible in the development of configuration variables for the WE1S Workspace.","title":"Variables"},{"location":"introduction-to-python/#functions","text":"A function is a command to do something with one of the data types and send back (or \"return\") a result. When functions form part of larger libraries, they are often called \"methods\". Since print () and str () are part of the standard library, they are often referred to as the print () and str () methods. Functions are called by using the name of the function (e.g. print ) followed by parentheses containing the data to be passed to the function. So print ( 'Hello world.' ) passes \"Hello world.\" to the print () function. The value in parentheses is called the argument or parameter . You can use a variable as the argument of a function. For instance, if you defined my_var = 1 , you could then use str ( my_var ) to convert 1 to a '1' . Another example is the append () function, which adds an item to the end of a list. For instance, you could add the number 1 to the my_list created above: my_list . append ( 1 ) . Code that is part of a function is indented by four spaces (a tab also works within the notebook environment). This also applies to certain function-like operations such as for loops or if ... then clauses. If you get an error around a function or either of these key words in the code, you may have accidentally changed the indentation. In the WE1S Workspace, many functions are pre-written and called from a single line of code that passes your configuration variables to the function. Complex functions are generally stored in separate files (typically in a scripts folder) and imported into the notebook in order to reduce clutter. Experienced Python users can open and modify the functions files.","title":"Functions"},{"location":"introduction-to-python/#comments","text":"Comments are blocks of text in the code that do not get executed as code. They exist to provide explanations for human readers to understand what the code is doing. In Python, comments are indicated by a preceding # . Anything with this preceding hash will be ignored. In the WE1S Workspace, code is heavily commented to help you see what its purpose is. A common trick is to \"comment out\" blocks of code so that they will not function by placing # at the beginning of the line. The code can then be \"uncommented\" by removing the # . A shortcut for commenting out and uncommenting code in the Jupyter notebooks environment is the key combination Ctrl + / . Use this for toggling commented code on and off.","title":"Comments"},{"location":"introduction-to-python/#json","text":"In the WE1S Workspace, data is typically accessed in a format called JSON. JSON is an acronym for Javascript Object Notation, and, as the name suggests, it derives originally from the Javascript programming language. JSON looks and behaves pretty much like a Python dict with one major gotcha: keys and values have to be in double quotation marks. (There are a few other differences as well, but this is the most obvious). So, if we want to work with JSON data in Python, we need to parse it into a Python dict. Likewise, if we want to, for instance, send the material in a Python dict to a web brower or save it to a file, we first convert it to JSON format, the data type of which is a string. Python has a json library that performs this task, and the WE1S Workspace uses it internally when it accesses project data stored as JSON files.","title":"JSON"},{"location":"workspace-structure/","text":"Workspace Structure The WE1S Workspace is a cloud of virtual computers with all the resources you will require to manage your workflow. Most of your work will be on the write volume, which we will generally refer to as the Workspace. Technically, however, the Workspace also includes other volumes, such as the MongoDB instance described below. Each volume has access to the same data files. This section provides a brief overview of what you will see when you launch the Workspace. An outline of the over structure is displayed below, followed by explanations of each component. write \u2523 getting_started \u2523 project_template \u2523 create_template_archive.ipynb \u2523 getting_started.html \u2523 mongo_express.html \u2523 new_project.ipynb \u2523 template_package.py \u2517 README.md The write Folder The write folder is the location where you will do most of your work. When you launch the Workspace, you will see five files and two folders located inside your write folder. The functions of each of these is described below. getting_started.html This is simply a convenient file that you can open to launch this Getting Started guide. It actually just redirects the browser to getting_started/index.html . It is a read-only file and cannot be modified. README.md This is a Markdown file containing information about the version of the project template in your Workspace. If you download a later version, it may no longer be compatible, and you may have to install the most recent version of the Workspace. The information in this file can therefore be useful to check whether your project template and your Workspace container are compatible. The README.md file is a read-only file and cannot be edited. template_package.py This is Python script that contains functions used by both the create_template_archive and new_project notebooks. It is a read-only file and cannot be edited. create_template_archive.ipynb This notebook can be used to make a compressed .tar.gz file of your project_template folder. Generally, you will only want to do this if you have customized the template and wish to use your customizations in another project. Use this notebook to create an archive of your custom template, and you can point to it as your template source when creating projects with the new_project notebook. new_project.ipynb This notebook is the main starting point for you workflow. Use the new_project notebook to create a new copy of the project_template folder with metadata about your specific project. You can then enter the new project folder to import your data into the new project. getting_started This folder contains the Getting Started web site you are currently reading. The site can be launched by going to getting_stared/index.html or by launching getting_started.html from the Workspace write folder. You should not need to modify anything in this folder, unless you want to use the Getting Started site as a place to add notes to yourself in the site's individual web pages. project_template This folder contains are the files and resources required for the individual modules in WE1S Workspace. When a new project is created, these resources are copied from the project_template folder into the new project folder and become available for use in the new project. MongoDB The Workspace comes packaged with an instance of the MongoDB database, running on mongodb://mongo:27017 . You can use this the MongoDB instance to manage your data. MongoDB stores data in json-like records similar to the json files used in Workspace projects, making the interchange of data between the two relatively transparent. Modules access MongoDB databases through Python's pymongo wrapper. The use of MongoDB is optional. You can also choose to import your data from flat files. If you wish to use MongoDB, the mongo-express.html file will launch MongoExpress , a web-based admin interface for MongoDB, running on port 8081. This will allow you to perform some database administration tasks without running Python code. To Do What other information might be useful here?","title":"Workspace Structure"},{"location":"workspace-structure/#workspace-structure","text":"The WE1S Workspace is a cloud of virtual computers with all the resources you will require to manage your workflow. Most of your work will be on the write volume, which we will generally refer to as the Workspace. Technically, however, the Workspace also includes other volumes, such as the MongoDB instance described below. Each volume has access to the same data files. This section provides a brief overview of what you will see when you launch the Workspace. An outline of the over structure is displayed below, followed by explanations of each component. write \u2523 getting_started \u2523 project_template \u2523 create_template_archive.ipynb \u2523 getting_started.html \u2523 mongo_express.html \u2523 new_project.ipynb \u2523 template_package.py \u2517 README.md","title":"Workspace Structure"},{"location":"workspace-structure/#the-write-folder","text":"The write folder is the location where you will do most of your work. When you launch the Workspace, you will see five files and two folders located inside your write folder. The functions of each of these is described below.","title":" The write Folder"},{"location":"workspace-structure/#getting_startedhtml","text":"This is simply a convenient file that you can open to launch this Getting Started guide. It actually just redirects the browser to getting_started/index.html . It is a read-only file and cannot be modified.","title":" getting_started.html"},{"location":"workspace-structure/#readmemd","text":"This is a Markdown file containing information about the version of the project template in your Workspace. If you download a later version, it may no longer be compatible, and you may have to install the most recent version of the Workspace. The information in this file can therefore be useful to check whether your project template and your Workspace container are compatible. The README.md file is a read-only file and cannot be edited.","title":" README.md"},{"location":"workspace-structure/#template_packagepy","text":"This is Python script that contains functions used by both the create_template_archive and new_project notebooks. It is a read-only file and cannot be edited.","title":" template_package.py"},{"location":"workspace-structure/#create_template_archiveipynb","text":"This notebook can be used to make a compressed .tar.gz file of your project_template folder. Generally, you will only want to do this if you have customized the template and wish to use your customizations in another project. Use this notebook to create an archive of your custom template, and you can point to it as your template source when creating projects with the new_project notebook.","title":" \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n create_template_archive.ipynb"},{"location":"workspace-structure/#new_projectipynb","text":"This notebook is the main starting point for you workflow. Use the new_project notebook to create a new copy of the project_template folder with metadata about your specific project. You can then enter the new project folder to import your data into the new project.","title":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n new_project.ipynb"},{"location":"workspace-structure/#getting_started","text":"This folder contains the Getting Started web site you are currently reading. The site can be launched by going to getting_stared/index.html or by launching getting_started.html from the Workspace write folder. You should not need to modify anything in this folder, unless you want to use the Getting Started site as a place to add notes to yourself in the site's individual web pages.","title":"  getting_started"},{"location":"workspace-structure/#project_template","text":"This folder contains are the files and resources required for the individual modules in WE1S Workspace. When a new project is created, these resources are copied from the project_template folder into the new project folder and become available for use in the new project.","title":" project_template"},{"location":"workspace-structure/#mongodb","text":"The Workspace comes packaged with an instance of the MongoDB database, running on mongodb://mongo:27017 . You can use this the MongoDB instance to manage your data. MongoDB stores data in json-like records similar to the json files used in Workspace projects, making the interchange of data between the two relatively transparent. Modules access MongoDB databases through Python's pymongo wrapper. The use of MongoDB is optional. You can also choose to import your data from flat files. If you wish to use MongoDB, the mongo-express.html file will launch MongoExpress , a web-based admin interface for MongoDB, running on port 8081. This will allow you to perform some database administration tasks without running Python code. To Do What other information might be useful here?","title":" MongoDB"},{"location":"modules/","text":"Modules This section provides brief overviews of the individual modules in the Workspace. More information and a User Guide for each module can be found on the individual module pages and in the README.md files inside each module folder. comparing The comparing module allows you to compare two sets of textual data to one another to discover how, and how much, they differ using the Wilcoxon rank sum test, a statistical test that determines if the relative frequencies of specific words in two populations are significantly different from one another (that is, they have different distributions). This helps you to determine what the most \"significant\" words in each dataset are. For more on the Wilcoxon rank sum test in the context of corpus analytics, see Jefrey Lijffijt, Terttu Nevalainen, Tanja S\u00e4ily, Panagiotis Papapetrou, Kai Puolam\u00e4ki, Heikki Mannila, \"Significance testing of word frequencies in corpora\", Digital Scholarship in the Humanities , Volume 31, Issue 2, June 2016, Pages 374\u2013397. counting The counting module contains notebooks for counting various aspects of project data. The notebooks allow you to count project documents ( count_documents.ipynb ), to count the number of documents containing a specific token ( docs_by_search_term.ipynb ), to calculate token frequencies ( frequency.ipynb ), to calculate tf-idf scores ( tfidf.ipynb ), to calculate various collocation metrics ( collocation.ipynb ), and to grab summary statistics of documents, tokens, etc in the project ( vocab.ipynb ). The vocab.ipynb notebook requires that your json data files contain a bag_of_words field with term counts. If you did not generate this field when you imported your data, you can do so using tokenize.ipynb , which leverages spaCy's tokenizer. The docs_by_search_term.ipynb , frequency.ipynb , tfidf.ipynb , and collocation.ipynb notebooks use a custom tokenizer based on the tokenizer available in NLTK . This differs from the tokenizer WE1S uses in its preprocessing and topic modeling pipelines, which only tokenizes unigrams. As a result, some features of these notebooks will not work if you do not have access to full-text data. dendrogram The dendrogram module performs hierarchical agglomerative clustering on the topics of a topic model or multiple topic models in a project. Several distance metrics and linkage methods are available. The default settings will conform closely to the output of pyLDAvis and the scaled view of dfr-browser. Information on alternative distance metrics can be found here , and information on alternative linkage methods can be found here , in the documentation for the Python scipy package, which is used \"under the hood\". As dendrograms typically become crowded and hard to read, the visualizations are generated with Plotly , which allows them to be browsed interactively with its pan and zoom features. dfr_browser The dfr_browser module implements the creation and customization of Andrew Goldstone's dfr-browser from a topic model produced with MALLET (the output of the topic_modeling module). Dfr-browser code, stored in this module in dfrb_scripts , was written by Andrew Goldstone and adapted for WE1S use to provide an easy pipeline from data import to topic modeling to visualisation. WE1S uses an older version of Goldstone's code (v0.5.1) (see https://agoldst.github.io/dfr-browser/ for a version history). WE1S uses Goldstone's prepare_data.py Python script to prepare the data files, not Goldstone's R package. diagnostics The diagnostics module produces a modified version of the diagnostics visualization on the MALLET website. This includes various metrics for analyzing the quality of topic models. Users can generate diagnostics to visualize a single model or a comparative visualisation for multiple models. export The export module provides utilities for exporting data from a project or a project as a whole to a compressed tar.gz archive. The tar format is preferred to the zip format for entire projects because it preserves file permissions. This is less important for exports of the data itself. import The import module is main starting point for using the WE1S Workspace project. Use this module to import data into your project. Imported data is stored in the project_data folder. The import pipeline attempts to massage all data into a collection of JSON files, stored in the project_data/json folder. These JSON files contain both data and metadata as a series of key-value pairs that conform to the JSON file format. The names of the keys (also called \"fields\") conform to the WE1S manifest schema and certain metadata required by the tools in the WE1S Workspace. After import, the text content of your data will be stored in the JSON files' content field. JSON files are human-readable text files, which you can open and inspect. However, it is very easy to corrupt their format. If you ever suspect that this may have happened, you can paste the text into a JSON validator like jsonlint.com to check for errors. The import notebook requires either a zip archive of your data or a connection to a MongoDB database where the data is stored. Zip archives may take one of the following forms: A zip archive containing plain text data and an accompanying CSV file with relevant metadata. A zip archive containing data already in JSON format. A zip archive of a Frictionless Data data package containing data already in JSON format. If your metadata does not contain the field names required by the WE1S workspace, the import module allows you to map your metadata's field names onto the WE1S field names that comply with the WE1S manifest schema. Most modules in the Workspace assume that your project's json files contain schema-compliant fields. json_utilities This module provides helpful methods of accessing the contents of you project's json folder. This folder can be quite large, and may cause the browser to freeze if opened using the Jupyter notebook file browser. The json_utilities.ipynb notebook in this module provides methods for reading the contents of files in the json folder and for performing database-like queries on its contents to filter results. The remove_fields.ipynb notebook will remove specified field from all files in the json folder. It is intended primarily for creating sample data sets for testing or for removing content that might be subject to intellectual property restrictions. metadata The metadata module enables you to generate some basic statistics about document counts based on the metadata fields available in your project's json files. You can also use the `add_metada1 notebook to add custom metadata fields to your project's json files after the data has been imported. This module also contains a notebook for generating visualizations based on project data using the Scattertext library. pyldavis This module allows you to produce pyLDAvis visualizations of topic models produced using MALLET (the output of the topic_modeling module). pyLDAvis is a port of the R LDAvis package for interactive topic model visualization by Carson Sievert and Kenny Shirley. pyLDAvis is designed to help users interpret the topics in a topic model by examining the relevance and salience of terms in topics. Once a pyLDAvis object has been generated, many of its properties can be inspected as in tabular form as a way to examine the model. However, the main output is a visualization of the relevance and salience of key terms to the topics. pyLDAvis is not designed to use MALLET data out of the box. This notebook transforms the MALLET state file into the appropriate data formats before generating the visualization. The code is based on Jeri Wieringa's blog post Using pyLDAvis with Mallet and has been slightly altered and commented. topic_bubbles This module creates a topic bubbles visualization from dfr-browser data generated in the dfr-browser notebook or from model data generated in the topic_modeling module. This module uses scripts originally written by Sihwa Park for the WE1S project. For more information on Park's script, see Park's topic bubbles Github repo and the README.md located in this module's tb_scripts folder. topic_modeling This module uses MALLET to topic model project data. Prior to modeling, the notebooks extract word vectors from the project's json files into a single doc_terms.txt file, which is then imported into MALLET. Stop words are stripped at this stage. By default, the WE1S Standard Stoplist contained in the scripts folder is used for this process. After modeling is complete, there is an option to create a scaled data file for use in visualisations such as dfr-browser and pyLDAvis . utilities This module is really a container for miscellaneous \"helper\" notebooks that can be used to interact with the data in the project. clear_caches.ipynb provides various methods for deleting data, clearing notebook outputs, or resetting a project folder to its original state. zip_folder.ipynb provides code to save any folder as a zip archive so that it can be exported from the Workspace.","title":"Modules"},{"location":"modules/#modules","text":"This section provides brief overviews of the individual modules in the Workspace. More information and a User Guide for each module can be found on the individual module pages and in the README.md files inside each module folder.","title":"Modules"},{"location":"modules/#comparing","text":"The comparing module allows you to compare two sets of textual data to one another to discover how, and how much, they differ using the Wilcoxon rank sum test, a statistical test that determines if the relative frequencies of specific words in two populations are significantly different from one another (that is, they have different distributions). This helps you to determine what the most \"significant\" words in each dataset are. For more on the Wilcoxon rank sum test in the context of corpus analytics, see Jefrey Lijffijt, Terttu Nevalainen, Tanja S\u00e4ily, Panagiotis Papapetrou, Kai Puolam\u00e4ki, Heikki Mannila, \"Significance testing of word frequencies in corpora\", Digital Scholarship in the Humanities , Volume 31, Issue 2, June 2016, Pages 374\u2013397.","title":"comparing"},{"location":"modules/#counting","text":"The counting module contains notebooks for counting various aspects of project data. The notebooks allow you to count project documents ( count_documents.ipynb ), to count the number of documents containing a specific token ( docs_by_search_term.ipynb ), to calculate token frequencies ( frequency.ipynb ), to calculate tf-idf scores ( tfidf.ipynb ), to calculate various collocation metrics ( collocation.ipynb ), and to grab summary statistics of documents, tokens, etc in the project ( vocab.ipynb ). The vocab.ipynb notebook requires that your json data files contain a bag_of_words field with term counts. If you did not generate this field when you imported your data, you can do so using tokenize.ipynb , which leverages spaCy's tokenizer. The docs_by_search_term.ipynb , frequency.ipynb , tfidf.ipynb , and collocation.ipynb notebooks use a custom tokenizer based on the tokenizer available in NLTK . This differs from the tokenizer WE1S uses in its preprocessing and topic modeling pipelines, which only tokenizes unigrams. As a result, some features of these notebooks will not work if you do not have access to full-text data.","title":"counting"},{"location":"modules/#dendrogram","text":"The dendrogram module performs hierarchical agglomerative clustering on the topics of a topic model or multiple topic models in a project. Several distance metrics and linkage methods are available. The default settings will conform closely to the output of pyLDAvis and the scaled view of dfr-browser. Information on alternative distance metrics can be found here , and information on alternative linkage methods can be found here , in the documentation for the Python scipy package, which is used \"under the hood\". As dendrograms typically become crowded and hard to read, the visualizations are generated with Plotly , which allows them to be browsed interactively with its pan and zoom features.","title":"dendrogram"},{"location":"modules/#dfr_browser","text":"The dfr_browser module implements the creation and customization of Andrew Goldstone's dfr-browser from a topic model produced with MALLET (the output of the topic_modeling module). Dfr-browser code, stored in this module in dfrb_scripts , was written by Andrew Goldstone and adapted for WE1S use to provide an easy pipeline from data import to topic modeling to visualisation. WE1S uses an older version of Goldstone's code (v0.5.1) (see https://agoldst.github.io/dfr-browser/ for a version history). WE1S uses Goldstone's prepare_data.py Python script to prepare the data files, not Goldstone's R package.","title":"dfr_browser"},{"location":"modules/#diagnostics","text":"The diagnostics module produces a modified version of the diagnostics visualization on the MALLET website. This includes various metrics for analyzing the quality of topic models. Users can generate diagnostics to visualize a single model or a comparative visualisation for multiple models.","title":"diagnostics"},{"location":"modules/#export","text":"The export module provides utilities for exporting data from a project or a project as a whole to a compressed tar.gz archive. The tar format is preferred to the zip format for entire projects because it preserves file permissions. This is less important for exports of the data itself.","title":"export"},{"location":"modules/#import","text":"The import module is main starting point for using the WE1S Workspace project. Use this module to import data into your project. Imported data is stored in the project_data folder. The import pipeline attempts to massage all data into a collection of JSON files, stored in the project_data/json folder. These JSON files contain both data and metadata as a series of key-value pairs that conform to the JSON file format. The names of the keys (also called \"fields\") conform to the WE1S manifest schema and certain metadata required by the tools in the WE1S Workspace. After import, the text content of your data will be stored in the JSON files' content field. JSON files are human-readable text files, which you can open and inspect. However, it is very easy to corrupt their format. If you ever suspect that this may have happened, you can paste the text into a JSON validator like jsonlint.com to check for errors. The import notebook requires either a zip archive of your data or a connection to a MongoDB database where the data is stored. Zip archives may take one of the following forms: A zip archive containing plain text data and an accompanying CSV file with relevant metadata. A zip archive containing data already in JSON format. A zip archive of a Frictionless Data data package containing data already in JSON format. If your metadata does not contain the field names required by the WE1S workspace, the import module allows you to map your metadata's field names onto the WE1S field names that comply with the WE1S manifest schema. Most modules in the Workspace assume that your project's json files contain schema-compliant fields.","title":"import"},{"location":"modules/#json_utilities","text":"This module provides helpful methods of accessing the contents of you project's json folder. This folder can be quite large, and may cause the browser to freeze if opened using the Jupyter notebook file browser. The json_utilities.ipynb notebook in this module provides methods for reading the contents of files in the json folder and for performing database-like queries on its contents to filter results. The remove_fields.ipynb notebook will remove specified field from all files in the json folder. It is intended primarily for creating sample data sets for testing or for removing content that might be subject to intellectual property restrictions.","title":"json_utilities"},{"location":"modules/#metadata","text":"The metadata module enables you to generate some basic statistics about document counts based on the metadata fields available in your project's json files. You can also use the `add_metada1 notebook to add custom metadata fields to your project's json files after the data has been imported. This module also contains a notebook for generating visualizations based on project data using the Scattertext library.","title":"metadata"},{"location":"modules/#pyldavis","text":"This module allows you to produce pyLDAvis visualizations of topic models produced using MALLET (the output of the topic_modeling module). pyLDAvis is a port of the R LDAvis package for interactive topic model visualization by Carson Sievert and Kenny Shirley. pyLDAvis is designed to help users interpret the topics in a topic model by examining the relevance and salience of terms in topics. Once a pyLDAvis object has been generated, many of its properties can be inspected as in tabular form as a way to examine the model. However, the main output is a visualization of the relevance and salience of key terms to the topics. pyLDAvis is not designed to use MALLET data out of the box. This notebook transforms the MALLET state file into the appropriate data formats before generating the visualization. The code is based on Jeri Wieringa's blog post Using pyLDAvis with Mallet and has been slightly altered and commented.","title":"pyldavis"},{"location":"modules/#topic_bubbles","text":"This module creates a topic bubbles visualization from dfr-browser data generated in the dfr-browser notebook or from model data generated in the topic_modeling module. This module uses scripts originally written by Sihwa Park for the WE1S project. For more information on Park's script, see Park's topic bubbles Github repo and the README.md located in this module's tb_scripts folder.","title":"topic_bubbles"},{"location":"modules/#topic_modeling","text":"This module uses MALLET to topic model project data. Prior to modeling, the notebooks extract word vectors from the project's json files into a single doc_terms.txt file, which is then imported into MALLET. Stop words are stripped at this stage. By default, the WE1S Standard Stoplist contained in the scripts folder is used for this process. After modeling is complete, there is an option to create a scaled data file for use in visualisations such as dfr-browser and pyLDAvis .","title":"topic_modeling"},{"location":"modules/#utilities","text":"This module is really a container for miscellaneous \"helper\" notebooks that can be used to interact with the data in the project. clear_caches.ipynb provides various methods for deleting data, clearing notebook outputs, or resetting a project folder to its original state. zip_folder.ipynb provides code to save any folder as a zip archive so that it can be exported from the Workspace.","title":"utilities"},{"location":"modules/comparing/","text":"Comparing About This Module This module allows you to compare two sets of textual data to one another to discover how, and how much, they differ based on word frequency. The notebook performs this comparison using the Wilcoxon rank sum test, a statistical test that determines if two samples (i.e., the relative frequencies of a specific word in two different datasets) are taken from populations that are significantly different from one another (meaning, if they have different distributions). When used to compare word frequency data from two different datasets, it helps you to determine what the most \"significant\" words in each dataset are. For details on the Wilcoxon rank sum test, see this description by the University of Virgina Library. For implementations of the Wilcoxon rank sum test in literary studies, see Andrew Piper and Eva Portelace's article How Cultural Capital Works: Prizewinning Novels, Bestsellers, and the Time of Reading ( Post45 , 5.10.16) , and chapter 4 \"Fictionality\" from Andrew Piper's book Enumerations . User Guide This module has only one notebook: compare-word-frequencies.ipynb . Prior to beginning the workflow, you already must have two doc-terms files, one for each dataset you wish to compare. A doc-terms file is a text file representing the vocabulary in each document in the dataset. doc-terms files must contain rows of space-delimited columns beginning with the filename and index number (starting with 0), and then a list of every token in the document (with each instance of a token in its own column). Each row in each file should represent one document. A small example is given below: sample_document1.json 0 21st a a a an an absolutely abyss academic addition advance afghanistan... sample_document2.json 1 a a a an an an artificial artificial food food ingredients... As you can see in the example above, the data in these files should already be tokenized and (ideally) stripped of stop words. If you have already prepared your data for topic modeling, there will be a doc_terms.txt file in your project's project_data/models , which you can use as an example of the format. This notebook does not include any processes for tokenizing or stripping stop words from document data. Important The two doc-terms files you are comparing must not have overlapping filenames. The compare_word_frequencies.ipynb notebook provides 3 different methods to help users select data for this test: you can simply provide the filepaths to the doc-terms files you want to use; you can use a list of filenames to select only specific files from a given doc-terms file; and/or you can select a random sampling of files from a given doc-terms file. Each of these options is described in the notebook. Once you have your doc-terms files ready, you can prepare your data for the test and then run the test. Please refer to the notebook for additional instructions about how to run the test. Module Structure comparing \u2523 results \u2523 scripts \u2503 \u2523 compare_word_frequencies.py \u2523 compare-word-frequencies.ipynb \u2517 README.md","title":"comparing"},{"location":"modules/comparing/#comparing","text":"","title":"Comparing"},{"location":"modules/comparing/#about-this-module","text":"This module allows you to compare two sets of textual data to one another to discover how, and how much, they differ based on word frequency. The notebook performs this comparison using the Wilcoxon rank sum test, a statistical test that determines if two samples (i.e., the relative frequencies of a specific word in two different datasets) are taken from populations that are significantly different from one another (meaning, if they have different distributions). When used to compare word frequency data from two different datasets, it helps you to determine what the most \"significant\" words in each dataset are. For details on the Wilcoxon rank sum test, see this description by the University of Virgina Library. For implementations of the Wilcoxon rank sum test in literary studies, see Andrew Piper and Eva Portelace's article How Cultural Capital Works: Prizewinning Novels, Bestsellers, and the Time of Reading ( Post45 , 5.10.16) , and chapter 4 \"Fictionality\" from Andrew Piper's book Enumerations .","title":"About This Module"},{"location":"modules/comparing/#user-guide","text":"This module has only one notebook: compare-word-frequencies.ipynb . Prior to beginning the workflow, you already must have two doc-terms files, one for each dataset you wish to compare. A doc-terms file is a text file representing the vocabulary in each document in the dataset. doc-terms files must contain rows of space-delimited columns beginning with the filename and index number (starting with 0), and then a list of every token in the document (with each instance of a token in its own column). Each row in each file should represent one document. A small example is given below: sample_document1.json 0 21st a a a an an absolutely abyss academic addition advance afghanistan... sample_document2.json 1 a a a an an an artificial artificial food food ingredients... As you can see in the example above, the data in these files should already be tokenized and (ideally) stripped of stop words. If you have already prepared your data for topic modeling, there will be a doc_terms.txt file in your project's project_data/models , which you can use as an example of the format. This notebook does not include any processes for tokenizing or stripping stop words from document data. Important The two doc-terms files you are comparing must not have overlapping filenames. The compare_word_frequencies.ipynb notebook provides 3 different methods to help users select data for this test: you can simply provide the filepaths to the doc-terms files you want to use; you can use a list of filenames to select only specific files from a given doc-terms file; and/or you can select a random sampling of files from a given doc-terms file. Each of these options is described in the notebook. Once you have your doc-terms files ready, you can prepare your data for the test and then run the test. Please refer to the notebook for additional instructions about how to run the test.","title":"User Guide"},{"location":"modules/comparing/#module-structure","text":"comparing \u2523 results \u2523 scripts \u2503 \u2523 compare_word_frequencies.py \u2523 compare-word-frequencies.ipynb \u2517 README.md","title":"Module Structure"},{"location":"modules/counting/","text":"Counting About This Module This module contains notebooks for counting various aspects of project data. The notebooks allow you to count project documents ( count_documents.ipynb ), to count the number of documents containing a specific token ( docs_by_search_term.ipynb ), to calculate token frequencies ( frequency.ipynb ), to calculate tf-idf scores ( tfidf.ipynb ), to calculate various collocation metrics ( collocation.ipynb ), and to grab summary statistics of documents, tokens, etc in the project ( vocab.ipynb ). The vocab.ipynb notebook requires that your json data files contain a bag_of_words field with term counts. If you did not generate this field when you imported your data, you can do so using tokenize.ipynb , which leverages spaCy's tokenizer. The docs_by_search_term.ipynb , frequency.ipynb , tfidf.ipynb , and collocation.ipynb notebooks use a custom tokenizer based on the tokenizer available in NLTK . This differs from the tokenizer WE1S uses in its preprocessing and topic modeling pipelines, which only tokenizes unigrams. As a result, some features of these notebooks will not work if you do not have access to full-text data. Notebooks in this module allow users to configure their text input field -- in other words, you can tell the code where to look to find the text you want to process. You have three options for this: the content field, the bag_of_words field, or the features field. The code expects data in these fields to be in the following formats: content : Full, plain-text data, stored as a string in each document. bag_of_words : A bag of words dictionary, where each key is a unique unigram, and each value is a count of the number of times that token appears in the document. The import module allows users to create the bag_of_words field and add it to their project data. Data is alphabetized by default, meaning the bags are not reconstructable. features : This field is inserted by the WE1S preprocessor using spaCy, and the recommended content_field to use if working with WE1S public data. It is a list of lists that contains information of the following kinds about each token in the document: [\"TOKEN\", \"NORM\", \"LEMMA\", \"POS\", \"TAG\", \"STOPWORD\", \"ENTITIES\"] . NORM is a lowercased version of the token. LEMMA is the dictionary headword (so the lemma of \"going\" is \"go\"). POS is the part of speech according to spaCy's taxonomy . TAG is the equivalent in the Penn-Treebank system. ENTITIES is a named entity as classified here . Lemmas, POS, tags, and entities are all predicted by spaCy using its language model. STOPWORD is whether or not the lower case form of a token is classed as a stop word in a stoplist. For WE1S data, this is the WE1S Standard Stoplist. spaCy has its own stoplist, and users can also supply their own. Alphabetized by default. If your json documents do not have a content field (if you are using publicly released WE1S data, for instance), and you are using the bag_of_words or features field as your text input, you will not be able to use some of the functions available in some notebooks in this module. You will also only be able to count unigrams (since all word bags or features tables are alphabetized and thus bigrams and trigrams are not reconstructable). User Guide What follows are brief summaries of each notebook in this module. The notebooks themselves are flexible and have a wide range of functionality. For this reason, they are heavily documented and provide information about how to use them and what their different sections mean. Please refer to the notebooks for instructions about how to use the notebooks. docs_by_search_term.ipynb This notebook allows you to count the number of documents in a project containing a specific word or phrase. You can also save document metadata to a dataframe, which you can explore in the notebook or download to your own machine. This notebook also allows you to download the documents containing this word or phrase themselves as either json or txt files. frequency.ipynb This notebook provides methods for calculating raw and/or relative frequency values for ngrams (uni-, bi-, and/or trigrams are accepted) within a single document or across all of the project's documents. tfidf.ipynb Tf-idf, or term frequency - inverse document frequency, is a common way of measuring the importance of tokens both within a given document and across your project as a whole. You calculate a token's tf-idf score by multipling its relative frequency within a given document by the inverse of the number of documents that token appears in throughout the corpus. See TF-IDF from scratch in python on real world dataset for a more in-depth explanation of the math. Generally speaking, tokens with higher tf-idf scores (those closer to 1) are more important to a given document or corpus. At the document level, \"distinctive\" is a rough synonym for \"important;\" tf-idf provides a way to discover the tokens that are most distinctive within each document in your project. At the corpus or project level, a higher average tf-idf score means that a token is more frequently a distinctive word for documents within your corpus, i.e., it is potentially an important token for understanding your corpus overall. collocation.ipynb Collocation is another way of discussing co-occurrence; in natural language processing, the term \"collocation\" usually refers to phrases of two or more tokens that commonly occur together in a given context. You can use this notebook to understand how common certain bi- and trigrams are in your project. Generally speaking, the more tokens you have in your project, and the larger your project data is, the more meaningful these metrics will be. This notebook allows you to calculate five different collocation metrics: 1) Likelihood ratio; 2) Mutual information (MI) scores; 2) Pointwise mutual information (PMI) scores; 4) Student's t-test; and 5) Chi-squared test. See below for more information on each metric. Collocation metrics are only useful when you can tokenize on bi- and trigrams. Therefore, this notebook assumes your documents include full-text data, and that this data is stored as a string in the content field of each document. Likelihood Ratio Likelihood ratios reflect the likelihood, within a given corpus (i.e., all documents in the project), of a specific bi- or trigram occurring (technically it tells us the likelihood that any two or three given words exist in a dependent relationship). The higher a likelihood score, the more strongly associated the words composing the bi- or trigram are with one another, roughly speaking. Likelihood ratios usually perform better than t-tests or chi-squared tests (see below) on sparse data (i.e., bigrams and trigrams), and so are often used in natural language processing. The code below is an implementation of Dunning's log likelihood test. For more on likelihood ratios in natural language processing, see Foundations of Statistical Natural Language Processing , pages 161-164, and How to Interpret Python NLTK Bigram-Likelihood Ratios . Mutual Information (MI) Score The MI score is a measure of the strength of association between any given token in a project and all of the project's tokens. An MI score measures how much more likely the words of a bi- or trigram are to co-occur within your project than they are to occur by themselves. A higher score indicates a higher strength of association. For more on this concept, see this guide on the mutual information scores and t-tests (see below) in corpus linguistics. The code in this notebook implements NLTK's version of mutual information. See NLTK documentation here . MI scores are sensitive to unique words, which can make results less meaningful because often unique words will occur much less frequently throughout a corpus. Therefore, we recommend employing a frequency filter when calculating MI scores; the notebook includes this capability. Pointwise Mutual Information (PMI) Score PMI scores build on MI scores. Like MI scores, PMI scores measure the association between any given token in a project and all of the project's tokens. A PMI score measures how much more likely the words of a bi- or trigram are to co-occur within your project than they are to occur by themselves. A higher score indicates a higher strength of association. It differs from an MI score in that it refers to single comparisons, while MI scores are a measure of the average PMI scores over all comparisons. For more on this concept, see Gerlof Bouma's widely cited paper, Normalized (Pointwise) Mutual Information in Collocation Extraction . Like MI scores, PMI scores are sensitive to unique words, which can make results less meaningful because often unique words will occur much less frequently throughout a corpus. Therefore, we recommend employing a frequency filter when calculating MI scores; the notebook includes this capability. Student's T-test The student's t-test is perhaps one of the most widely used methods of hypothesis testing. This implementation of the t-test assumes that the words in any bi- or trigram are independent, and it measures how likely the words are to appear together in your project. Like the PMI score, a higher t-test score indicates a higher likelihood that the words in the bi- or trigram occur together in your project than that they occur separately. However, t-tests and chi-square tests (see below) have been shown to not perform as well with sparse data like bigrams and trigrams. You can find a good general discussion of what a t-test is here . Chi-Square Test The chi-square test is another test of statistical significance. Like a t-test, a chi-squared test assumes that the words in any bi- or trigram are independent. But unlike a t-test, a chi-squared test does not assume a normal distribution. The code below uses an implementation of Pearson's chi-square test of association. As with PMI and t-test scores, a higher chi-squared test score indicates a greater degree of likelihood, i.e., a higher likelihood that the words in a given bi- or trigram occur together in your project. Decent explanations of the chi-squared test can be found here and here . tokenize.ipynb Normally text analysis tools have to divide a text into countable \"tokens\" (most frequently words). This process is called tokenization. This cell allows you to pre-tokenize your data so that other tools do not need to take this step. It generates a dictionary of token-count pairs such as {\"cat\": 3, \"dog\": 2} for each of your JSON files. This dictionary is appended to the JSON file in the bag_of_words field. This notebook offers two tokenization methods. The default method is strips all non-alphanumeric characters and then divides the text into tokens on white space. Alternatively, you can use the spaCy Natural Language Processing library to tokenize based on spaCy's language model. spaCy extracts linguistic features from your text, not only tokens but parts of speech and named entities. This is instrinsically slower and may require a lot of memory for large texts. To use WE1S's custom spaCy tokenizer, set method='we1s' . If your text has been previously processed by spaCy and there is a features table in your JSON file, the tokenizer will attempt to use it to build the bag_of_words dictionary. Errors will be logged to the path you set for the log_file. vocab.ipynb This notebook allows you to build a single json vocab file containing term counts for all the documents in your project's json directory. It also allows you to access information about the vocab in a convenient manner. If your data does not already have bag_of_words fields, you should run tokenize.ipynb first. Module Structure counting \u2523 scripts \u2503 \u2523 count_docs.py \u2503 \u2523 count_tokens.py \u2503 \u2523 tokenizer.py \u2503 \u2523 vocab.py \u2523 collocation.ipynb \u2523 count_documents.ipynb \u2523 docs_by_search_term.ipynb \u2523 frequency.ipynb \u2523 tfidf.ipynb \u2523 README.md \u2523 tokenize.ipynb \u2517 vocab.ipynb","title":"counting"},{"location":"modules/counting/#counting","text":"","title":"Counting"},{"location":"modules/counting/#about-this-module","text":"This module contains notebooks for counting various aspects of project data. The notebooks allow you to count project documents ( count_documents.ipynb ), to count the number of documents containing a specific token ( docs_by_search_term.ipynb ), to calculate token frequencies ( frequency.ipynb ), to calculate tf-idf scores ( tfidf.ipynb ), to calculate various collocation metrics ( collocation.ipynb ), and to grab summary statistics of documents, tokens, etc in the project ( vocab.ipynb ). The vocab.ipynb notebook requires that your json data files contain a bag_of_words field with term counts. If you did not generate this field when you imported your data, you can do so using tokenize.ipynb , which leverages spaCy's tokenizer. The docs_by_search_term.ipynb , frequency.ipynb , tfidf.ipynb , and collocation.ipynb notebooks use a custom tokenizer based on the tokenizer available in NLTK . This differs from the tokenizer WE1S uses in its preprocessing and topic modeling pipelines, which only tokenizes unigrams. As a result, some features of these notebooks will not work if you do not have access to full-text data. Notebooks in this module allow users to configure their text input field -- in other words, you can tell the code where to look to find the text you want to process. You have three options for this: the content field, the bag_of_words field, or the features field. The code expects data in these fields to be in the following formats: content : Full, plain-text data, stored as a string in each document. bag_of_words : A bag of words dictionary, where each key is a unique unigram, and each value is a count of the number of times that token appears in the document. The import module allows users to create the bag_of_words field and add it to their project data. Data is alphabetized by default, meaning the bags are not reconstructable. features : This field is inserted by the WE1S preprocessor using spaCy, and the recommended content_field to use if working with WE1S public data. It is a list of lists that contains information of the following kinds about each token in the document: [\"TOKEN\", \"NORM\", \"LEMMA\", \"POS\", \"TAG\", \"STOPWORD\", \"ENTITIES\"] . NORM is a lowercased version of the token. LEMMA is the dictionary headword (so the lemma of \"going\" is \"go\"). POS is the part of speech according to spaCy's taxonomy . TAG is the equivalent in the Penn-Treebank system. ENTITIES is a named entity as classified here . Lemmas, POS, tags, and entities are all predicted by spaCy using its language model. STOPWORD is whether or not the lower case form of a token is classed as a stop word in a stoplist. For WE1S data, this is the WE1S Standard Stoplist. spaCy has its own stoplist, and users can also supply their own. Alphabetized by default. If your json documents do not have a content field (if you are using publicly released WE1S data, for instance), and you are using the bag_of_words or features field as your text input, you will not be able to use some of the functions available in some notebooks in this module. You will also only be able to count unigrams (since all word bags or features tables are alphabetized and thus bigrams and trigrams are not reconstructable).","title":"About This Module"},{"location":"modules/counting/#user-guide","text":"What follows are brief summaries of each notebook in this module. The notebooks themselves are flexible and have a wide range of functionality. For this reason, they are heavily documented and provide information about how to use them and what their different sections mean. Please refer to the notebooks for instructions about how to use the notebooks.","title":"User Guide"},{"location":"modules/counting/#docs_by_search_termipynb","text":"This notebook allows you to count the number of documents in a project containing a specific word or phrase. You can also save document metadata to a dataframe, which you can explore in the notebook or download to your own machine. This notebook also allows you to download the documents containing this word or phrase themselves as either json or txt files.","title":"docs_by_search_term.ipynb"},{"location":"modules/counting/#frequencyipynb","text":"This notebook provides methods for calculating raw and/or relative frequency values for ngrams (uni-, bi-, and/or trigrams are accepted) within a single document or across all of the project's documents.","title":"frequency.ipynb"},{"location":"modules/counting/#tfidfipynb","text":"Tf-idf, or term frequency - inverse document frequency, is a common way of measuring the importance of tokens both within a given document and across your project as a whole. You calculate a token's tf-idf score by multipling its relative frequency within a given document by the inverse of the number of documents that token appears in throughout the corpus. See TF-IDF from scratch in python on real world dataset for a more in-depth explanation of the math. Generally speaking, tokens with higher tf-idf scores (those closer to 1) are more important to a given document or corpus. At the document level, \"distinctive\" is a rough synonym for \"important;\" tf-idf provides a way to discover the tokens that are most distinctive within each document in your project. At the corpus or project level, a higher average tf-idf score means that a token is more frequently a distinctive word for documents within your corpus, i.e., it is potentially an important token for understanding your corpus overall.","title":"tfidf.ipynb"},{"location":"modules/counting/#collocationipynb","text":"Collocation is another way of discussing co-occurrence; in natural language processing, the term \"collocation\" usually refers to phrases of two or more tokens that commonly occur together in a given context. You can use this notebook to understand how common certain bi- and trigrams are in your project. Generally speaking, the more tokens you have in your project, and the larger your project data is, the more meaningful these metrics will be. This notebook allows you to calculate five different collocation metrics: 1) Likelihood ratio; 2) Mutual information (MI) scores; 2) Pointwise mutual information (PMI) scores; 4) Student's t-test; and 5) Chi-squared test. See below for more information on each metric. Collocation metrics are only useful when you can tokenize on bi- and trigrams. Therefore, this notebook assumes your documents include full-text data, and that this data is stored as a string in the content field of each document.","title":"collocation.ipynb"},{"location":"modules/counting/#likelihood-ratio","text":"Likelihood ratios reflect the likelihood, within a given corpus (i.e., all documents in the project), of a specific bi- or trigram occurring (technically it tells us the likelihood that any two or three given words exist in a dependent relationship). The higher a likelihood score, the more strongly associated the words composing the bi- or trigram are with one another, roughly speaking. Likelihood ratios usually perform better than t-tests or chi-squared tests (see below) on sparse data (i.e., bigrams and trigrams), and so are often used in natural language processing. The code below is an implementation of Dunning's log likelihood test. For more on likelihood ratios in natural language processing, see Foundations of Statistical Natural Language Processing , pages 161-164, and How to Interpret Python NLTK Bigram-Likelihood Ratios .","title":"Likelihood Ratio"},{"location":"modules/counting/#mutual-information-mi-score","text":"The MI score is a measure of the strength of association between any given token in a project and all of the project's tokens. An MI score measures how much more likely the words of a bi- or trigram are to co-occur within your project than they are to occur by themselves. A higher score indicates a higher strength of association. For more on this concept, see this guide on the mutual information scores and t-tests (see below) in corpus linguistics. The code in this notebook implements NLTK's version of mutual information. See NLTK documentation here . MI scores are sensitive to unique words, which can make results less meaningful because often unique words will occur much less frequently throughout a corpus. Therefore, we recommend employing a frequency filter when calculating MI scores; the notebook includes this capability.","title":"Mutual Information (MI) Score"},{"location":"modules/counting/#pointwise-mutual-information-pmi-score","text":"PMI scores build on MI scores. Like MI scores, PMI scores measure the association between any given token in a project and all of the project's tokens. A PMI score measures how much more likely the words of a bi- or trigram are to co-occur within your project than they are to occur by themselves. A higher score indicates a higher strength of association. It differs from an MI score in that it refers to single comparisons, while MI scores are a measure of the average PMI scores over all comparisons. For more on this concept, see Gerlof Bouma's widely cited paper, Normalized (Pointwise) Mutual Information in Collocation Extraction . Like MI scores, PMI scores are sensitive to unique words, which can make results less meaningful because often unique words will occur much less frequently throughout a corpus. Therefore, we recommend employing a frequency filter when calculating MI scores; the notebook includes this capability.","title":"Pointwise Mutual Information (PMI) Score"},{"location":"modules/counting/#students-t-test","text":"The student's t-test is perhaps one of the most widely used methods of hypothesis testing. This implementation of the t-test assumes that the words in any bi- or trigram are independent, and it measures how likely the words are to appear together in your project. Like the PMI score, a higher t-test score indicates a higher likelihood that the words in the bi- or trigram occur together in your project than that they occur separately. However, t-tests and chi-square tests (see below) have been shown to not perform as well with sparse data like bigrams and trigrams. You can find a good general discussion of what a t-test is here .","title":"Student's T-test"},{"location":"modules/counting/#chi-square-test","text":"The chi-square test is another test of statistical significance. Like a t-test, a chi-squared test assumes that the words in any bi- or trigram are independent. But unlike a t-test, a chi-squared test does not assume a normal distribution. The code below uses an implementation of Pearson's chi-square test of association. As with PMI and t-test scores, a higher chi-squared test score indicates a greater degree of likelihood, i.e., a higher likelihood that the words in a given bi- or trigram occur together in your project. Decent explanations of the chi-squared test can be found here and here .","title":"Chi-Square Test"},{"location":"modules/counting/#tokenizeipynb","text":"Normally text analysis tools have to divide a text into countable \"tokens\" (most frequently words). This process is called tokenization. This cell allows you to pre-tokenize your data so that other tools do not need to take this step. It generates a dictionary of token-count pairs such as {\"cat\": 3, \"dog\": 2} for each of your JSON files. This dictionary is appended to the JSON file in the bag_of_words field. This notebook offers two tokenization methods. The default method is strips all non-alphanumeric characters and then divides the text into tokens on white space. Alternatively, you can use the spaCy Natural Language Processing library to tokenize based on spaCy's language model. spaCy extracts linguistic features from your text, not only tokens but parts of speech and named entities. This is instrinsically slower and may require a lot of memory for large texts. To use WE1S's custom spaCy tokenizer, set method='we1s' . If your text has been previously processed by spaCy and there is a features table in your JSON file, the tokenizer will attempt to use it to build the bag_of_words dictionary. Errors will be logged to the path you set for the log_file.","title":"tokenize.ipynb"},{"location":"modules/counting/#vocabipynb","text":"This notebook allows you to build a single json vocab file containing term counts for all the documents in your project's json directory. It also allows you to access information about the vocab in a convenient manner. If your data does not already have bag_of_words fields, you should run tokenize.ipynb first.","title":"vocab.ipynb"},{"location":"modules/counting/#module-structure","text":"counting \u2523 scripts \u2503 \u2523 count_docs.py \u2503 \u2523 count_tokens.py \u2503 \u2523 tokenizer.py \u2503 \u2523 vocab.py \u2523 collocation.ipynb \u2523 count_documents.ipynb \u2523 docs_by_search_term.ipynb \u2523 frequency.ipynb \u2523 tfidf.ipynb \u2523 README.md \u2523 tokenize.ipynb \u2517 vocab.ipynb","title":"Module Structure"},{"location":"modules/dendrogram/","text":"Dendrogram About This Module This module performs hierarchical agglomerative clustering on the topics of a topic model or multiple topic models in a project. Several distance metrics and linkage methods are available. The default settings will conform closely to the output of pyLDAvis and the scaled view of dfr-browser. Information on alternative distance metrics can be found here and information on alternative linkage methods can be found here . As dendrograms typically become crowded and hard to read, the visualisations are generated with Plotly, so that they can be browsed interactively with its pan and zoom features. Module Organization Plotly exports dendrograms as HTML <div> elements which can be inserted into web pages. These are stored in the partials folder as .html files. The dendrogram module functions can then access these elements by inserting them into web pages that load Plotly's Javascript functions off the internet. Because these web pages access the internet, they will only work in a server environment. The dendrogram module also allows you to combine the partials and Plotly Javascript in a single, standalone HTML file that will work without an server environment, but they can be very large (several megabytes). The module consists of two notebooks, create_dendrogram.ipynb , which allows you to create single dendrograms, and batch_dendrogram.ipynb , which allows you to create multiple dendrograms at once. batch_dendrogram.ipynb also enables you to generate an html index file for swapping between visualisations. User Guide create_dendrogram.ipynb Setup Imports Python libaries and scripts to the notebook and defines important file paths. Configuration To select the model you want to produce a dendrogram for: Navigate to the your_project_name/project_data/models directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which model you would like to produce a dendrogram visualization for, change the value of selection in the cell below to the corresponding subdirectory. For example, if you wanted to produce a dendrogram for the 50-topic model you created, change the value of selection below to this: selection = 'topics50' Please follow this format exactly. Note You can select only one model to produce a dendrogram for at a time. The dendrogram will be saved to the name you set for the filename configuration. It should end in .html . The file will be saved to the module's partials folder. It is often useful to give files names like topics25-euclidean-single.html to indicate the number of topics, distance metric, and linkage method used for the cluster analysis. This is especially helpful if you want to run the Create Index for Multiple Dendrograms section below. Set the distance metric to euclidean and cosine . The linkage method may be single , complete , average , or ward . Ward linkage requires that the distance metric be set to euclidean . Important The default output will only work in a server environment. If you wish to create a standalone version that can be run on a local computer, set standalone=True . The disadvantage of this method is that the file will be large, about 3 MB. Load Data from the MALLET State File This cell instantiates a Model object and loads the model's data from the MALLET state file. Cluster the Model By default, the cluster wil be saved as an html div element in the partials folder. Create Web Page for a Single Dendrogram This cell generates a web page that displays a single dendrogram. This web page will not work without an internet connection. If you wish to download a copy that does not require an internet connection, set the standalone configuration to True in the Configuration section and re-run the cluster analysis. If you have already produced multiple dendrograms and wish to publish them with an index page, skip to the next section. Create Index Page for Multiple Dendrograms This section will produce an index page allowing you to navigate between multiple dendrogram files, which you have already created. Make sure that you configure the settings as described below. The source_filenames must be the same as dendrogram filenames in your partials folder. The menu_items and dendrogram_titles lists should correspond to the order of the source_filenames . The former will appear as menu labels for navigating between dendrograms, and the latter will be titles for each dendrogram. If you would like to save the index and dendrograms to a zip archive for export, set zip=True . Important The default output will only work in a server environment. Generate the Index Page This cell will display a link to your index page. batch_dendrogram.ipynb This notebook allows you to perform hierarchical cluster analysis on multiple models with multiple clustering options. The output is an HTML index file which allows you to display the generated cluster analyses as dendrograms. The last (optional) cell in this notebook allows you to generate standalone HTML files for a list of already-generated dendrograms. Setup Imports Python libaries and scripts to the notebook and defines important file paths. Configuration Provide a list of all models you wish to cluster and the distance metrics and linkage methods you wish to apply to each of the models. Set models = [] if you wish to cluster all the models available in our project. Otherwise, provide a list of the folder names for each model you wish to cluster. Available distance metrics are 'euclidean' and 'cosine'. Available linkage methods are 'average', 'single', 'complete', and 'ward'. You will most likely not need to adjust the advanced configuration options, but their use is described below: orientation ('top', 'left', 'bottom', 'right'): The location of the dendrogram root height : The height of the dendrogram in pixels width : The width of the dendrogram in pixels hovertext : A list of hovertext for constituent traces of dendrogram clusters truncate_mode : The dendrogram can be hard to read when the original observation matrix from which the linkage is derived is large. Truncation is used to condense the dendrogram. There are several modes: None (no truncation, the default), lastp (the last p non-singleton clusters formed in the linkage are the only non-leaf nodes in the linkage), 'level' (No more than p levels of the dendrogram tree are displayed). p : The p parameter for truncate_mode color_threshold : The value at which all descendent links below a cluster node will be given the same colour For further details, see the scipy.cluster.hierarchy.dendrogram documentation Cluster This cell starts the cluster analysis and can be run without modification. Create Standalone Dendrograms Run the cells in this section if you wish to create standalone versions of any of the dendrograms you have already created. They will be saved into your project's dendrogram module folder. The dendrograms can be downloaded and will work locally, as long as you have an internet connection. You only need to configure the list of dendrogram names you wish to save. Module Structure dendrogram \u2523 partials \u2523 scripts \u2503 \u2523 batch_cluster.py \u2503 \u2523 index_template.html \u2503 \u2523 model.py \u2503 \u2517 standalone.py \u2523 batch_dendrogram.ipynb \u2523 create_dendrogram.ipynb \u2517 README.md","title":"dendrogram"},{"location":"modules/dendrogram/#dendrogram","text":"","title":"Dendrogram"},{"location":"modules/dendrogram/#about-this-module","text":"This module performs hierarchical agglomerative clustering on the topics of a topic model or multiple topic models in a project. Several distance metrics and linkage methods are available. The default settings will conform closely to the output of pyLDAvis and the scaled view of dfr-browser. Information on alternative distance metrics can be found here and information on alternative linkage methods can be found here . As dendrograms typically become crowded and hard to read, the visualisations are generated with Plotly, so that they can be browsed interactively with its pan and zoom features.","title":"About This Module"},{"location":"modules/dendrogram/#module-organization","text":"Plotly exports dendrograms as HTML <div> elements which can be inserted into web pages. These are stored in the partials folder as .html files. The dendrogram module functions can then access these elements by inserting them into web pages that load Plotly's Javascript functions off the internet. Because these web pages access the internet, they will only work in a server environment. The dendrogram module also allows you to combine the partials and Plotly Javascript in a single, standalone HTML file that will work without an server environment, but they can be very large (several megabytes). The module consists of two notebooks, create_dendrogram.ipynb , which allows you to create single dendrograms, and batch_dendrogram.ipynb , which allows you to create multiple dendrograms at once. batch_dendrogram.ipynb also enables you to generate an html index file for swapping between visualisations.","title":"Module Organization"},{"location":"modules/dendrogram/#user-guide","text":"","title":"User Guide"},{"location":"modules/dendrogram/#create_dendrogramipynb","text":"","title":"create_dendrogram.ipynb"},{"location":"modules/dendrogram/#setup","text":"Imports Python libaries and scripts to the notebook and defines important file paths.","title":"Setup"},{"location":"modules/dendrogram/#configuration","text":"To select the model you want to produce a dendrogram for: Navigate to the your_project_name/project_data/models directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which model you would like to produce a dendrogram visualization for, change the value of selection in the cell below to the corresponding subdirectory. For example, if you wanted to produce a dendrogram for the 50-topic model you created, change the value of selection below to this: selection = 'topics50' Please follow this format exactly. Note You can select only one model to produce a dendrogram for at a time. The dendrogram will be saved to the name you set for the filename configuration. It should end in .html . The file will be saved to the module's partials folder. It is often useful to give files names like topics25-euclidean-single.html to indicate the number of topics, distance metric, and linkage method used for the cluster analysis. This is especially helpful if you want to run the Create Index for Multiple Dendrograms section below. Set the distance metric to euclidean and cosine . The linkage method may be single , complete , average , or ward . Ward linkage requires that the distance metric be set to euclidean . Important The default output will only work in a server environment. If you wish to create a standalone version that can be run on a local computer, set standalone=True . The disadvantage of this method is that the file will be large, about 3 MB.","title":"Configuration"},{"location":"modules/dendrogram/#load-data-from-the-mallet-state-file","text":"This cell instantiates a Model object and loads the model's data from the MALLET state file.","title":"Load Data from the MALLET State File"},{"location":"modules/dendrogram/#cluster-the-model","text":"By default, the cluster wil be saved as an html div element in the partials folder.","title":"Cluster the Model"},{"location":"modules/dendrogram/#create-web-page-for-a-single-dendrogram","text":"This cell generates a web page that displays a single dendrogram. This web page will not work without an internet connection. If you wish to download a copy that does not require an internet connection, set the standalone configuration to True in the Configuration section and re-run the cluster analysis. If you have already produced multiple dendrograms and wish to publish them with an index page, skip to the next section.","title":"Create Web Page for a Single Dendrogram"},{"location":"modules/dendrogram/#create-index-page-for-multiple-dendrograms","text":"This section will produce an index page allowing you to navigate between multiple dendrogram files, which you have already created. Make sure that you configure the settings as described below. The source_filenames must be the same as dendrogram filenames in your partials folder. The menu_items and dendrogram_titles lists should correspond to the order of the source_filenames . The former will appear as menu labels for navigating between dendrograms, and the latter will be titles for each dendrogram. If you would like to save the index and dendrograms to a zip archive for export, set zip=True . Important The default output will only work in a server environment.","title":"Create Index Page for Multiple Dendrograms"},{"location":"modules/dendrogram/#generate-the-index-page","text":"This cell will display a link to your index page.","title":"Generate the Index Page"},{"location":"modules/dendrogram/#batch_dendrogramipynb","text":"This notebook allows you to perform hierarchical cluster analysis on multiple models with multiple clustering options. The output is an HTML index file which allows you to display the generated cluster analyses as dendrograms. The last (optional) cell in this notebook allows you to generate standalone HTML files for a list of already-generated dendrograms.","title":"batch_dendrogram.ipynb"},{"location":"modules/dendrogram/#setup_1","text":"Imports Python libaries and scripts to the notebook and defines important file paths.","title":"Setup"},{"location":"modules/dendrogram/#configuration_1","text":"Provide a list of all models you wish to cluster and the distance metrics and linkage methods you wish to apply to each of the models. Set models = [] if you wish to cluster all the models available in our project. Otherwise, provide a list of the folder names for each model you wish to cluster. Available distance metrics are 'euclidean' and 'cosine'. Available linkage methods are 'average', 'single', 'complete', and 'ward'. You will most likely not need to adjust the advanced configuration options, but their use is described below: orientation ('top', 'left', 'bottom', 'right'): The location of the dendrogram root height : The height of the dendrogram in pixels width : The width of the dendrogram in pixels hovertext : A list of hovertext for constituent traces of dendrogram clusters truncate_mode : The dendrogram can be hard to read when the original observation matrix from which the linkage is derived is large. Truncation is used to condense the dendrogram. There are several modes: None (no truncation, the default), lastp (the last p non-singleton clusters formed in the linkage are the only non-leaf nodes in the linkage), 'level' (No more than p levels of the dendrogram tree are displayed). p : The p parameter for truncate_mode color_threshold : The value at which all descendent links below a cluster node will be given the same colour For further details, see the scipy.cluster.hierarchy.dendrogram documentation","title":"Configuration"},{"location":"modules/dendrogram/#cluster","text":"This cell starts the cluster analysis and can be run without modification.","title":"Cluster"},{"location":"modules/dendrogram/#create-standalone-dendrograms","text":"Run the cells in this section if you wish to create standalone versions of any of the dendrograms you have already created. They will be saved into your project's dendrogram module folder. The dendrograms can be downloaded and will work locally, as long as you have an internet connection. You only need to configure the list of dendrogram names you wish to save.","title":"Create Standalone Dendrograms"},{"location":"modules/dendrogram/#module-structure","text":"dendrogram \u2523 partials \u2523 scripts \u2503 \u2523 batch_cluster.py \u2503 \u2523 index_template.html \u2503 \u2523 model.py \u2503 \u2517 standalone.py \u2523 batch_dendrogram.ipynb \u2523 create_dendrogram.ipynb \u2517 README.md","title":"Module Structure"},{"location":"modules/dfr-browser/","text":"Dfr-Browser About This Module The notebooks in this module implement the creation and customization of Andrew Goldstone's dfr-browser from a topic model produced with MALLET. Dfr-browser code, stored in this module in dfrb_scripts , was written by Andrew Goldstone and adapted for WE1S use and data. WE1S uses an older version of Goldstone's code (v0.5.1); see https://agoldst.github.io/dfr-browser/ for a version history of Goldstone's code. WE1S uses Goldstone's prepare_data.py Python script to prepare the data files, NOT the R package. This module has two notebooks: create_dfrbrowser.ipynb for creating dfr-browsers and customize_dfrbrowser.ipynb for customizing the dfr-browser's display. User Guide create_dfrbrowser.ipynb Settings The Settings cell defines paths and important variables used to create a Dfr-Browser visualization. The default settings will create a folder inside the dfr_browser module for each topic model in your project or for a selection of models. In most cases, you will not need to change the default settings. Create Dfr-Browser Metadata Files from JSON Files The dfrb_metadata() function opens up each json in your project's json directory and grabs the metadata information dfr-browser needs. It creates both the metadata_csv_file file and the browser_meta_file_temp file. Create Browser: Create files needed for Dfr-browser By default, this notebook is set to create Dfr-browsers for all of the models you produced using the topic_modeling module. If you would like to select only certain models to produce Dfr-browsers for, make those selections in the next cell (see next paragraph). Otherwise leave the value in the next cell set to All , which is the default. To produce browsers for a selection of the models you created, but not all: Navigate to the your_project_name/project_data/models directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which subdirectory/ies you would like to produce browsers for, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to to selection = ['topics50','topics75'] . The get_model_state() function grabs the filepaths of model subdirectories in order to visualize and their state and scaled files. Optionally, you can instead set values for subdir_list , state_file_list , and scaled_file_list manually in the second cell. The create_dfrbrowser() function creates the files needed for Dfr-browser, using the model state and scaled files for all selected models. It prints output from Goldstone's prepare_data.py script to the notebook cell. Once your Dfr-browser(s) have been created, the display_links() function displays links to open the Dfr-browser(s) in a tab in your web browser. Create Zipped Copies of Your Visualizations (Optional) This section zips up your dfr-browser visualizations for serving on a different machine or server. By default, browsers for all available models will be zipped. If you wish to zip only one model, change the models setting to indicate the name of the model folder (e.g. 'topics25' ). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. ['topics25', 'topics50'] ). This section also includes instructions for downloading and running your dfrbrowser visualization(s) on a different machine (i.e., outside of the WE1S container system). customize_dfrbrowser.ipynb The customize_dfrbrowser.ipynb notebook allows you to customize certain Dfr-browser settings. You can only customize 1 topic model at a time. You can customize the title, names and contact info for contributors, a description of the model, a list of custom metadata fields, the number of words to display in topic bubbles, the font size of topic labels, the number of documents to display in Topic View, and the topic labels. This notebook edits your Dfr-browser's info.json file. You can also just edit this file manually to make these customizations. For more information on how to do this, see Goldstone's documentation here: https://github.com/agoldst/dfr-browser#tune-the-visualization-parameters. Module Structure dfr_browser \u2523 scripts \u2503 \u2503 \u2523 create_dfrbrowser.py \u2503 \u2503 \u2517 zip.py \u2523 dfrb_scripts \u2503 \u2523 bin \u2503 \u2503 \u2523 prepare-data \u2503 \u2503 \u2517 server \u2503 \u2523 css \u2503 \u2503 \u2523 bootstrap-theme.min.css \u2503 \u2503 \u2523 bootstrap.min.css \u2503 \u2503 \u2517 index.css \u2503 \u2523 fonts \u2503 \u2503 \u2523 glyphicons-halflings-regular.eot \u2503 \u2503 \u2523 glyphicons-halflings-regular.svg \u2503 \u2503 \u2523 glyphicons-halflings-regular.ttf \u2503 \u2503 \u2517 glyphicons-halflings-regular.woff \u2503 \u2523 img \u2503 \u2503 \u2517 loading.gif \u2503 \u2523 js \u2503 \u2503 \u2523 d3-mouse-event.js \u2503 \u2503 \u2523 dfb.min.js.custom \u2503 \u2503 \u2523 utils.min.js \u2503 \u2503 \u2517 worker.min.js \u2503 \u2523 lib \u2503 \u2503 \u2523 bootstrap.min.js \u2503 \u2503 \u2523 d3.min.js \u2503 \u2503 \u2523 query-1.11.0.min.js \u2503 \u2503 \u2517 jszip.min.js \u2503 \u2523 index.html \u2503 \u2517 LICENSE \u2523 create_dfrbrowser.ipynb \u2523 customize_dfrbrowser.ipynb \u2517 README.md","title":"dfr_browser"},{"location":"modules/dfr-browser/#dfr-browser","text":"","title":"Dfr-Browser"},{"location":"modules/dfr-browser/#about-this-module","text":"The notebooks in this module implement the creation and customization of Andrew Goldstone's dfr-browser from a topic model produced with MALLET. Dfr-browser code, stored in this module in dfrb_scripts , was written by Andrew Goldstone and adapted for WE1S use and data. WE1S uses an older version of Goldstone's code (v0.5.1); see https://agoldst.github.io/dfr-browser/ for a version history of Goldstone's code. WE1S uses Goldstone's prepare_data.py Python script to prepare the data files, NOT the R package. This module has two notebooks: create_dfrbrowser.ipynb for creating dfr-browsers and customize_dfrbrowser.ipynb for customizing the dfr-browser's display.","title":"About This Module"},{"location":"modules/dfr-browser/#user-guide","text":"","title":"User Guide"},{"location":"modules/dfr-browser/#create_dfrbrowseripynb","text":"","title":"create_dfrbrowser.ipynb"},{"location":"modules/dfr-browser/#settings","text":"The Settings cell defines paths and important variables used to create a Dfr-Browser visualization. The default settings will create a folder inside the dfr_browser module for each topic model in your project or for a selection of models. In most cases, you will not need to change the default settings.","title":"Settings"},{"location":"modules/dfr-browser/#create-dfr-browser-metadata-files-from-json-files","text":"The dfrb_metadata() function opens up each json in your project's json directory and grabs the metadata information dfr-browser needs. It creates both the metadata_csv_file file and the browser_meta_file_temp file.","title":"Create Dfr-Browser Metadata Files from JSON Files"},{"location":"modules/dfr-browser/#create-browser-create-files-needed-for-dfr-browser","text":"By default, this notebook is set to create Dfr-browsers for all of the models you produced using the topic_modeling module. If you would like to select only certain models to produce Dfr-browsers for, make those selections in the next cell (see next paragraph). Otherwise leave the value in the next cell set to All , which is the default. To produce browsers for a selection of the models you created, but not all: Navigate to the your_project_name/project_data/models directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which subdirectory/ies you would like to produce browsers for, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to to selection = ['topics50','topics75'] . The get_model_state() function grabs the filepaths of model subdirectories in order to visualize and their state and scaled files. Optionally, you can instead set values for subdir_list , state_file_list , and scaled_file_list manually in the second cell. The create_dfrbrowser() function creates the files needed for Dfr-browser, using the model state and scaled files for all selected models. It prints output from Goldstone's prepare_data.py script to the notebook cell. Once your Dfr-browser(s) have been created, the display_links() function displays links to open the Dfr-browser(s) in a tab in your web browser.","title":"Create Browser: Create files needed for Dfr-browser"},{"location":"modules/dfr-browser/#create-zipped-copies-of-your-visualizations-optional","text":"This section zips up your dfr-browser visualizations for serving on a different machine or server. By default, browsers for all available models will be zipped. If you wish to zip only one model, change the models setting to indicate the name of the model folder (e.g. 'topics25' ). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. ['topics25', 'topics50'] ). This section also includes instructions for downloading and running your dfrbrowser visualization(s) on a different machine (i.e., outside of the WE1S container system).","title":"Create Zipped Copies of Your Visualizations (Optional)"},{"location":"modules/dfr-browser/#customize_dfrbrowseripynb","text":"The customize_dfrbrowser.ipynb notebook allows you to customize certain Dfr-browser settings. You can only customize 1 topic model at a time. You can customize the title, names and contact info for contributors, a description of the model, a list of custom metadata fields, the number of words to display in topic bubbles, the font size of topic labels, the number of documents to display in Topic View, and the topic labels. This notebook edits your Dfr-browser's info.json file. You can also just edit this file manually to make these customizations. For more information on how to do this, see Goldstone's documentation here: https://github.com/agoldst/dfr-browser#tune-the-visualization-parameters.","title":"customize_dfrbrowser.ipynb"},{"location":"modules/dfr-browser/#module-structure","text":"dfr_browser \u2523 scripts \u2503 \u2503 \u2523 create_dfrbrowser.py \u2503 \u2503 \u2517 zip.py \u2523 dfrb_scripts \u2503 \u2523 bin \u2503 \u2503 \u2523 prepare-data \u2503 \u2503 \u2517 server \u2503 \u2523 css \u2503 \u2503 \u2523 bootstrap-theme.min.css \u2503 \u2503 \u2523 bootstrap.min.css \u2503 \u2503 \u2517 index.css \u2503 \u2523 fonts \u2503 \u2503 \u2523 glyphicons-halflings-regular.eot \u2503 \u2503 \u2523 glyphicons-halflings-regular.svg \u2503 \u2503 \u2523 glyphicons-halflings-regular.ttf \u2503 \u2503 \u2517 glyphicons-halflings-regular.woff \u2503 \u2523 img \u2503 \u2503 \u2517 loading.gif \u2503 \u2523 js \u2503 \u2503 \u2523 d3-mouse-event.js \u2503 \u2503 \u2523 dfb.min.js.custom \u2503 \u2503 \u2523 utils.min.js \u2503 \u2503 \u2517 worker.min.js \u2503 \u2523 lib \u2503 \u2503 \u2523 bootstrap.min.js \u2503 \u2503 \u2523 d3.min.js \u2503 \u2503 \u2523 query-1.11.0.min.js \u2503 \u2503 \u2517 jszip.min.js \u2503 \u2523 index.html \u2503 \u2517 LICENSE \u2523 create_dfrbrowser.ipynb \u2523 customize_dfrbrowser.ipynb \u2517 README.md","title":"Module Structure"},{"location":"modules/diagnostics/","text":"Diagnostics About This Module This notebook produces a modified version of the diagnostics visualisation on the MALLET website. A single model will be viewable as a web page called index.html . The notebook also produces a comparative visualisation file for multiple models called comparison.html . The main notebook for this module is visualize_diagnostics.ipynb . User Guide This module assembles the MALLET diagnostics xml filse together with assets for a web-based visualization of the contents. Because it does not generate any information itself, it simply outputs a link to the visualization index file. Create Diagnostics Visualizations This cell copies all of the diagnostics xml files from the diagnostics module directory and generates two web pages called index.html and comparison.html . Opening index.html on the public visualization port (a link is created by the notebook) launches the visualizations. Instructions for using the visualizations can be viewed by clicking \"About This Tool\" in the menu. The \"Model Comparison Tool\" menu item switches to the comparison view, from which the \"Individual Model Tool\" will take you back to the single-model visualization. Important In the Model Comparison Tool, one or two scatterplots may sometimes fail to load due to other browser activity. Usually doing a hard refresh of the page will allow them to load. Zip Diagnostics This optional cell The second cell optionally creates a zip archive of the visualization, which is suitable for export, in the module directory. Module Structure diagnostics \u2523 css \u2503 \u2503 bootstrap.min.css \u2503 \u2503 all.min.css \u2503 \u2503 styles.css \u2503 \u2517 bootstrap.min.css \u2523 js \u2503 \u2523 bootstrap.min.js \u2503 \u2523 d3.v3.min.js \u2503 \u2523 jquery-3.4.1.slim.min.js \u2503 \u2517 popper.min.js \u2523 scripts \u2503 \u2523 comparison_template.html \u2503 \u2523 diagnostics.py \u2503 \u2523 index_template.html \u2503 \u2523 zip.py \u2523 webfonts \u2503 \u2523 fa-solid-900.woff2 \u2503 \u2523 fa-solid-900.woff \u2503 \u2523 fa-solid-900.ttf \u2503 \u2523 fa-solid-900.svg \u2503 \u2523 fa-solid-900.eot \u2503 \u2523 fa-regular-400.woff2 \u2503 \u2523 fa-regular-400.woff \u2503 \u2523 fa-regular-400.ttf \u2503 \u2523 fa-regular-400.svg \u2503 \u2523 fa-regular-400.eot \u2503 \u2523 fa-brands-400.woff2 \u2503 \u2523 fa-brands-400.woff \u2503 \u2523 fa-brands-400.ttf \u2503 \u2523 fa-brands-400.svg \u2503 \u2517 fa-brands-400.eot \u2523 xml \u2523 README.md \u2517 visualize_diagnostics.ipynb","title":"diagnostics"},{"location":"modules/diagnostics/#diagnostics","text":"","title":"Diagnostics"},{"location":"modules/diagnostics/#about-this-module","text":"This notebook produces a modified version of the diagnostics visualisation on the MALLET website. A single model will be viewable as a web page called index.html . The notebook also produces a comparative visualisation file for multiple models called comparison.html . The main notebook for this module is visualize_diagnostics.ipynb .","title":"About This Module"},{"location":"modules/diagnostics/#user-guide","text":"This module assembles the MALLET diagnostics xml filse together with assets for a web-based visualization of the contents. Because it does not generate any information itself, it simply outputs a link to the visualization index file.","title":"User Guide"},{"location":"modules/diagnostics/#create-diagnostics-visualizations","text":"This cell copies all of the diagnostics xml files from the diagnostics module directory and generates two web pages called index.html and comparison.html . Opening index.html on the public visualization port (a link is created by the notebook) launches the visualizations. Instructions for using the visualizations can be viewed by clicking \"About This Tool\" in the menu. The \"Model Comparison Tool\" menu item switches to the comparison view, from which the \"Individual Model Tool\" will take you back to the single-model visualization. Important In the Model Comparison Tool, one or two scatterplots may sometimes fail to load due to other browser activity. Usually doing a hard refresh of the page will allow them to load.","title":"Create Diagnostics Visualizations"},{"location":"modules/diagnostics/#zip-diagnostics","text":"This optional cell The second cell optionally creates a zip archive of the visualization, which is suitable for export, in the module directory.","title":"Zip Diagnostics"},{"location":"modules/diagnostics/#module-structure","text":"diagnostics \u2523 css \u2503 \u2503 bootstrap.min.css \u2503 \u2503 all.min.css \u2503 \u2503 styles.css \u2503 \u2517 bootstrap.min.css \u2523 js \u2503 \u2523 bootstrap.min.js \u2503 \u2523 d3.v3.min.js \u2503 \u2523 jquery-3.4.1.slim.min.js \u2503 \u2517 popper.min.js \u2523 scripts \u2503 \u2523 comparison_template.html \u2503 \u2523 diagnostics.py \u2503 \u2523 index_template.html \u2503 \u2523 zip.py \u2523 webfonts \u2503 \u2523 fa-solid-900.woff2 \u2503 \u2523 fa-solid-900.woff \u2503 \u2523 fa-solid-900.ttf \u2503 \u2523 fa-solid-900.svg \u2503 \u2523 fa-solid-900.eot \u2503 \u2523 fa-regular-400.woff2 \u2503 \u2523 fa-regular-400.woff \u2503 \u2523 fa-regular-400.ttf \u2503 \u2523 fa-regular-400.svg \u2503 \u2523 fa-regular-400.eot \u2503 \u2523 fa-brands-400.woff2 \u2503 \u2523 fa-brands-400.woff \u2503 \u2523 fa-brands-400.ttf \u2503 \u2523 fa-brands-400.svg \u2503 \u2517 fa-brands-400.eot \u2523 xml \u2523 README.md \u2517 visualize_diagnostics.ipynb","title":"Module Structure"},{"location":"modules/export/","text":"Export About This Module The export module provides utilities for exporting data from a project or a project as a whole to a compressed tar archive. The tar format is preferred to the zip format for entire projects because it preserves file permissions. This is less important for exports of the data itself. The module has two notebooks: export_project.ipynb for exporting an entire project and json_to_txt_csv.ipynb for exporting files from the project json folder to a directory of plain text files with an accompanying metadata CSV file. User Guide Export Project This notebook provides the ability to export an entire project to a single file in the form of a .tar.gz archive. File size can be reduced by setting a list of folders for exclusion. Once the archive has been created, its location can optionally be recorded in a MongoDB database. (Eventually it will be possible to store the archive in the database, but this feature is not yet available. Configuration The notebook expects the following values in the Configuration cell: name : The name of the project archive author : The name of the author of the archive version : The version number save_path : The filepath where the archive will be save (including filename) exclude (optional): List of folder paths to ignore. Paths should be relative to the project folder without a leading '/'. client (optional): The url of the MongoDB client database (optional): The name of a MongoDB database collection (optional): The name of a MongoDB database If you are not working with MongoDB, leave these the client , database , and collection set to None . Build Data Package This cell instantiates the ExportPackage object and builds a Frictionless Data data package detailing the project's resources. If the project directory contains a datapackage.json and/or README.md , a datetime stamp will be added; otherwise, these files will be created. Once the data package is built, it is possible to access it with export_package.datapackage , and the README text can be accessed with export_package.readme . Make Archive This cell creates the archive file. It can be run without modification. Extract Archive The last two cells can be used to extract an existing project archive file to a project folder. Before running the last cell set the following configurations: archive_file : The path the archive file to be extracted destination_dir : The path to the project folder where the project will be extracted. If the folder does not exist, it will be created. remove_archive : By default, the archive file copied to the project folder (not the original one) will be deleted after it is extracted. If you wish to retain it, set remove_archive=False . json_to_txt_csv The WE1S workflows use JSON format internally for manipulating data. However, you may wish to export JSON data from a project to plain text files with a CSV metadata file for use with other external tools. This notebook uses JSON project data to export a collection of plain txt files \u2014 one per JSON document \u2014 containing only the document contents field or bag of words. Each file is named with the name of the JSON document and a .txt extension. It also produces a metadata.csv file. This file contains a header and one row per document with the document filename plus required fields. Output from this notebook can be imported using the import module by copying the txt.zip and metadata.csv from project_data/txt to project_data/import . However, it is generally not recommended to export and then reimport data, as you may lose metadata in the process. Configuration The default configuration assumes: There are JSON files in project_data/json . Each JSON has the required fields pub_date , title , author . Each JSON file has either: a content field, or a bag_of_words field created using the import module tokenizer. The following configurations are accepted: limit : The number of files to export. Set to 0 to export all files. txt_dir : The path to the directory where text files will be saved. metafile : The path to the metadata CSV file (including filename) that will be saved. zipfile : The path to the zip archive (including filename) that will be saved if zip_output=True . zip_output : Whether or not to create a zip archive the exported plain text files. This option automatically deletes the plain text files after they are zipped. clear_cache : If set to True , previous export contents in the txt directory, including metadata and zip files will be deleted before an export is started. txt_content_fields : A list of JSON fields to be checked in order for data content. The first field encountered in a document will be used for the data export. Documents with no listed field will be excluded from export. csv_export_fields : A list of JSON fields to be exported to the metadata file. Fields in this list will become the columns in the CSV file. Export This cell starts the export. It can be run without modification. Export Features Tables If your data contains features tables (lists of lists containing linguistic features), you can use this cell to export features tables as CSV files for each document in your JSON folder. Set the save_path to a directory where you wish to save the CSV files. Module Structure export \u2523 scripts \u2503 \u2523 export_package.py \u2503 \u2517 json_to_txt_csv.py \u2523 export_project.ipynb \u2523 json_to_txt_csv.ipynb \u2517 README.md","title":"export"},{"location":"modules/export/#export","text":"","title":"Export"},{"location":"modules/export/#about-this-module","text":"The export module provides utilities for exporting data from a project or a project as a whole to a compressed tar archive. The tar format is preferred to the zip format for entire projects because it preserves file permissions. This is less important for exports of the data itself. The module has two notebooks: export_project.ipynb for exporting an entire project and json_to_txt_csv.ipynb for exporting files from the project json folder to a directory of plain text files with an accompanying metadata CSV file.","title":"About This Module"},{"location":"modules/export/#user-guide","text":"","title":"User Guide"},{"location":"modules/export/#export-project","text":"This notebook provides the ability to export an entire project to a single file in the form of a .tar.gz archive. File size can be reduced by setting a list of folders for exclusion. Once the archive has been created, its location can optionally be recorded in a MongoDB database. (Eventually it will be possible to store the archive in the database, but this feature is not yet available.","title":"Export Project"},{"location":"modules/export/#configuration","text":"The notebook expects the following values in the Configuration cell: name : The name of the project archive author : The name of the author of the archive version : The version number save_path : The filepath where the archive will be save (including filename) exclude (optional): List of folder paths to ignore. Paths should be relative to the project folder without a leading '/'. client (optional): The url of the MongoDB client database (optional): The name of a MongoDB database collection (optional): The name of a MongoDB database If you are not working with MongoDB, leave these the client , database , and collection set to None .","title":"Configuration"},{"location":"modules/export/#build-data-package","text":"This cell instantiates the ExportPackage object and builds a Frictionless Data data package detailing the project's resources. If the project directory contains a datapackage.json and/or README.md , a datetime stamp will be added; otherwise, these files will be created. Once the data package is built, it is possible to access it with export_package.datapackage , and the README text can be accessed with export_package.readme .","title":"Build Data Package"},{"location":"modules/export/#make-archive","text":"This cell creates the archive file. It can be run without modification.","title":"Make Archive"},{"location":"modules/export/#extract-archive","text":"The last two cells can be used to extract an existing project archive file to a project folder. Before running the last cell set the following configurations: archive_file : The path the archive file to be extracted destination_dir : The path to the project folder where the project will be extracted. If the folder does not exist, it will be created. remove_archive : By default, the archive file copied to the project folder (not the original one) will be deleted after it is extracted. If you wish to retain it, set remove_archive=False .","title":"Extract Archive"},{"location":"modules/export/#json_to_txt_csv","text":"The WE1S workflows use JSON format internally for manipulating data. However, you may wish to export JSON data from a project to plain text files with a CSV metadata file for use with other external tools. This notebook uses JSON project data to export a collection of plain txt files \u2014 one per JSON document \u2014 containing only the document contents field or bag of words. Each file is named with the name of the JSON document and a .txt extension. It also produces a metadata.csv file. This file contains a header and one row per document with the document filename plus required fields. Output from this notebook can be imported using the import module by copying the txt.zip and metadata.csv from project_data/txt to project_data/import . However, it is generally not recommended to export and then reimport data, as you may lose metadata in the process.","title":"json_to_txt_csv"},{"location":"modules/export/#configuration_1","text":"The default configuration assumes: There are JSON files in project_data/json . Each JSON has the required fields pub_date , title , author . Each JSON file has either: a content field, or a bag_of_words field created using the import module tokenizer. The following configurations are accepted: limit : The number of files to export. Set to 0 to export all files. txt_dir : The path to the directory where text files will be saved. metafile : The path to the metadata CSV file (including filename) that will be saved. zipfile : The path to the zip archive (including filename) that will be saved if zip_output=True . zip_output : Whether or not to create a zip archive the exported plain text files. This option automatically deletes the plain text files after they are zipped. clear_cache : If set to True , previous export contents in the txt directory, including metadata and zip files will be deleted before an export is started. txt_content_fields : A list of JSON fields to be checked in order for data content. The first field encountered in a document will be used for the data export. Documents with no listed field will be excluded from export. csv_export_fields : A list of JSON fields to be exported to the metadata file. Fields in this list will become the columns in the CSV file.","title":"Configuration"},{"location":"modules/export/#export_1","text":"This cell starts the export. It can be run without modification.","title":"Export"},{"location":"modules/export/#export-features-tables","text":"If your data contains features tables (lists of lists containing linguistic features), you can use this cell to export features tables as CSV files for each document in your JSON folder. Set the save_path to a directory where you wish to save the CSV files.","title":"Export Features Tables"},{"location":"modules/export/#module-structure","text":"export \u2523 scripts \u2503 \u2523 export_package.py \u2503 \u2517 json_to_txt_csv.py \u2523 export_project.ipynb \u2523 json_to_txt_csv.ipynb \u2517 README.md","title":"Module Structure"},{"location":"modules/import/","text":"Import About This Module The import module is main starting point for using the WE1S Workspace project. Use this module to import data into your project. Imported data is stored in the project_data folder. The import pipeline attempts to massage all data into a collection of JSON files, stored in the project_data/json folder. These JSON files contain both data and metadata as a series of key-value pairs that conform to the JSON file format. The names of the keys (also called \"fields\") conform to the WE1S manifest schema and certain metadata required by the tools in the WE1S Workspace. After import, the text content of your data will be stored in the JSON files' content field. JSON files are human-readable text files, which you can open and inspect. However, it is very easy to corrupt their format. If you ever suspect that this may have happened, you can paste the text into a JSON validator like jsonlint.com to check for errors. The import notebook requires either a zip archive of your data or a connection to a MongoDB database where the data is stored. Zip archives may take one of the following forms: A zip archive containing plain text data and an accompanying CSV file with relevant metadata. A zip archive containing data already in JSON format. A zip archive of a Frictionless Data data package containing data already in JSON format. If your metadata does not contain the field names required by the WE1S workspace, the import module allows to map your metadata's field names onto the WE1S field names. User Guide Setup This cell imports various Python modules and defines paths that will be used by the module. In most cases, you will not need to change the default settings. Configuration Configuration options are explained briefly below. zip_file : The name of the zip archive containing your data. By default, the archive is called import.zip , but you can modify the filename. If the data is in plain text format, you must also prepare a metadata.csv file. Does not apply when importing from MongoDB (you can set it to None ). metadata.csv : The name of your metadata file if you are importing plain text data. By default, it is called metadata.csv , but you can change the name. Important The metadata file must have filename , pub_date , title , and author as its first four headers. You can include additional metadata fields after the author field. Does not apply when importing directly from JSON files or from MongoDB (you can set it to None ). remove_existing_json : Empty the json folder before importing. The default is False , so it is possible to add additional data on multiple runs. delete_imports_dir : If set to True , the folder containing your zip_file and metadata.csv file will be deleted when the import is complete. Does not apply when importing from MongoDB (you can set it to None ). delete_text_dir : If set to True , the folder containing your imported plain text files will be deleted after they are converted to json format. Does not apply when importing directly from JSON files or from MongoDB (you can set it to None ). data_dirs : If you are importing data already in json format, you can specify a list of paths in your zip archive or Frictionless Data data package where the json files are located. Does not apply when importing from MongoDB (you can set it to None ). title_field : If you are importing data already in json format that does not contain a field named title you can map an existing field to this key by providing the name of the existing field here. author_field : If you are importing data already in json format that does not contain a field named author you can map an existing field to this key by providing the name of the existing field here. pub_date_field : If you are importing data already in json format that does not contain a field named pub_date you can map an existing field to this key by providing the name of the existing field here. content_field : If you are importing data already in json format that does not contain a field named content you can map an existing field to this key by providing the name of the existing field here. dedupe : If set to True , the script will check for duplicate files within the project that may have been created by importing data from multiple zip archives. Duplicate files will be given the extension .dupe . This option also changes the extension of json files containing empty content fields to .empty . Warning For very large projects (~100,000 or more documents), duplicate detection may take up to several hours to run and, depending on other traffic on the server, may cause a server error. random_sample : If you wish to import a random sample of the data in your zip_file , specify the number of documents you wish to import. random_seed : Specify a number to initialize the random sampling. This ensures reproducibility if you have to run the import multiple times. In most cases, the setting can be left as 1 . required_phrase : A word or phrase which will be used to filter the imported data. Only documents that contain the required_phrase value will be imported to your project. log_file : The path to the file where errors and deduping results are logged. The default is import_log.txt in the same folder as this notebook. If you are importing your data directly to MongoDB, rather than a project folder, configure your MongoDB client , your database as db , and the name of your collection . For the client setting you can simply enter MONGODB_CLIENT to use your project's configuration. If importing from MongoDB, the query setting should be a valid MongoDB query. Since MongoDB syntax can be difficult \u2014 especially for complex queries \u2014 you may wish to use the WE1S QueryBuilder to construct your query and then paste it into the configuration cell. For information on using the Query Builder with your data, see on Using the QueryBuilder below. Prepare the Workspace for File Import You should use this cell only if you are importing from a zip archive. This cell sets up an import task based on the configuration you have supplied in the previous cell. When you run this cell, it will indicate whether the setup process was successful. Once the task is set up, instructions are displayed for uploading your data and metadata files to the Workspace. Perform the Import This cell starts the import process. A progress bar indicates the percentage of files imported. You may also receive error messages if there was a problem importing specific files. A log file will be generated, and you can inspect this file to see the nature of the errors. When the process is finished, instructions for tokenize the data are displayed. See the Tokenize the Data section below. MongoDB This cell starts the import process directly from MongoDB. Assuming that the client, database, and collection have been configured correctly in the Configuration section, it should work automatically. A progress bar indicates the percentage of files imported. You may also receive error messages if there was a problem importing specific files. A log file will be generated, and you can inspect this file to see the nature of the errors. When the process is finished, instructions for tokenize the data are displayed. See the Tokenize the Data section below. For help with constructing MongoDB queries, you may wish to use the WE1S QueryBuilder to construct your query and then paste it into the configuration cell. For information on using the Query Builder with your data, see on Using the QueryBuilder below. Tokenize the Data This cell is optional, but it can save time when performing tasks in other tools. Normally text analysis tools have to divide a text into countable \"tokens\" (most frequently words). This process is called tokenization. This cell allows you to pre-tokenize your data so that other tools do not need to take this step. It generates a dictionary of token-count pairs such as {\"cat\": 3, \"dog\": 2} for each of your JSON files. This dictionary is appended to the JSON file in the bag_of_words field. The import tokenizer offers two tokenization methods. The default method is strips all non-alphanumeric characters and then divides the text into tokens on white space. Alternatively, you can use the spaCy Natural Language Processing library to tokenize based on spaCy's language model. spaCy extracts linguistic features from your text, not only tokens but parts of speech and named entities. This is instrinsically slower and may require a lot of memory for large texts. To use WE1S's custom spaCy tokenizer, set method='we1s' . If your text has been previously processed by spaCy and there is a features table in your JSON file, the tokenizer will attempt to use it to build the bag_of_words dictionary. If you do not have a features table but would like to save one to your JSON files, configure save_features_table=True . Using the QueryBuilder The QueryBuilder is a simple web-based form that allows you to select metadata field names, operators such as \"is equal to\" or \"contains\", and values to match in the database. It can be used to generate very complex queries that are difficult to write by hand. To launch the QueryBuilder, open query-builder/index.html , configure a query in the form, and click \"Get Query to display the query you have configured. You can then copy the one-line query string into the import notebook's query setting. The QueryBuilder may not work if you open the web page on a sandboxed server. If you try to use it in this setting, you will receive a warning with the suggestion to download the query-builder-bundle.zip file. Download and extract this file to your local computer, and you can open index.html to run the QueryBuilder locally. By default, the QueryBuilder is configured to display common fields in the WE1S manifest schema. However, you may have metadata fields in your JSON files that are not part of the schema and will therefore not appear in the dropdown menu. If this is the case, you can easily configure the QueryBuilder for metadata fields since its configuration file is also in JSON format. Just open query-builder/assets/config.js . Copy a section between curly braces and rename the the id and label to your desired field name. The type should be \"string\", \"integer\", \"boolean\", or \"date\", depending on the type of value that occurs in that metadata category. There are a couple of examples that perform validation (such as for date format) that can be used as models for your own fields. The QueryBuilder is based on jQuery QueryBuilder . See its documentation for more sophisticated forms of customization. We recommend that you make a backup of the schema.js file before modifying it. If find that you have corrupted the format of the format, you can paste it into a JSON validator like jsonlint.com (omitting the var schema = at the beginning) to check for errors. Module Structure import \u2523 scripts \u2503 \u2503 \u2523 import.py \u2503 \u2503 \u2523 import_tokenizer.py \u2503 \u2503 \u2517 timer.py \u2523 query-builder \u2503 \u2523 assets \u2503 \u2503 \u2523 config \u2503 \u2503 \u2503 \u2523 schema.js \u2503 \u2503 \u2523 css \u2503 \u2503 \u2503 \u2523 query-builder.default.min.css \u2503 \u2503 \u2503 \u2517 tyles.css \u2503 \u2503 \u2523 js \u2503 \u2503 \u2503 \u2523 query-builder.standalone.min.js \u2503 \u2503 \u2503 \u2517 builder.js \u2503 \u2523 index.html \u2503 \u2517 README.md \u2523 import.ipynb \u2523 query-builder-bundle.zip \u2517 README.md","title":"import"},{"location":"modules/import/#import","text":"","title":"Import"},{"location":"modules/import/#about-this-module","text":"The import module is main starting point for using the WE1S Workspace project. Use this module to import data into your project. Imported data is stored in the project_data folder. The import pipeline attempts to massage all data into a collection of JSON files, stored in the project_data/json folder. These JSON files contain both data and metadata as a series of key-value pairs that conform to the JSON file format. The names of the keys (also called \"fields\") conform to the WE1S manifest schema and certain metadata required by the tools in the WE1S Workspace. After import, the text content of your data will be stored in the JSON files' content field. JSON files are human-readable text files, which you can open and inspect. However, it is very easy to corrupt their format. If you ever suspect that this may have happened, you can paste the text into a JSON validator like jsonlint.com to check for errors. The import notebook requires either a zip archive of your data or a connection to a MongoDB database where the data is stored. Zip archives may take one of the following forms: A zip archive containing plain text data and an accompanying CSV file with relevant metadata. A zip archive containing data already in JSON format. A zip archive of a Frictionless Data data package containing data already in JSON format. If your metadata does not contain the field names required by the WE1S workspace, the import module allows to map your metadata's field names onto the WE1S field names.","title":"About This Module"},{"location":"modules/import/#user-guide","text":"","title":"User Guide"},{"location":"modules/import/#setup","text":"This cell imports various Python modules and defines paths that will be used by the module. In most cases, you will not need to change the default settings.","title":"Setup"},{"location":"modules/import/#configuration","text":"Configuration options are explained briefly below. zip_file : The name of the zip archive containing your data. By default, the archive is called import.zip , but you can modify the filename. If the data is in plain text format, you must also prepare a metadata.csv file. Does not apply when importing from MongoDB (you can set it to None ). metadata.csv : The name of your metadata file if you are importing plain text data. By default, it is called metadata.csv , but you can change the name. Important The metadata file must have filename , pub_date , title , and author as its first four headers. You can include additional metadata fields after the author field. Does not apply when importing directly from JSON files or from MongoDB (you can set it to None ). remove_existing_json : Empty the json folder before importing. The default is False , so it is possible to add additional data on multiple runs. delete_imports_dir : If set to True , the folder containing your zip_file and metadata.csv file will be deleted when the import is complete. Does not apply when importing from MongoDB (you can set it to None ). delete_text_dir : If set to True , the folder containing your imported plain text files will be deleted after they are converted to json format. Does not apply when importing directly from JSON files or from MongoDB (you can set it to None ). data_dirs : If you are importing data already in json format, you can specify a list of paths in your zip archive or Frictionless Data data package where the json files are located. Does not apply when importing from MongoDB (you can set it to None ). title_field : If you are importing data already in json format that does not contain a field named title you can map an existing field to this key by providing the name of the existing field here. author_field : If you are importing data already in json format that does not contain a field named author you can map an existing field to this key by providing the name of the existing field here. pub_date_field : If you are importing data already in json format that does not contain a field named pub_date you can map an existing field to this key by providing the name of the existing field here. content_field : If you are importing data already in json format that does not contain a field named content you can map an existing field to this key by providing the name of the existing field here. dedupe : If set to True , the script will check for duplicate files within the project that may have been created by importing data from multiple zip archives. Duplicate files will be given the extension .dupe . This option also changes the extension of json files containing empty content fields to .empty . Warning For very large projects (~100,000 or more documents), duplicate detection may take up to several hours to run and, depending on other traffic on the server, may cause a server error. random_sample : If you wish to import a random sample of the data in your zip_file , specify the number of documents you wish to import. random_seed : Specify a number to initialize the random sampling. This ensures reproducibility if you have to run the import multiple times. In most cases, the setting can be left as 1 . required_phrase : A word or phrase which will be used to filter the imported data. Only documents that contain the required_phrase value will be imported to your project. log_file : The path to the file where errors and deduping results are logged. The default is import_log.txt in the same folder as this notebook. If you are importing your data directly to MongoDB, rather than a project folder, configure your MongoDB client , your database as db , and the name of your collection . For the client setting you can simply enter MONGODB_CLIENT to use your project's configuration. If importing from MongoDB, the query setting should be a valid MongoDB query. Since MongoDB syntax can be difficult \u2014 especially for complex queries \u2014 you may wish to use the WE1S QueryBuilder to construct your query and then paste it into the configuration cell. For information on using the Query Builder with your data, see on Using the QueryBuilder below.","title":"Configuration"},{"location":"modules/import/#prepare-the-workspace-for-file-import","text":"You should use this cell only if you are importing from a zip archive. This cell sets up an import task based on the configuration you have supplied in the previous cell. When you run this cell, it will indicate whether the setup process was successful. Once the task is set up, instructions are displayed for uploading your data and metadata files to the Workspace.","title":"Prepare the Workspace for File Import"},{"location":"modules/import/#perform-the-import","text":"This cell starts the import process. A progress bar indicates the percentage of files imported. You may also receive error messages if there was a problem importing specific files. A log file will be generated, and you can inspect this file to see the nature of the errors. When the process is finished, instructions for tokenize the data are displayed. See the Tokenize the Data section below.","title":"Perform the Import"},{"location":"modules/import/#mongodb","text":"This cell starts the import process directly from MongoDB. Assuming that the client, database, and collection have been configured correctly in the Configuration section, it should work automatically. A progress bar indicates the percentage of files imported. You may also receive error messages if there was a problem importing specific files. A log file will be generated, and you can inspect this file to see the nature of the errors. When the process is finished, instructions for tokenize the data are displayed. See the Tokenize the Data section below. For help with constructing MongoDB queries, you may wish to use the WE1S QueryBuilder to construct your query and then paste it into the configuration cell. For information on using the Query Builder with your data, see on Using the QueryBuilder below.","title":"MongoDB"},{"location":"modules/import/#tokenize-the-data","text":"This cell is optional, but it can save time when performing tasks in other tools. Normally text analysis tools have to divide a text into countable \"tokens\" (most frequently words). This process is called tokenization. This cell allows you to pre-tokenize your data so that other tools do not need to take this step. It generates a dictionary of token-count pairs such as {\"cat\": 3, \"dog\": 2} for each of your JSON files. This dictionary is appended to the JSON file in the bag_of_words field. The import tokenizer offers two tokenization methods. The default method is strips all non-alphanumeric characters and then divides the text into tokens on white space. Alternatively, you can use the spaCy Natural Language Processing library to tokenize based on spaCy's language model. spaCy extracts linguistic features from your text, not only tokens but parts of speech and named entities. This is instrinsically slower and may require a lot of memory for large texts. To use WE1S's custom spaCy tokenizer, set method='we1s' . If your text has been previously processed by spaCy and there is a features table in your JSON file, the tokenizer will attempt to use it to build the bag_of_words dictionary. If you do not have a features table but would like to save one to your JSON files, configure save_features_table=True .","title":"Tokenize the Data"},{"location":"modules/import/#using-the-querybuilder","text":"The QueryBuilder is a simple web-based form that allows you to select metadata field names, operators such as \"is equal to\" or \"contains\", and values to match in the database. It can be used to generate very complex queries that are difficult to write by hand. To launch the QueryBuilder, open query-builder/index.html , configure a query in the form, and click \"Get Query to display the query you have configured. You can then copy the one-line query string into the import notebook's query setting. The QueryBuilder may not work if you open the web page on a sandboxed server. If you try to use it in this setting, you will receive a warning with the suggestion to download the query-builder-bundle.zip file. Download and extract this file to your local computer, and you can open index.html to run the QueryBuilder locally. By default, the QueryBuilder is configured to display common fields in the WE1S manifest schema. However, you may have metadata fields in your JSON files that are not part of the schema and will therefore not appear in the dropdown menu. If this is the case, you can easily configure the QueryBuilder for metadata fields since its configuration file is also in JSON format. Just open query-builder/assets/config.js . Copy a section between curly braces and rename the the id and label to your desired field name. The type should be \"string\", \"integer\", \"boolean\", or \"date\", depending on the type of value that occurs in that metadata category. There are a couple of examples that perform validation (such as for date format) that can be used as models for your own fields. The QueryBuilder is based on jQuery QueryBuilder . See its documentation for more sophisticated forms of customization. We recommend that you make a backup of the schema.js file before modifying it. If find that you have corrupted the format of the format, you can paste it into a JSON validator like jsonlint.com (omitting the var schema = at the beginning) to check for errors.","title":"Using the QueryBuilder"},{"location":"modules/import/#module-structure","text":"import \u2523 scripts \u2503 \u2503 \u2523 import.py \u2503 \u2503 \u2523 import_tokenizer.py \u2503 \u2503 \u2517 timer.py \u2523 query-builder \u2503 \u2523 assets \u2503 \u2503 \u2523 config \u2503 \u2503 \u2503 \u2523 schema.js \u2503 \u2503 \u2523 css \u2503 \u2503 \u2503 \u2523 query-builder.default.min.css \u2503 \u2503 \u2503 \u2517 tyles.css \u2503 \u2503 \u2523 js \u2503 \u2503 \u2503 \u2523 query-builder.standalone.min.js \u2503 \u2503 \u2503 \u2517 builder.js \u2503 \u2523 index.html \u2503 \u2517 README.md \u2523 import.ipynb \u2523 query-builder-bundle.zip \u2517 README.md","title":"Module Structure"},{"location":"modules/json-utilities/","text":"json_utilities About This Module This notebook provides a method of accessing the contents of a project's json folder. These folders can be quite large, and they will cause the browser to freeze if they are opened using the Jupyter notebook file browser. This notebook creates a Documents object with which you can call methods that list or read the contents of the files in the json folder. It also allows you to perform database-like queries on the contents to filter your results and to export the results to a zip archive. User Guide The main notebook json_utilities.ipynb functions more like a tutorial than the notebooks of other modules. There are built-in examples, which you can work through and modify according to your data and research questions. You can also use remove_fields.ipynb to remove specific fields from json files. json_utilities.ipynb Setup This cell imports Python libraries and scripts. View Metadata Fields (Optional) If you wish to perform a query on your documents, it can be helpful to know what metadata fields are available. The cell below will read the first 100 documents and extract the keys for each metadata field. Note that listed keys may not be available in all documents. If you think that your metadata is very inconsistent, you may want to run docs.get_metadata_keys() without start and end values. However, this can take a long time, so it is not recommended unless you have reason to think that there are large discrepancies across your collection. It is also possible to get the keys for a specific file with docs.get_metadata_keys(filelist=['file1', 'file2', etc.]) . If you have already run something like result = docs.get_file_list(0, 5) , you can simply run docs.get_metadata_keys(filelist=result) . In the second cell, you can generate a table of your documents with get_table() . It takes a list of files and a list of fields as its arguments, as in the example below. Columns can be re-ordered, sorted, and filtered. However, it is recommended that you only supply a small number of columns. The bigger the table, the longer the lag time when you scroll. If you wish to save the table after you have sorted and/or filtered it, set the filename in the third cell and run the cell. Performing Queries Although many questions about your data can be answered by working with the table above, sometimes you may need to perform more sophisticated database-like queries to filter the data in your project's json folder. The cell below provide an interface for performing these queries. A basic query is given in the form of a tuple with the syntax (fieldname, operator, value) . The fieldname is the name of the metadata field you wish to search. The value is the value you are looking for in the field, and the operator is the method by which you will evaluate the value. Here are the possible operators: < , <= , = (or == ), != (meaning \"not equal to\"), > , >= , contains . The last will match any value anywhere in the field. For greater power, you can use regex as the operator and a regex pattern as the value . Important The fieldname and operator must be enclosed in single quotes. The value must also be single quotes unless it is a number or Boolean ( True or False ). The find() method takes three arguments: a list of filenames, a query, and, optionally, a Boolean lower_case value. If lower_case=True the value data will be converted to lower case before it is evaluated. The default is False . In the cell below, we will get a list of the first 5 files (to keep things quick) and search for the ones that contain \"Politics\" in the document's name field. Note: There is a built-in timer class that can be used to time queries of long file lists. Its use is illustrated in the cell below, but it can be used to time any of the methods. Performing Multiple Queries You can pass multiple queries to the find() method by using a list of tuples. As you can see from the example below. The result will be every document that matches any of the queries in the list. Adding Boolean Logic It is possible to add more complex Boolean logic by passing a dictionary as the query with 'and' or 'or' as the key. The value should be a list of one or more tuples, or a list of dictionaries as shown in the examples. Exporting the Results of a Query You can save the documents found by your query to a zip file with the export() method. It takes a list of filenames and a path where you wish to save the zip file. A filename is sufficient if you wish to save it in the current folder. The export() method takes an optional text_only argument. Setting text_only=True will export only the content fields as plain text files. Here is an example in which you create a Documents object, get a file list, find files in the list that match your query, and export the results to a zip archive. The timer class is automatically applied to exports. remove_fields.ipynb This notebook will remove specified field from all files in the json folder. It is intended primarily for creating sample data sets for testing, but we could consider documenting it fully and keeping it in the release version of the module. Configuration Configure a list of fields to remove. By default, the folder containing the json files is the project's json directory, but you can configure another folder (relative to the project root). Errors will be logged to a file saved with the name you configure. Setup This cell loads your configurations and instantiates your removal job. Remove Fields This cell removes the fields from json files in the directory you have configured. Module Structure import \u2523 scripts \u2503 \u2517 json_utilities.py \u2523 json_utilities.ipynb \u2523 remove_fields.ipynb \u2517 README.md","title":"json_utilities"},{"location":"modules/json-utilities/#json_utilities","text":"","title":"json_utilities"},{"location":"modules/json-utilities/#about-this-module","text":"This notebook provides a method of accessing the contents of a project's json folder. These folders can be quite large, and they will cause the browser to freeze if they are opened using the Jupyter notebook file browser. This notebook creates a Documents object with which you can call methods that list or read the contents of the files in the json folder. It also allows you to perform database-like queries on the contents to filter your results and to export the results to a zip archive.","title":"About This Module"},{"location":"modules/json-utilities/#user-guide","text":"The main notebook json_utilities.ipynb functions more like a tutorial than the notebooks of other modules. There are built-in examples, which you can work through and modify according to your data and research questions. You can also use remove_fields.ipynb to remove specific fields from json files.","title":"User Guide"},{"location":"modules/json-utilities/#json_utilitiesipynb","text":"","title":"json_utilities.ipynb"},{"location":"modules/json-utilities/#setup","text":"This cell imports Python libraries and scripts.","title":"Setup"},{"location":"modules/json-utilities/#view-metadata-fields-optional","text":"If you wish to perform a query on your documents, it can be helpful to know what metadata fields are available. The cell below will read the first 100 documents and extract the keys for each metadata field. Note that listed keys may not be available in all documents. If you think that your metadata is very inconsistent, you may want to run docs.get_metadata_keys() without start and end values. However, this can take a long time, so it is not recommended unless you have reason to think that there are large discrepancies across your collection. It is also possible to get the keys for a specific file with docs.get_metadata_keys(filelist=['file1', 'file2', etc.]) . If you have already run something like result = docs.get_file_list(0, 5) , you can simply run docs.get_metadata_keys(filelist=result) . In the second cell, you can generate a table of your documents with get_table() . It takes a list of files and a list of fields as its arguments, as in the example below. Columns can be re-ordered, sorted, and filtered. However, it is recommended that you only supply a small number of columns. The bigger the table, the longer the lag time when you scroll. If you wish to save the table after you have sorted and/or filtered it, set the filename in the third cell and run the cell.","title":"View Metadata Fields (Optional)"},{"location":"modules/json-utilities/#performing-queries","text":"Although many questions about your data can be answered by working with the table above, sometimes you may need to perform more sophisticated database-like queries to filter the data in your project's json folder. The cell below provide an interface for performing these queries. A basic query is given in the form of a tuple with the syntax (fieldname, operator, value) . The fieldname is the name of the metadata field you wish to search. The value is the value you are looking for in the field, and the operator is the method by which you will evaluate the value. Here are the possible operators: < , <= , = (or == ), != (meaning \"not equal to\"), > , >= , contains . The last will match any value anywhere in the field. For greater power, you can use regex as the operator and a regex pattern as the value . Important The fieldname and operator must be enclosed in single quotes. The value must also be single quotes unless it is a number or Boolean ( True or False ). The find() method takes three arguments: a list of filenames, a query, and, optionally, a Boolean lower_case value. If lower_case=True the value data will be converted to lower case before it is evaluated. The default is False . In the cell below, we will get a list of the first 5 files (to keep things quick) and search for the ones that contain \"Politics\" in the document's name field. Note: There is a built-in timer class that can be used to time queries of long file lists. Its use is illustrated in the cell below, but it can be used to time any of the methods.","title":"Performing Queries"},{"location":"modules/json-utilities/#performing-multiple-queries","text":"You can pass multiple queries to the find() method by using a list of tuples. As you can see from the example below. The result will be every document that matches any of the queries in the list.","title":"Performing Multiple Queries"},{"location":"modules/json-utilities/#adding-boolean-logic","text":"It is possible to add more complex Boolean logic by passing a dictionary as the query with 'and' or 'or' as the key. The value should be a list of one or more tuples, or a list of dictionaries as shown in the examples.","title":"Adding Boolean Logic"},{"location":"modules/json-utilities/#exporting-the-results-of-a-query","text":"You can save the documents found by your query to a zip file with the export() method. It takes a list of filenames and a path where you wish to save the zip file. A filename is sufficient if you wish to save it in the current folder. The export() method takes an optional text_only argument. Setting text_only=True will export only the content fields as plain text files. Here is an example in which you create a Documents object, get a file list, find files in the list that match your query, and export the results to a zip archive. The timer class is automatically applied to exports.","title":"Exporting the Results of a Query"},{"location":"modules/json-utilities/#remove_fieldsipynb","text":"This notebook will remove specified field from all files in the json folder. It is intended primarily for creating sample data sets for testing, but we could consider documenting it fully and keeping it in the release version of the module.","title":"remove_fields.ipynb"},{"location":"modules/json-utilities/#configuration","text":"Configure a list of fields to remove. By default, the folder containing the json files is the project's json directory, but you can configure another folder (relative to the project root). Errors will be logged to a file saved with the name you configure.","title":"Configuration"},{"location":"modules/json-utilities/#setup_1","text":"This cell loads your configurations and instantiates your removal job.","title":"Setup"},{"location":"modules/json-utilities/#remove-fields","text":"This cell removes the fields from json files in the directory you have configured.","title":"Remove Fields"},{"location":"modules/json-utilities/#module-structure","text":"import \u2523 scripts \u2503 \u2517 json_utilities.py \u2523 json_utilities.ipynb \u2523 remove_fields.ipynb \u2517 README.md","title":"Module Structure"},{"location":"modules/metadata/","text":"Metadata About This Module The main notebook in this module, topic_statistics_by_metadata.ipynb , enables you to generate some basic statistics about document counts based on the metadata fields available in your project's JSON files. There is also a notebook ( scattertext.ipynb ) for generating visualisations using the Scattertext library. Since these notebooks, and other tools in the Workspace, make use of document metadata, this module also provides a utility notebook ( add_metadata.ipynb ) for adding metadata fields to your project's JSON files after the data has been imported. User Guide add_metadata.ipynb This notebook can be used to add metadata to project JSON files from a CSV file after they have been imported into the project. The CSV file must have a filename column referencing files in the json folder. If this does not exist, the script will look for a name column, append .json , and attempt to use it as a filename. Configuration The only required configuration is the path to your metadata CSV file. Load the Metadata This cell loads the metadata file and performs other crucial setup functions. When it is finished, it displays the metadata in tabular format. Add the Metadata to Project JSON files This cell iterates through the metadata rows and adds the metadata field values to each file listed with a corresponding file in the JSON directory. If the metadata CSV does not have a filename listed, the notebook will attempt to create one from a name field. If a filename still cannot be found, the row will be skipped. Error messages are displayed if a filename could not be detected in the metadata CSV or if there is no corresponding file in the JSON folder. scattertext.ipynb This notebook uses Jason Kessler's Scattertext library to allow you to explore key terms in your corpus in relation to your documents' metadata. It does not require you to have topic modelled your data. Because Scattertext is under active development and has many more functions than are made available through this notebook, WE1S does not distribute it as part of the Workspace. Instead, it is downloaded and intalled in your environment when you run the first cell. You may receive a warning that Scattertext is already installed if you run the cell again. You can safely ignore this warning. Note that for the purpose of working with Scattertext, the original text is re-tokenized using slightly different rules from the WE1S preprocessor, so there may be some small discrepancies. By default, the WE1S standard stoplist in your project's MALLET module is applied. Load Documents This cell loads the json documents for the entire collection. Configuration also takes place in this cell, so be sure to set the configurations as detailed below before running it. If you have run this cell previously and wish to use the same settings, you can skip this cell. The next cell will load your Documents dataframe from a stored copy called documents_df.parquet . If you wish to save the settings, this fille will be overwritten with the new dataframe, so make a backup if you wish to keep the old one. Loading can take a while, so, for experimentation, it is best to set end to a smaller number. This will limit the number of documents loaded. If you do not wish to start at the first document, set start to a higher number. Numbering is zero-indexed, so the first document is document 0. By default, only your documents' content is loaded, but you can load other metadata from your JSON files using the the extra_fields setting, which takes a dictionary of key-value pairs. The value is the name of the field you wish to use from your JSON files. It will be referenced by the equivalent key in Scattertext. So, if you want to use your pub_date field and refer to it as date in Scattertext, the dictionary would be {'date': 'pub_date'}. Note that WE1S data contains specific tags with hierarchical filepath like formats. If these are found, they will be parsed automatically. Tags such as \"media/news wires\" and \"media/news agency\" cannot be resolved to a single table column, so only the last tag will appear. If you are not using WE1S data but you have a field called tags (the values of which must be a dictionary), open scattertext.py and delete or comment out line 118: tags = tags_to_dict(doc) . If you wish to randomly sample a percentage of your corpus, set random_sampling to a number (without the \"%\" sign). If you do not wish to perform random sampling, set random_sample=None . Save the Dataframe to CSV By default, the output of the previous cell is a manipulatable dataframe. You can sort the columns by clicking on their labels or filter the table by clicking on the icons. If you wish to save the dataframe to a CSV file, configure a filename and then uncomment one of the lines below. The first will save the original dataframe and the second will save the dataframe after any sorting or filtering you have done. Generate Document Counts Report This cell provides a table of document counts for each field column beginning with the one configured for the start_column value. Each column provides the document counts for each metadata field in the data. The rows provide the document counts by value . This information will be used for configuring the following cells. Build a Corpus When the corpus is built, each document is parsed using spaCy , so this can take a while. For that reason, it is a good idea to set the limit to around 2000 documents or smaller. Before generating the corpus, the cell will automatically look for a previously-saved corpus file to speed loading time. If you have changed your limit or field settings, change the name of the corpus_file or set from_file=False . If you do not change the name of corpus_file , any previous corpus with that filename will be overwritten. Important A Scattertext corpus requires a field category corresponding to one of the column headings in the Document Counts Report above. The column must contain at least two non-zero. If you get an error, you may not have chosen a valid field. Results seem to be improved by using lemmas rather than the original tokens, but this can be changed with use_lemmas=False . The other options are more unpredicatable. The entity_types_to_use and tag_types_to_use lists allow you to specify entity and part of speech categories that should be retained in the analysis. Tokens not belonging to the types you specify will be excluded from the corpus. A list of the category abbreviations can be found in the spaCy documentation for named entities and part of speech tags . If you wish to use all the categories, set these values to All . You can also \"censor\" certain types, which replaces the original token it entity or part of speech abbreviation. Lastly, periods at the ends of tokens can be stripped if they have escaped spaCy's tokenizer. For convenience, here are lists of all entity and part of speech abbreviations, which you can use to copy and paste into the cell below. Named Entities \"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\" Parts of Speech \"$\", \"``\", \"''\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"CC\", \"CD\", \"DT\", \"EX\", \"IN\", \"LS\", \"NFP\", \"NIL\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RP\", \"SYM\", \"TO\", \"UH\", \"WDT\", \"WP\", \"WP$\", \"WRB\" You can perform quite a variety of configurations before generating your corpus: limit : A number less than or equal to the end value in Load Documents field : The metadata field you wish to explore corpus_file : The name of the corpus file (no file extension is necessary) from_file : If set to True , the notebook will look for a pre-existing corpus file before generating one from scratch. stoplist_path : Path to a custom stop word file use_lemmas : If set to True , the notebook will use lemmas (e.g. \"go\" for \"going\") instead of raw tokens. Lemmas are taken from spaCy's language model. The only modification is that the lemma of \"humanities\" is \"humanities\", not \"humanity\". entity_types_to_use : A list of the above entity types to use. If the value None is provided, all types are used. entity_types_to_censor : A list of entity types to \"censor\". This means that the tokens will be replaced by the entity label. tag_types_to_use : A list of the above tag types to use. If the value None is provided, all types are used. tag_types_to_censor : A list of tag types to \"censor\". This means that the tokens will be replaced by the tag label. strip_final_period : Removes periods (full stops) from the end of tokens if they have been missed by the tokenizer. Generate terms that differentiate the collection from a general English corpus This cell provides a list of terms in your corpus that make it distinct from a general English corpus. We think that Scattertext uses the Brown University Standard Corpus of Present-Day American English (AKA the Brown Corpus) for comparison, but we have not been able to confirm this. As such, the results need to be interpreted with this uncertainty in mind. The limit configuration controls the number of terms displayed in the output. Generate terms associated with a field value This cell provides a list of terms particularly associated with a particular metadata field value within your corpus. For the score_query configuration, supply one of the row values in the Generate Column Counts Report cell. The score_label can be a more human-readable or descriptive label for the value. Generate a Scattertext Visualization of Term Associations This cell generates a Scattertext visualization, which provides a rich environment for exploring your corpus. The Scattertext visualization is saved as a single HTML file at the location you specify for filename . Be sure to set limit to the same number you used for generating the corpus. The field_name value should be taken from one of the row values in the Counts Report above. In the graph, the axis for this field will be labelled with the value you provide for field_label . The other axis will be labelled by the value you provide for non_field_label . You can also modify the width of the graph and supply an extra metadata category, which will be the name of a column in the Documents table above. The values for that category will be displayed above sample documents in the graph. The results can be filtered by minimum term frequency and pointwise mutual information (the higher the number the greater the requirement that terms co-occur in the same document). Warning Scattertext HTML files are enormous and can take a very long time to load in your browser. A file generated from a corpus of 2000 documents will generally take about 15 minutes. So plan accordingly! Topic Statistics by Metadata This notebook extracts information form a model's topic-docs.txt file and combines it with the information in the documents' metadata fields to provide counts of the number of documents associated with specific metadata fields. Since MALLET automatically selects the top 100 documents in each topic, this is the basis for the data. The results can be viewed in pandas dataframes and saved to CSV files. The results may be visualised in a static bar chart (stacked or unstacked) or an interactive plotly bar chart for any metadata field. The visusalisations may be saved to static PNG files. Configuration Select one model to explore. Please run the next cell regardless of whether you change anything. If you are unsure of the name of your model, navigate to the your_project_name/project_data/models directory in your project, and choose the name of one of the subdirectories in that folder. Each subdirectory should be called topicsn1 , where n1 is the number of topics you chose to model, for example: selection = 'topics100' . Please follow this format exactly. Info In most cases, you should not need to change the data_path and from_file configurations. The data_path variable specifies the folder where the notebook's assets will be saved. Some cells below attempt to load assets from this data folder so that you do not need to re-run procedures if you have already run the cell once. If for some reason you wish to bypass loading data from a saved file, set from_file=False in the cell's configuration section. Read Topic-Docs File This cell loads the topic-docs.txt file data into a pandas dataframe. Export Data from Top Documents Sometimes it can be useful to extract the text from the top documents in a topic in order to study them using a different tool. This cell allows you to export this content as plain text files save them as a zip archive. Before running the cell, configure the save_path with the path to the directory where you want to save the text files. If topic_num is set to All , the content of all documents will be exported. If you set it to a topic number, only the documents associated with that topic will be exported. This cell is not required for running subsequent cells. Gather Collection Metadata This cell gathers metadata for a list of JSON fields from the documents in the collection. In the dataframe output, tag attributes which exist but without subattributes have values of 1 ; missing tag attributes have values of 0 . Tags with values like \"funding/US private college\" are represented on the table under the \"funding\" column with the value \"US private colleges\". You can drag column boundaries to change their width or column labels to re-order the columns. To sort the columns, click the column label (and click again to sort in reverse order). Click the filter icon to filter your data by column values. Note: This cell can take some time to run. If you have already run it once, it should read the metadata from a saved file. Set from_file=False to re-generate the data. Save to CSV If you wish to save a copy of the output of Gather Collection Metadata to a CSV file, set save_path to a filename to save the CSV to. In the sample below, N represents the topic number, which is recommended so that you do not overwrite a file from a different model. By default, the CSV will reflect the table above after any modifications you make by filtering or sorting. If you wish to use the original table, set use_original=True . Get Counts by Field Values This cell calculates document counts for each topic by field value using the topic_docs_metadata dataframe. In the configuration section, set field to the name of the metadata field you wish to calculate. Set save_path to a path to a CSV file if you wish to save the table. Set use_original=True if you wish the file to contain the original output. Otherwise, it will reflect any changes you make such as sorting and filtering. Visualise Metadata with a Simple Bar Plot (Static Version) This cell generates a simple bar plot for visualising the data generated by the notebook. Set fields to a list of column headings in the sums_and_means dataframe. If you wish to display different names in the legend, provide a list of corresponding names for legend_labels (in the same order). You may adjust the title , xlabel (for the x-axis) and ylabel (for the y-axis) to describe the content of your data accurately. Since plots can be very cramped, you may want to look at a limited range of topics. To do this, modify the start_topic and end_topic values. You can also save space by creating a stacked plot with stacked=True . (The interactive plot in the next cell provides another option with pan and zoom features.) To save the plot a file, set save_path to a full file path, including the filename. The type of file is inferred from the extension. For instance, files ending in .png will be saved as PNG files and files ending in .pdf will be saved as PDF files. SVG format is also available. Visualise Metadata with a Plotly Bar Plot (Interactive Version) The interactive plot (using Plotly) takes the same settings as the static plot above, except the stacked mode is not available. However, because Plotly provides zoom and pan features, it is possible to display the entire range of topics in a single graph. Click and drag over the graph to zoom in on a location. Click the home icon in the Plotly toolbar to restore the default zoom level. Click on the boxes in the legend to show and hide specific categories. Double-click to restore the default display. You can download the plot as PNG file by clicking the camera icon in the Plotly toolbar. If you wish to save the interactive plot as a standalone web page, set the save_path to a full file path, including the filename ending in .html . Generate Topic-Doc Dictionary (Optional Utility) This cell generates a dictionary with topic numbers as keys and a list of filenames in each topic as the values. Individual topics can be expected with topic_docs_dict[1] , where \"1\" is the desired topic number. The dictionary can be saved as a JSON file. Module Structure metadata \u2523 data \u2523 scripts \u2503 \u2523 add_metadata.py \u2503 \u2523 scattertext.py \u2503 \u2517 topic_stats.py \u2523 add_metadata.ipynb \u2523 README.md \u2523 scattertext.ipynb \u2517 topic_statistics_by_metadata.ipynb","title":"metadata"},{"location":"modules/metadata/#metadata","text":"","title":"Metadata"},{"location":"modules/metadata/#about-this-module","text":"The main notebook in this module, topic_statistics_by_metadata.ipynb , enables you to generate some basic statistics about document counts based on the metadata fields available in your project's JSON files. There is also a notebook ( scattertext.ipynb ) for generating visualisations using the Scattertext library. Since these notebooks, and other tools in the Workspace, make use of document metadata, this module also provides a utility notebook ( add_metadata.ipynb ) for adding metadata fields to your project's JSON files after the data has been imported.","title":"About This Module"},{"location":"modules/metadata/#user-guide","text":"","title":"User Guide"},{"location":"modules/metadata/#add_metadataipynb","text":"This notebook can be used to add metadata to project JSON files from a CSV file after they have been imported into the project. The CSV file must have a filename column referencing files in the json folder. If this does not exist, the script will look for a name column, append .json , and attempt to use it as a filename.","title":"add_metadata.ipynb"},{"location":"modules/metadata/#configuration","text":"The only required configuration is the path to your metadata CSV file.","title":"Configuration"},{"location":"modules/metadata/#load-the-metadata","text":"This cell loads the metadata file and performs other crucial setup functions. When it is finished, it displays the metadata in tabular format.","title":"Load the Metadata"},{"location":"modules/metadata/#add-the-metadata-to-project-json-files","text":"This cell iterates through the metadata rows and adds the metadata field values to each file listed with a corresponding file in the JSON directory. If the metadata CSV does not have a filename listed, the notebook will attempt to create one from a name field. If a filename still cannot be found, the row will be skipped. Error messages are displayed if a filename could not be detected in the metadata CSV or if there is no corresponding file in the JSON folder.","title":"Add the Metadata to Project JSON files"},{"location":"modules/metadata/#scattertextipynb","text":"This notebook uses Jason Kessler's Scattertext library to allow you to explore key terms in your corpus in relation to your documents' metadata. It does not require you to have topic modelled your data. Because Scattertext is under active development and has many more functions than are made available through this notebook, WE1S does not distribute it as part of the Workspace. Instead, it is downloaded and intalled in your environment when you run the first cell. You may receive a warning that Scattertext is already installed if you run the cell again. You can safely ignore this warning. Note that for the purpose of working with Scattertext, the original text is re-tokenized using slightly different rules from the WE1S preprocessor, so there may be some small discrepancies. By default, the WE1S standard stoplist in your project's MALLET module is applied.","title":"scattertext.ipynb"},{"location":"modules/metadata/#load-documents","text":"This cell loads the json documents for the entire collection. Configuration also takes place in this cell, so be sure to set the configurations as detailed below before running it. If you have run this cell previously and wish to use the same settings, you can skip this cell. The next cell will load your Documents dataframe from a stored copy called documents_df.parquet . If you wish to save the settings, this fille will be overwritten with the new dataframe, so make a backup if you wish to keep the old one. Loading can take a while, so, for experimentation, it is best to set end to a smaller number. This will limit the number of documents loaded. If you do not wish to start at the first document, set start to a higher number. Numbering is zero-indexed, so the first document is document 0. By default, only your documents' content is loaded, but you can load other metadata from your JSON files using the the extra_fields setting, which takes a dictionary of key-value pairs. The value is the name of the field you wish to use from your JSON files. It will be referenced by the equivalent key in Scattertext. So, if you want to use your pub_date field and refer to it as date in Scattertext, the dictionary would be {'date': 'pub_date'}. Note that WE1S data contains specific tags with hierarchical filepath like formats. If these are found, they will be parsed automatically. Tags such as \"media/news wires\" and \"media/news agency\" cannot be resolved to a single table column, so only the last tag will appear. If you are not using WE1S data but you have a field called tags (the values of which must be a dictionary), open scattertext.py and delete or comment out line 118: tags = tags_to_dict(doc) . If you wish to randomly sample a percentage of your corpus, set random_sampling to a number (without the \"%\" sign). If you do not wish to perform random sampling, set random_sample=None .","title":"Load Documents"},{"location":"modules/metadata/#save-the-dataframe-to-csv","text":"By default, the output of the previous cell is a manipulatable dataframe. You can sort the columns by clicking on their labels or filter the table by clicking on the icons. If you wish to save the dataframe to a CSV file, configure a filename and then uncomment one of the lines below. The first will save the original dataframe and the second will save the dataframe after any sorting or filtering you have done.","title":"Save the Dataframe to CSV"},{"location":"modules/metadata/#generate-document-counts-report","text":"This cell provides a table of document counts for each field column beginning with the one configured for the start_column value. Each column provides the document counts for each metadata field in the data. The rows provide the document counts by value . This information will be used for configuring the following cells.","title":"Generate Document Counts Report"},{"location":"modules/metadata/#build-a-corpus","text":"When the corpus is built, each document is parsed using spaCy , so this can take a while. For that reason, it is a good idea to set the limit to around 2000 documents or smaller. Before generating the corpus, the cell will automatically look for a previously-saved corpus file to speed loading time. If you have changed your limit or field settings, change the name of the corpus_file or set from_file=False . If you do not change the name of corpus_file , any previous corpus with that filename will be overwritten. Important A Scattertext corpus requires a field category corresponding to one of the column headings in the Document Counts Report above. The column must contain at least two non-zero. If you get an error, you may not have chosen a valid field. Results seem to be improved by using lemmas rather than the original tokens, but this can be changed with use_lemmas=False . The other options are more unpredicatable. The entity_types_to_use and tag_types_to_use lists allow you to specify entity and part of speech categories that should be retained in the analysis. Tokens not belonging to the types you specify will be excluded from the corpus. A list of the category abbreviations can be found in the spaCy documentation for named entities and part of speech tags . If you wish to use all the categories, set these values to All . You can also \"censor\" certain types, which replaces the original token it entity or part of speech abbreviation. Lastly, periods at the ends of tokens can be stripped if they have escaped spaCy's tokenizer. For convenience, here are lists of all entity and part of speech abbreviations, which you can use to copy and paste into the cell below. Named Entities \"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\" Parts of Speech \"$\", \"``\", \"''\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"CC\", \"CD\", \"DT\", \"EX\", \"IN\", \"LS\", \"NFP\", \"NIL\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RP\", \"SYM\", \"TO\", \"UH\", \"WDT\", \"WP\", \"WP$\", \"WRB\" You can perform quite a variety of configurations before generating your corpus: limit : A number less than or equal to the end value in Load Documents field : The metadata field you wish to explore corpus_file : The name of the corpus file (no file extension is necessary) from_file : If set to True , the notebook will look for a pre-existing corpus file before generating one from scratch. stoplist_path : Path to a custom stop word file use_lemmas : If set to True , the notebook will use lemmas (e.g. \"go\" for \"going\") instead of raw tokens. Lemmas are taken from spaCy's language model. The only modification is that the lemma of \"humanities\" is \"humanities\", not \"humanity\". entity_types_to_use : A list of the above entity types to use. If the value None is provided, all types are used. entity_types_to_censor : A list of entity types to \"censor\". This means that the tokens will be replaced by the entity label. tag_types_to_use : A list of the above tag types to use. If the value None is provided, all types are used. tag_types_to_censor : A list of tag types to \"censor\". This means that the tokens will be replaced by the tag label. strip_final_period : Removes periods (full stops) from the end of tokens if they have been missed by the tokenizer.","title":"Build a Corpus"},{"location":"modules/metadata/#generate-terms-that-differentiate-the-collection-from-a-general-english-corpus","text":"This cell provides a list of terms in your corpus that make it distinct from a general English corpus. We think that Scattertext uses the Brown University Standard Corpus of Present-Day American English (AKA the Brown Corpus) for comparison, but we have not been able to confirm this. As such, the results need to be interpreted with this uncertainty in mind. The limit configuration controls the number of terms displayed in the output.","title":"Generate terms that differentiate the collection from a general English corpus"},{"location":"modules/metadata/#generate-terms-associated-with-a-field-value","text":"This cell provides a list of terms particularly associated with a particular metadata field value within your corpus. For the score_query configuration, supply one of the row values in the Generate Column Counts Report cell. The score_label can be a more human-readable or descriptive label for the value.","title":"Generate terms associated with a field value"},{"location":"modules/metadata/#generate-a-scattertext-visualization-of-term-associations","text":"This cell generates a Scattertext visualization, which provides a rich environment for exploring your corpus. The Scattertext visualization is saved as a single HTML file at the location you specify for filename . Be sure to set limit to the same number you used for generating the corpus. The field_name value should be taken from one of the row values in the Counts Report above. In the graph, the axis for this field will be labelled with the value you provide for field_label . The other axis will be labelled by the value you provide for non_field_label . You can also modify the width of the graph and supply an extra metadata category, which will be the name of a column in the Documents table above. The values for that category will be displayed above sample documents in the graph. The results can be filtered by minimum term frequency and pointwise mutual information (the higher the number the greater the requirement that terms co-occur in the same document). Warning Scattertext HTML files are enormous and can take a very long time to load in your browser. A file generated from a corpus of 2000 documents will generally take about 15 minutes. So plan accordingly!","title":"Generate a Scattertext Visualization of Term Associations"},{"location":"modules/metadata/#topic-statistics-by-metadata","text":"This notebook extracts information form a model's topic-docs.txt file and combines it with the information in the documents' metadata fields to provide counts of the number of documents associated with specific metadata fields. Since MALLET automatically selects the top 100 documents in each topic, this is the basis for the data. The results can be viewed in pandas dataframes and saved to CSV files. The results may be visualised in a static bar chart (stacked or unstacked) or an interactive plotly bar chart for any metadata field. The visusalisations may be saved to static PNG files.","title":"Topic Statistics by Metadata"},{"location":"modules/metadata/#configuration_1","text":"Select one model to explore. Please run the next cell regardless of whether you change anything. If you are unsure of the name of your model, navigate to the your_project_name/project_data/models directory in your project, and choose the name of one of the subdirectories in that folder. Each subdirectory should be called topicsn1 , where n1 is the number of topics you chose to model, for example: selection = 'topics100' . Please follow this format exactly. Info In most cases, you should not need to change the data_path and from_file configurations. The data_path variable specifies the folder where the notebook's assets will be saved. Some cells below attempt to load assets from this data folder so that you do not need to re-run procedures if you have already run the cell once. If for some reason you wish to bypass loading data from a saved file, set from_file=False in the cell's configuration section.","title":"Configuration"},{"location":"modules/metadata/#read-topic-docs-file","text":"This cell loads the topic-docs.txt file data into a pandas dataframe.","title":"Read Topic-Docs File"},{"location":"modules/metadata/#export-data-from-top-documents","text":"Sometimes it can be useful to extract the text from the top documents in a topic in order to study them using a different tool. This cell allows you to export this content as plain text files save them as a zip archive. Before running the cell, configure the save_path with the path to the directory where you want to save the text files. If topic_num is set to All , the content of all documents will be exported. If you set it to a topic number, only the documents associated with that topic will be exported. This cell is not required for running subsequent cells.","title":"Export Data from Top Documents"},{"location":"modules/metadata/#gather-collection-metadata","text":"This cell gathers metadata for a list of JSON fields from the documents in the collection. In the dataframe output, tag attributes which exist but without subattributes have values of 1 ; missing tag attributes have values of 0 . Tags with values like \"funding/US private college\" are represented on the table under the \"funding\" column with the value \"US private colleges\". You can drag column boundaries to change their width or column labels to re-order the columns. To sort the columns, click the column label (and click again to sort in reverse order). Click the filter icon to filter your data by column values. Note: This cell can take some time to run. If you have already run it once, it should read the metadata from a saved file. Set from_file=False to re-generate the data.","title":"Gather Collection Metadata"},{"location":"modules/metadata/#save-to-csv","text":"If you wish to save a copy of the output of Gather Collection Metadata to a CSV file, set save_path to a filename to save the CSV to. In the sample below, N represents the topic number, which is recommended so that you do not overwrite a file from a different model. By default, the CSV will reflect the table above after any modifications you make by filtering or sorting. If you wish to use the original table, set use_original=True .","title":"Save to CSV"},{"location":"modules/metadata/#get-counts-by-field-values","text":"This cell calculates document counts for each topic by field value using the topic_docs_metadata dataframe. In the configuration section, set field to the name of the metadata field you wish to calculate. Set save_path to a path to a CSV file if you wish to save the table. Set use_original=True if you wish the file to contain the original output. Otherwise, it will reflect any changes you make such as sorting and filtering.","title":"Get Counts by Field Values"},{"location":"modules/metadata/#visualise-metadata-with-a-simple-bar-plot-static-version","text":"This cell generates a simple bar plot for visualising the data generated by the notebook. Set fields to a list of column headings in the sums_and_means dataframe. If you wish to display different names in the legend, provide a list of corresponding names for legend_labels (in the same order). You may adjust the title , xlabel (for the x-axis) and ylabel (for the y-axis) to describe the content of your data accurately. Since plots can be very cramped, you may want to look at a limited range of topics. To do this, modify the start_topic and end_topic values. You can also save space by creating a stacked plot with stacked=True . (The interactive plot in the next cell provides another option with pan and zoom features.) To save the plot a file, set save_path to a full file path, including the filename. The type of file is inferred from the extension. For instance, files ending in .png will be saved as PNG files and files ending in .pdf will be saved as PDF files. SVG format is also available.","title":"Visualise Metadata with a Simple Bar Plot (Static Version)"},{"location":"modules/metadata/#visualise-metadata-with-a-plotly-bar-plot-interactive-version","text":"The interactive plot (using Plotly) takes the same settings as the static plot above, except the stacked mode is not available. However, because Plotly provides zoom and pan features, it is possible to display the entire range of topics in a single graph. Click and drag over the graph to zoom in on a location. Click the home icon in the Plotly toolbar to restore the default zoom level. Click on the boxes in the legend to show and hide specific categories. Double-click to restore the default display. You can download the plot as PNG file by clicking the camera icon in the Plotly toolbar. If you wish to save the interactive plot as a standalone web page, set the save_path to a full file path, including the filename ending in .html .","title":"Visualise Metadata with a Plotly Bar Plot (Interactive Version)"},{"location":"modules/metadata/#generate-topic-doc-dictionary-optional-utility","text":"This cell generates a dictionary with topic numbers as keys and a list of filenames in each topic as the values. Individual topics can be expected with topic_docs_dict[1] , where \"1\" is the desired topic number. The dictionary can be saved as a JSON file.","title":"Generate Topic-Doc Dictionary (Optional Utility)"},{"location":"modules/metadata/#module-structure","text":"metadata \u2523 data \u2523 scripts \u2503 \u2523 add_metadata.py \u2503 \u2523 scattertext.py \u2503 \u2517 topic_stats.py \u2523 add_metadata.ipynb \u2523 README.md \u2523 scattertext.ipynb \u2517 topic_statistics_by_metadata.ipynb","title":"Module Structure"},{"location":"modules/pyldavis/","text":"pyLDAvis About This Module pyLDAvis is a port of the R LDAvis package for interactive topic model visualization by Carson Sievert and Kenny Shirley. pyLDAvis is designed to help users interpret the topics in a topic model by examining the relevance and salience of terms in topics. Once a pyLDAvis object has been generated, many of its properties can be inspected as in tabular form as a way to examine the model. However, the main output is a visualization of the relevance and salience of key terms to the topics. pyLDAvis is not designed to use MALLET data out of the box. This notebook transforms the MALLET state file into the appropriate data formats before generating the visualization. The code is based on Jeri Wieringa's blog post Using pyLDAvis with Mallet and has been slightly altered and commented. The main module for this notebook is create_pyldavis.ipynb . User Guide Settings The Settings cell defines paths and important variables used to create the pyLDAVis visualizations. The default settings will create a folder inside the pyLDAvis module for each topic model in your project (you can select specific models in the Configuration cell below). In most case, you will not need to change the default settings. Configuration Select models to create pyLDAvis visualizations for. Please run the next cell regardless of whether you change anything. By default, this notebook is set to create a pyLDAvis for all of the models in your project models directory. If you would like to select only certain models to produce a pyLDAvis for, make those selections in the next cell (see next paragraph). Otherwise leave the value for selection as All , which is the default. To produce pyLDAvis for a selection of the models you created, but not all: Navigate to the your_project_name/project_data/models directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which subdirectory/ies you would like to produce browsers for, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to this: Example: selection = ['topics50','topics75'] Once you have configured your topic models, run the next cell in the section to ensure that all of the models you selected are detectable by the pyLDAvis module. Add Metadata and Labels for the User Interface This section is option and is for more advanced users. Skip this cell if you want to generate a basic pyLDAvis plot. The pyLDAvis plot can be customized to display metadata information in your project's json files. To do this, identify the index number for each model in the list displayed by the second Configuration cell and add the necessary information using the following lines in the next cell. models [ 0 ][ 'metadata' ] = 'pub' models [ 0 ][ 'ui_labels' ] = [ 'Intertopic Distance Map (via multidimensional scaling)' , 'topic' , 'publication' , 'publications' , 'tokens' ] Remember that the first model is models[0] . Additional models would be models[1] , models[2] , etc. For the metadata line, enter the json field you would like to use. For instance, a basic pyLDAvis displays your model's documents. If you wish to display, publications, you could use the pub field. The ui_labels must be given in the following order: The title of the multidimensional scaling graph The type of unit represented by the graph circles The singular form of the unit represented in the bar graphs on the right The plural form of the unit represented in the bar graph on the right. The unit represented by the percentage in the Relevance display. The example above indicates that the model will represent a map of intertopic distances in which each topic will show the distribution of publications, as represented by the percentage of topic tokens in the publication. Hint If you are unsure what to put, you do not have to assign ui_labels . A visualization will still be generated but may not have appropriate labels for the type of metadata you are using. Generate Visualizations This cell generates the pyLDAvis visualizations for all selected topic models. The output is a set of links to all pyLDAvis visualizations in your project folder. See the next cell if you wish to make them public. Since this cell can take some time to run, the output is captured instead of shown as the script is processing. Run the following output.show() cell when it is finished to check that everything ran as expected. Create Zipped Copies of your Visualizations for Export This section allows you to create zip archives of your pyLDAvis visualizations for export. By default, visualizations for all available models will be zipped. If you wish to zip only one model, change the models setting to indicate the name of the model folder (e.g. 'topics25' ). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. ['topics25', 'topics50'] ). Access pyLDAvis Data Attributes (Optional) pyLDAvis generates a number of useful variables which it can be helpful for understanding the data underlying the visualization or for use in other applications. These variables can be accessed via the vis object created in Generate the Visualizations section. You can view the data by calling show_attribute() with the appropriate attribute configured. Possible attributes are model_state (a matrix o the models state file), hyperparameters , alpha , beta , doc_lengths (document lengths), phi (the matrix of topic-term distributions), phi_df (a pivoted dataframe of phi), theta (the matrix of document-topic), theta_df (a pivoted dataframe of theta), vocab (a matrix of term frequencies). Further information about these attributes can be found in the discussion of Jeri Wieringa's blog post Using pyLDAvis with Mallet . You can restrict the number of lines shown by modifying the start and end settings. If you wish to save the result to a file, set the save_path . Tabular data should be saved to a csv file; everything else can be plain text. Module Structure pyldavis \u2523 scripts \u2503 \u2523 PyLDAvis.py \u2503 \u2523 PyLDAvis_custom.js \u2503 \u2517 zip.py \u2523 jupyter-jupyter-logo: create_pyldavis.ipynb \u2517 README.md","title":"pyldavis"},{"location":"modules/pyldavis/#pyldavis","text":"","title":"pyLDAvis"},{"location":"modules/pyldavis/#about-this-module","text":"pyLDAvis is a port of the R LDAvis package for interactive topic model visualization by Carson Sievert and Kenny Shirley. pyLDAvis is designed to help users interpret the topics in a topic model by examining the relevance and salience of terms in topics. Once a pyLDAvis object has been generated, many of its properties can be inspected as in tabular form as a way to examine the model. However, the main output is a visualization of the relevance and salience of key terms to the topics. pyLDAvis is not designed to use MALLET data out of the box. This notebook transforms the MALLET state file into the appropriate data formats before generating the visualization. The code is based on Jeri Wieringa's blog post Using pyLDAvis with Mallet and has been slightly altered and commented. The main module for this notebook is create_pyldavis.ipynb .","title":"About This Module"},{"location":"modules/pyldavis/#user-guide","text":"","title":"User Guide"},{"location":"modules/pyldavis/#settings","text":"The Settings cell defines paths and important variables used to create the pyLDAVis visualizations. The default settings will create a folder inside the pyLDAvis module for each topic model in your project (you can select specific models in the Configuration cell below). In most case, you will not need to change the default settings.","title":"Settings"},{"location":"modules/pyldavis/#configuration","text":"Select models to create pyLDAvis visualizations for. Please run the next cell regardless of whether you change anything. By default, this notebook is set to create a pyLDAvis for all of the models in your project models directory. If you would like to select only certain models to produce a pyLDAvis for, make those selections in the next cell (see next paragraph). Otherwise leave the value for selection as All , which is the default. To produce pyLDAvis for a selection of the models you created, but not all: Navigate to the your_project_name/project_data/models directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which subdirectory/ies you would like to produce browsers for, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to this: Example: selection = ['topics50','topics75'] Once you have configured your topic models, run the next cell in the section to ensure that all of the models you selected are detectable by the pyLDAvis module.","title":"Configuration"},{"location":"modules/pyldavis/#add-metadata-and-labels-for-the-user-interface","text":"This section is option and is for more advanced users. Skip this cell if you want to generate a basic pyLDAvis plot. The pyLDAvis plot can be customized to display metadata information in your project's json files. To do this, identify the index number for each model in the list displayed by the second Configuration cell and add the necessary information using the following lines in the next cell. models [ 0 ][ 'metadata' ] = 'pub' models [ 0 ][ 'ui_labels' ] = [ 'Intertopic Distance Map (via multidimensional scaling)' , 'topic' , 'publication' , 'publications' , 'tokens' ] Remember that the first model is models[0] . Additional models would be models[1] , models[2] , etc. For the metadata line, enter the json field you would like to use. For instance, a basic pyLDAvis displays your model's documents. If you wish to display, publications, you could use the pub field. The ui_labels must be given in the following order: The title of the multidimensional scaling graph The type of unit represented by the graph circles The singular form of the unit represented in the bar graphs on the right The plural form of the unit represented in the bar graph on the right. The unit represented by the percentage in the Relevance display. The example above indicates that the model will represent a map of intertopic distances in which each topic will show the distribution of publications, as represented by the percentage of topic tokens in the publication. Hint If you are unsure what to put, you do not have to assign ui_labels . A visualization will still be generated but may not have appropriate labels for the type of metadata you are using.","title":"Add Metadata and Labels for the User Interface"},{"location":"modules/pyldavis/#generate-visualizations","text":"This cell generates the pyLDAvis visualizations for all selected topic models. The output is a set of links to all pyLDAvis visualizations in your project folder. See the next cell if you wish to make them public. Since this cell can take some time to run, the output is captured instead of shown as the script is processing. Run the following output.show() cell when it is finished to check that everything ran as expected.","title":"Generate Visualizations"},{"location":"modules/pyldavis/#create-zipped-copies-of-your-visualizations-for-export","text":"This section allows you to create zip archives of your pyLDAvis visualizations for export. By default, visualizations for all available models will be zipped. If you wish to zip only one model, change the models setting to indicate the name of the model folder (e.g. 'topics25' ). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. ['topics25', 'topics50'] ).","title":"Create Zipped Copies of your Visualizations for Export"},{"location":"modules/pyldavis/#access-pyldavis-data-attributes-optional","text":"pyLDAvis generates a number of useful variables which it can be helpful for understanding the data underlying the visualization or for use in other applications. These variables can be accessed via the vis object created in Generate the Visualizations section. You can view the data by calling show_attribute() with the appropriate attribute configured. Possible attributes are model_state (a matrix o the models state file), hyperparameters , alpha , beta , doc_lengths (document lengths), phi (the matrix of topic-term distributions), phi_df (a pivoted dataframe of phi), theta (the matrix of document-topic), theta_df (a pivoted dataframe of theta), vocab (a matrix of term frequencies). Further information about these attributes can be found in the discussion of Jeri Wieringa's blog post Using pyLDAvis with Mallet . You can restrict the number of lines shown by modifying the start and end settings. If you wish to save the result to a file, set the save_path . Tabular data should be saved to a csv file; everything else can be plain text.","title":"Access pyLDAvis Data Attributes (Optional)"},{"location":"modules/pyldavis/#module-structure","text":"pyldavis \u2523 scripts \u2503 \u2523 PyLDAvis.py \u2503 \u2523 PyLDAvis_custom.js \u2503 \u2517 zip.py \u2523 jupyter-jupyter-logo: create_pyldavis.ipynb \u2517 README.md","title":"Module Structure"},{"location":"modules/topic-bubbles/","text":"Topic Bubbles About This Module This module creates a topic bubbles visualization from dfr-browser data generated in the dfr-browser notebook or from model data generated in the topic modeling notebook. This module uses scripts originally written by Sihwa Park for the WE1S project. For more information on Park's script, see Park's topic bubbles Github repo and the README.md located in this module's tb_scripts folder. Notebooks create_topic_bubbles.ipynb is the main notebook for the module. User Guide Settings The Settings cell defines paths and important variables used to create a topic bubbles visualization. The default settings will create a folder inside the topic_bubbles module for each topic model in your project or for a selection of models. In most cases, you will not need to change the default settings. Create Topic Bubbles Using Dfr-Browser Metadata If you ran the dfr-browser module to create dfr-browser visualizations for your models, the next cells will import data produced via that module into the topic_bubbles module. If you did not run the dfr_browser module, skip to the next section: Create Topic Bubbles without Dfr-Browser Metadata . By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the dfr_browser module. This means that the selection variable below is set to its default value of All ( selection = 'All' ). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to All . You must run this cell regardless of whether you change anything. To produce topic bubbles for a selection of models: Navigate to the modules/dfr_browser directory and look for subdirectories titled topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to selection = ['topics50', 'topics75'] . Create Topic Bubbles without Dfr-Browser Metadata If you have not yet created Dfr-browsers for this project, run the following cells to create your Topic Bubbles visualization. By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the dfr_browser module. This means that the selection variable below is set to its default value of All ( selection = 'All' ). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to All . You must run this cell regardless of whether you change anything. To produce topic bubbles for a selection of models: Navigate to the modules/dfr_browser directory and look for subdirectories titled topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to selection = ['topics50', 'topics75'] . Create Files Needed for Topic Bubbles Select Models for Which You Would Like to Create Visualizations By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the dfr_browser module. This means that the selection variable below is set to its default value of All ( selection = 'All' ). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to All . You must run this cell regardless of whether you change anything. To produce topic bubbles for a selection of models: Navigate to the modules/dfr_browser directory and look for subdirectories titled topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to selection = ['topics50', 'topics75'] . The get_model_state() function in the second cell grabs the filepaths of model subdirectories in order to visualize and their state and scaled files. Optionally, you can instead set values for subdir_list , state_file_list , and scaled_file_list manually in the third cell. The create_topicbubbles() function creates the files needed for topic bubbles, using the model state and scaled files for all selected modelsand Goldstone's prepare_data.py script (to produce the dfr-browser files needed for topic bubbles). It prints output from Goldstone's prepare_data.py script to the notebook cell. Create Zipped Copies of Your Visualizations (Optional) This section zips up your topic bubbles visualizations for serving on a different machine or server. By default, browsers for all available models will be zipped. If you wish to zip only one model, change the models setting to indicate the name of the model folder (e.g. 'topics25' ). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. ['topics25', 'topics50'] ). This section also includes instructions for downloading and running your topic bubble visualization(s) on a different machine (i.e., outside of the WE1S container system). Module Structure topic_bubbles \u2523 scripts \u2503 \u2523 create_dfrbrowser.py \u2503 \u2523 create_topic_bubbles.py \u2503 \u2523 zip.py \u2523 tb_scripts \u2503 \u2523 css \u2503 \u2503 \u2517 style.css \u2503 \u2523 data \u2503 \u2503 \u2517 config.json \u2503 \u2523 img \u2503 \u2503 \u2523 screenshot.png \u2503 \u2503 \u2517 we1s_logo.png \u2503 \u2523 js \u2503 \u2503 \u2523 d3-mouse-event.js \u2503 \u2503 \u2523 script.js \u2503 \u2503 \u2523 utils.min.js \u2503 \u2503 \u2517 worker.min.js \u2503 \u2523 lib \u2503 \u2523 index.html \u2503 \u2523 LICENSE \u2503 \u2517 README.md \u2523 create_topic_bubbles.ipynb \u2517 README.md","title":"topic_bubbles"},{"location":"modules/topic-bubbles/#topic-bubbles","text":"","title":"Topic Bubbles"},{"location":"modules/topic-bubbles/#about-this-module","text":"This module creates a topic bubbles visualization from dfr-browser data generated in the dfr-browser notebook or from model data generated in the topic modeling notebook. This module uses scripts originally written by Sihwa Park for the WE1S project. For more information on Park's script, see Park's topic bubbles Github repo and the README.md located in this module's tb_scripts folder.","title":"About This Module"},{"location":"modules/topic-bubbles/#notebooks","text":"create_topic_bubbles.ipynb is the main notebook for the module.","title":"Notebooks"},{"location":"modules/topic-bubbles/#user-guide","text":"","title":"User Guide"},{"location":"modules/topic-bubbles/#settings","text":"The Settings cell defines paths and important variables used to create a topic bubbles visualization. The default settings will create a folder inside the topic_bubbles module for each topic model in your project or for a selection of models. In most cases, you will not need to change the default settings.","title":"Settings"},{"location":"modules/topic-bubbles/#create-topic-bubbles-using-dfr-browser-metadata","text":"If you ran the dfr-browser module to create dfr-browser visualizations for your models, the next cells will import data produced via that module into the topic_bubbles module. If you did not run the dfr_browser module, skip to the next section: Create Topic Bubbles without Dfr-Browser Metadata . By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the dfr_browser module. This means that the selection variable below is set to its default value of All ( selection = 'All' ). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to All . You must run this cell regardless of whether you change anything. To produce topic bubbles for a selection of models: Navigate to the modules/dfr_browser directory and look for subdirectories titled topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to selection = ['topics50', 'topics75'] .","title":"Create Topic Bubbles Using Dfr-Browser Metadata"},{"location":"modules/topic-bubbles/#create-topic-bubbles-without-dfr-browser-metadata","text":"If you have not yet created Dfr-browsers for this project, run the following cells to create your Topic Bubbles visualization. By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the dfr_browser module. This means that the selection variable below is set to its default value of All ( selection = 'All' ). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to All . You must run this cell regardless of whether you change anything. To produce topic bubbles for a selection of models: Navigate to the modules/dfr_browser directory and look for subdirectories titled topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to selection = ['topics50', 'topics75'] .","title":"Create Topic Bubbles without Dfr-Browser Metadata"},{"location":"modules/topic-bubbles/#create-files-needed-for-topic-bubbles","text":"","title":"Create Files Needed for Topic Bubbles"},{"location":"modules/topic-bubbles/#select-models-for-which-you-would-like-to-create-visualizations","text":"By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the dfr_browser module. This means that the selection variable below is set to its default value of All ( selection = 'All' ). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to All . You must run this cell regardless of whether you change anything. To produce topic bubbles for a selection of models: Navigate to the modules/dfr_browser directory and look for subdirectories titled topicsn1 , where n1 is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to selection = ['topics50', 'topics75'] . The get_model_state() function in the second cell grabs the filepaths of model subdirectories in order to visualize and their state and scaled files. Optionally, you can instead set values for subdir_list , state_file_list , and scaled_file_list manually in the third cell. The create_topicbubbles() function creates the files needed for topic bubbles, using the model state and scaled files for all selected modelsand Goldstone's prepare_data.py script (to produce the dfr-browser files needed for topic bubbles). It prints output from Goldstone's prepare_data.py script to the notebook cell.","title":"Select Models for Which You Would Like to Create Visualizations"},{"location":"modules/topic-bubbles/#create-zipped-copies-of-your-visualizations-optional","text":"This section zips up your topic bubbles visualizations for serving on a different machine or server. By default, browsers for all available models will be zipped. If you wish to zip only one model, change the models setting to indicate the name of the model folder (e.g. 'topics25' ). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. ['topics25', 'topics50'] ). This section also includes instructions for downloading and running your topic bubble visualization(s) on a different machine (i.e., outside of the WE1S container system).","title":"Create Zipped Copies of Your Visualizations (Optional)"},{"location":"modules/topic-bubbles/#module-structure","text":"topic_bubbles \u2523 scripts \u2503 \u2523 create_dfrbrowser.py \u2503 \u2523 create_topic_bubbles.py \u2503 \u2523 zip.py \u2523 tb_scripts \u2503 \u2523 css \u2503 \u2503 \u2517 style.css \u2503 \u2523 data \u2503 \u2503 \u2517 config.json \u2503 \u2523 img \u2503 \u2503 \u2523 screenshot.png \u2503 \u2503 \u2517 we1s_logo.png \u2503 \u2523 js \u2503 \u2503 \u2523 d3-mouse-event.js \u2503 \u2503 \u2523 script.js \u2503 \u2503 \u2523 utils.min.js \u2503 \u2503 \u2517 worker.min.js \u2503 \u2523 lib \u2503 \u2523 index.html \u2503 \u2523 LICENSE \u2503 \u2517 README.md \u2523 create_topic_bubbles.ipynb \u2517 README.md","title":"Module Structure"},{"location":"modules/topic-modeling/","text":"Topic Modeling About This Module This module uses MALLET to topic model project data. Prior to modelling, the notebooks extract word vectors from the project's JSON files into a single doc_terms.txt file, which is then imported into MALLET. Stop words are stripped at this stage. By default, the WE1S Standard Stoplist contained in the scripts folder is used for this process. After modelling is complete, there is an option to create a scaled data file for use in visualisations such as dfr-browser and pyLDAvis . The main notebook for this module is model_topics.ipynb . User Guide This notebook performs topic modelling with MALLET by providing an easy-to-configure interface for the two basic steps, importing your data to MALLET and training your topics. The WE1S project performs a preprocessing step (creating the file to input to MALLET) before the import step and a post-processing step (scaling) after training. These steps are explained below. Both the preprocessing and the postprocessing steps are optional. These instructions provide a step-by-step guide to using the notebook, once cell at a time. Each cell is referenced here by its title. Note that when importing data to MALLET or creating topic models, files with the same name will be over-written. If you use different filenames, files from any previous runs will not be deleted. Settings The purpose of this cell is to load some external Python code and define some commonly used file paths. If you encounter errors in any of the subsequent cells, it is worth re-running the Settings cell to check that all variables have been defined and all external code has been loaded. In general, you can run this cell without changing anything. You are most likely to want to customise the following settings: log_file , language_model , stoplist_file . The log_file is a text file that lists errors generated when you run the Create File for Importing to MALLET cell. By default, it is saved to your models directory, but you can save it elsewhere if you wish. The language_model is the spaCy language model to use when tokenising your text if you run Create File for Importing to MALLET without a pre-tokenised collection of documents. More information on this is given in the instructions for Create File for Importing to MALLET below. The stoplist_file is the full path to the file containing stop words (words to be omitted from your model). The default setting points to a copy of the WE1S Standard Stoplist stored in the module's scripts folder. If you wish to use a different stoplist, set stoplist_file to the full path to your stoplist's location (including the filename). Create File for Importing to MALLET MALLET has two methods of importing data: from a flat directory of plain text files or a single file with one document per line. By default, the WE1S project uses the latter method. Create File for Importing to MALLET is a preprocessing step that collects data from your project's JSON files and assembles it into a doc_terms file, which is saved as models/doc_terms.txt in you project folder. Hint If you are running a model on a directory of plain text documents and you want to process them purely with MALLET, you can skip this cell.** Note that the file that is produced is a plain-text file containing 1 document in the corpus per line. To produce what we call the doc_terms file, each term in a document is counted, and the term is then written however many times it occurs in the document to the document's row in the doc_terms file. For example, if the word \"humanities\" occurs twice in a document, the row in the doc_terms file will read \"humanities humanites\". The document's row in the doc_terms file is a string of repeated terms like this, separated by spaces (what is sometimes called a bag of words). The prepare MALLET import script ( scripts/prepare_mallet_import.py ) creates a PrepareMalletImport object that handles the creation of a text file to be imported by MALLET. Although a PrepareMalletImport object can be created on a document with mimport=PrepareMalletImport(0, 'manifest_path.json', 'mallet_import.txt', model='en_core_web_sm', stoplist='we1s_standard_stoplist.txt') , it will typically be used with a directory of json files. This function loops through a sorted list of json files in the directory, creates a MalletImport object from each one, and saves its bag of words as a row in the doc_terms.txt file. This is the file that will be imported by MALLET. If stop words are to be filtered prior to topic modelling, the prepare_data() function in prepare_mallet_import.py should be fed a stoplist file. If you do not want to strip stop words, give it an empty file. prepare_mallet_import.py first looks for a bag_of_words field in your JSON files. A bag_of_words is a pre-packaged set of words already counted. If the field is not present, it will next look for a features field, extract the tokens from that field, and generate a bag of words. In both cases, the tokens are assumed to have been previously tokenised using the WE1S preprocessor. If neither the bag_of_words nor features field is present, the script will attempt to tokenise the text in the content field on the fly using a slimmed-down version of the WE1S preprocessor. Note that this process will necessarily be slower and can take a long time for large projects. Tokenisation uses the Python spaCy package, which predicts linguistic features based on a language model. If spaCy is called to do the tokenisation, it will use the language model your designated in Settings in most cases, en_core_web_sm will be sufficient. Note that the language model must be installed in your environment for this process to work. Normally you will run Create File for Importing to MALLET without changing any of the settings. When it finishes, you will see a preview of the beginning of the doc_terms.txt file. By default, five rows will be displayed, with each row clipped at 200 characters. You can change these settings in the final line of the cell, or remove them if you wish to display the whole file (not recommended in a Jupyter notebook). You can also navigate to models/doc_terms.txt and download or open the file to inspect it. Each row in the doc_terms.txt file is one document in your corpus, and each row lists the document's filename, its index number, and its bag of words. Setup MALLET In the first cell, you configure a list of models you wish to run. Models are listed by the number of topics you select. For instance, if you wish to run three models of 25, 50, and 100 topics each, you should set num_topics = [25, 50, 100] . The second cell instantiates a Mallet object (referenced as mallet and creates subdirectories in your projects models folder for each model you listed. Important The Mallet object is pre-configured with MALLET settings used by the WE1S project. These settings are given below. `import_file_path=import_file_path `import_sources='file' num_iterations=1000 optimize_interval=10 use_random_seed=True random_seed=10 keep_sequence=True preserve_case=False `token_regex=\"\\S+\" remove_stopwords=False extra_stopwords=None stoplist_file=None generate_diagnostics=True You can list the values for any of these settings with print(mallet.import_file_path) , print(mallet.random_seed) , etc. The first two options set the data source to be a file read from the location of the import_file_path configured in Settings . If a setting is False or None , it is not used by MALLET for importing data or training the models. In addition, random_seed is ignored if use_random_seed is False . Because WE1S input is pre-tokenised and has stop words removed, remove_stopwords is set to False and the token_regex setting just splits the doc_terms file on whitespace between words. Once you have run this cell, you are ready to begin importing your data to MALLET. However, you may need to adjust MALLET's configuration as described below. Custom Configuration Once the setup is complete, you can change any of the MALLET settings below with commands like mallet.optimize_interval = 11 (this model goes to 11!). You can use most arguments available in MALLET on the command line (see MALLET's documentation ), but replace hyphens with underscores. For example, reference MALLET's --num-iterations argument in this notebook with mallet.num_iterations . Set you custom configurations in the cell in this section. For instance, if you wish to change the optimize_interval setting to '11', add mallet.optimize_interval = 11 (this model goes to 11!). Another example with num_iterations = 1000 . Importing Plain Text files One common use case for custom configurations is if you wish to import data from a directory of plain text files. You can do this with mallet.import_source='path_to_directory' . You can also choose MALLET's default tokenisation and stop words instead of the WE1S tokenisation algorithm and stoplist using mallet.token_regex=None and mallet.remove_stopwords=True . These configurations are provided for convenience in the cell below. You just have to uncomment them and run the cell. Note that using plain text files that just contain document contents (and not metadata) as your data source means that certain visualisation tools like Dfr-Browser and Topic Bubbles, which require metadata, will not be usable. Therefore, this method is not recommended. If you wish to generate these visualisations, it is best to use the import module to import your data into your project's json folder first. Import Data to MALLET You can probably simply run this cell as is, and the import process will begin. It may take a long time if your collection is large. By default, the topic list you supplied in Setup MALLET will be used. However, if, for instance, you wish to import data for only models 25 and 50, you can also provide a topic list here by typing mallet.import_models([25, 50]) . This cell generates a MALLET command and uses it to call MALLET. If you run into a problem and you wish to see the MALLET command, create a new cell and run print(mallet.import_command) . Once the import process is complete, you are ready to begin training your models. Train Models As with the previous cell, you can probably simply run this cell as is. Likewise, if you do not wish to train the models specified in Setup MALLET , you can supply a list of models here by typing mallet.train_models([25, 50]) . When the training begins, MALLET gives continuous feedback with each iteration of the modelling process. By default, this feedback is hidden, and a progress bar indicates how close the model is to completion. You may wish to change this behaviour with one of the following settings: mallet.train_models(progress_bar=False) : Display a plain text progress indicator every 10%. mallet.train_models(capture_output=True) : Capture the output and display it only when training is complete. This is useful for job that takes a long time because it allows you to close the window. mallet.train_models(log_file='path_to_mallet_log.txt') : Save the output to a log file at the path specified. This is useful if you wish to save a record of MALLET's feedback. If you run into a problem, you can inspect the last MALLET command by creating a new cell and running print(mallet.train_command) . Scale Topics This cell uses Multidimensional Scaling (MDS) to adjust the topic weights for use in visualisation tools such as Dfr-Browser, pyLDAvis, and Topic Bubbles. These modules will not work properly if you do not perform scaling by running this cell. The generated scaling information is stored as topic_scaled.csv in the model's directory. By default, scaling files will be generated for the models you configured in Setup MALLET above. If you wish to specify which models to scale in this cell, replace num_topics with a list of topic desired topic numbers (e.g. [50, 100] ) in the code below. Module Structure topic_modeling \u2523 scripts \u2503 \u2523 scale_topics.py \u2503 \u2523 timer.py \u2503 \u2523 mallet.py \u2503 \u2523 prepare_mallet_import.py \u2503 \u2523 slow.py \u2503 \u2523 timer.py \u2503 \u2517 we1s_standard_stoplist.txt \u2523 model_topics.ipynb \u2523 README.md","title":"topic_modeling"},{"location":"modules/topic-modeling/#topic-modeling","text":"","title":"Topic Modeling"},{"location":"modules/topic-modeling/#about-this-module","text":"This module uses MALLET to topic model project data. Prior to modelling, the notebooks extract word vectors from the project's JSON files into a single doc_terms.txt file, which is then imported into MALLET. Stop words are stripped at this stage. By default, the WE1S Standard Stoplist contained in the scripts folder is used for this process. After modelling is complete, there is an option to create a scaled data file for use in visualisations such as dfr-browser and pyLDAvis . The main notebook for this module is model_topics.ipynb .","title":"About This Module"},{"location":"modules/topic-modeling/#user-guide","text":"This notebook performs topic modelling with MALLET by providing an easy-to-configure interface for the two basic steps, importing your data to MALLET and training your topics. The WE1S project performs a preprocessing step (creating the file to input to MALLET) before the import step and a post-processing step (scaling) after training. These steps are explained below. Both the preprocessing and the postprocessing steps are optional. These instructions provide a step-by-step guide to using the notebook, once cell at a time. Each cell is referenced here by its title. Note that when importing data to MALLET or creating topic models, files with the same name will be over-written. If you use different filenames, files from any previous runs will not be deleted.","title":"User Guide"},{"location":"modules/topic-modeling/#settings","text":"The purpose of this cell is to load some external Python code and define some commonly used file paths. If you encounter errors in any of the subsequent cells, it is worth re-running the Settings cell to check that all variables have been defined and all external code has been loaded. In general, you can run this cell without changing anything. You are most likely to want to customise the following settings: log_file , language_model , stoplist_file . The log_file is a text file that lists errors generated when you run the Create File for Importing to MALLET cell. By default, it is saved to your models directory, but you can save it elsewhere if you wish. The language_model is the spaCy language model to use when tokenising your text if you run Create File for Importing to MALLET without a pre-tokenised collection of documents. More information on this is given in the instructions for Create File for Importing to MALLET below. The stoplist_file is the full path to the file containing stop words (words to be omitted from your model). The default setting points to a copy of the WE1S Standard Stoplist stored in the module's scripts folder. If you wish to use a different stoplist, set stoplist_file to the full path to your stoplist's location (including the filename).","title":"Settings"},{"location":"modules/topic-modeling/#create-file-for-importing-to-mallet","text":"MALLET has two methods of importing data: from a flat directory of plain text files or a single file with one document per line. By default, the WE1S project uses the latter method. Create File for Importing to MALLET is a preprocessing step that collects data from your project's JSON files and assembles it into a doc_terms file, which is saved as models/doc_terms.txt in you project folder. Hint If you are running a model on a directory of plain text documents and you want to process them purely with MALLET, you can skip this cell.** Note that the file that is produced is a plain-text file containing 1 document in the corpus per line. To produce what we call the doc_terms file, each term in a document is counted, and the term is then written however many times it occurs in the document to the document's row in the doc_terms file. For example, if the word \"humanities\" occurs twice in a document, the row in the doc_terms file will read \"humanities humanites\". The document's row in the doc_terms file is a string of repeated terms like this, separated by spaces (what is sometimes called a bag of words). The prepare MALLET import script ( scripts/prepare_mallet_import.py ) creates a PrepareMalletImport object that handles the creation of a text file to be imported by MALLET. Although a PrepareMalletImport object can be created on a document with mimport=PrepareMalletImport(0, 'manifest_path.json', 'mallet_import.txt', model='en_core_web_sm', stoplist='we1s_standard_stoplist.txt') , it will typically be used with a directory of json files. This function loops through a sorted list of json files in the directory, creates a MalletImport object from each one, and saves its bag of words as a row in the doc_terms.txt file. This is the file that will be imported by MALLET. If stop words are to be filtered prior to topic modelling, the prepare_data() function in prepare_mallet_import.py should be fed a stoplist file. If you do not want to strip stop words, give it an empty file. prepare_mallet_import.py first looks for a bag_of_words field in your JSON files. A bag_of_words is a pre-packaged set of words already counted. If the field is not present, it will next look for a features field, extract the tokens from that field, and generate a bag of words. In both cases, the tokens are assumed to have been previously tokenised using the WE1S preprocessor. If neither the bag_of_words nor features field is present, the script will attempt to tokenise the text in the content field on the fly using a slimmed-down version of the WE1S preprocessor. Note that this process will necessarily be slower and can take a long time for large projects. Tokenisation uses the Python spaCy package, which predicts linguistic features based on a language model. If spaCy is called to do the tokenisation, it will use the language model your designated in Settings in most cases, en_core_web_sm will be sufficient. Note that the language model must be installed in your environment for this process to work. Normally you will run Create File for Importing to MALLET without changing any of the settings. When it finishes, you will see a preview of the beginning of the doc_terms.txt file. By default, five rows will be displayed, with each row clipped at 200 characters. You can change these settings in the final line of the cell, or remove them if you wish to display the whole file (not recommended in a Jupyter notebook). You can also navigate to models/doc_terms.txt and download or open the file to inspect it. Each row in the doc_terms.txt file is one document in your corpus, and each row lists the document's filename, its index number, and its bag of words.","title":"Create File for Importing to MALLET"},{"location":"modules/topic-modeling/#setup-mallet","text":"In the first cell, you configure a list of models you wish to run. Models are listed by the number of topics you select. For instance, if you wish to run three models of 25, 50, and 100 topics each, you should set num_topics = [25, 50, 100] . The second cell instantiates a Mallet object (referenced as mallet and creates subdirectories in your projects models folder for each model you listed. Important The Mallet object is pre-configured with MALLET settings used by the WE1S project. These settings are given below. `import_file_path=import_file_path `import_sources='file' num_iterations=1000 optimize_interval=10 use_random_seed=True random_seed=10 keep_sequence=True preserve_case=False `token_regex=\"\\S+\" remove_stopwords=False extra_stopwords=None stoplist_file=None generate_diagnostics=True You can list the values for any of these settings with print(mallet.import_file_path) , print(mallet.random_seed) , etc. The first two options set the data source to be a file read from the location of the import_file_path configured in Settings . If a setting is False or None , it is not used by MALLET for importing data or training the models. In addition, random_seed is ignored if use_random_seed is False . Because WE1S input is pre-tokenised and has stop words removed, remove_stopwords is set to False and the token_regex setting just splits the doc_terms file on whitespace between words. Once you have run this cell, you are ready to begin importing your data to MALLET. However, you may need to adjust MALLET's configuration as described below.","title":"Setup MALLET"},{"location":"modules/topic-modeling/#custom-configuration","text":"Once the setup is complete, you can change any of the MALLET settings below with commands like mallet.optimize_interval = 11 (this model goes to 11!). You can use most arguments available in MALLET on the command line (see MALLET's documentation ), but replace hyphens with underscores. For example, reference MALLET's --num-iterations argument in this notebook with mallet.num_iterations . Set you custom configurations in the cell in this section. For instance, if you wish to change the optimize_interval setting to '11', add mallet.optimize_interval = 11 (this model goes to 11!). Another example with num_iterations = 1000 .","title":"Custom Configuration"},{"location":"modules/topic-modeling/#importing-plain-text-files","text":"One common use case for custom configurations is if you wish to import data from a directory of plain text files. You can do this with mallet.import_source='path_to_directory' . You can also choose MALLET's default tokenisation and stop words instead of the WE1S tokenisation algorithm and stoplist using mallet.token_regex=None and mallet.remove_stopwords=True . These configurations are provided for convenience in the cell below. You just have to uncomment them and run the cell. Note that using plain text files that just contain document contents (and not metadata) as your data source means that certain visualisation tools like Dfr-Browser and Topic Bubbles, which require metadata, will not be usable. Therefore, this method is not recommended. If you wish to generate these visualisations, it is best to use the import module to import your data into your project's json folder first.","title":"Importing Plain Text files"},{"location":"modules/topic-modeling/#import-data-to-mallet","text":"You can probably simply run this cell as is, and the import process will begin. It may take a long time if your collection is large. By default, the topic list you supplied in Setup MALLET will be used. However, if, for instance, you wish to import data for only models 25 and 50, you can also provide a topic list here by typing mallet.import_models([25, 50]) . This cell generates a MALLET command and uses it to call MALLET. If you run into a problem and you wish to see the MALLET command, create a new cell and run print(mallet.import_command) . Once the import process is complete, you are ready to begin training your models.","title":"Import Data to MALLET"},{"location":"modules/topic-modeling/#train-models","text":"As with the previous cell, you can probably simply run this cell as is. Likewise, if you do not wish to train the models specified in Setup MALLET , you can supply a list of models here by typing mallet.train_models([25, 50]) . When the training begins, MALLET gives continuous feedback with each iteration of the modelling process. By default, this feedback is hidden, and a progress bar indicates how close the model is to completion. You may wish to change this behaviour with one of the following settings: mallet.train_models(progress_bar=False) : Display a plain text progress indicator every 10%. mallet.train_models(capture_output=True) : Capture the output and display it only when training is complete. This is useful for job that takes a long time because it allows you to close the window. mallet.train_models(log_file='path_to_mallet_log.txt') : Save the output to a log file at the path specified. This is useful if you wish to save a record of MALLET's feedback. If you run into a problem, you can inspect the last MALLET command by creating a new cell and running print(mallet.train_command) .","title":"Train Models"},{"location":"modules/topic-modeling/#scale-topics","text":"This cell uses Multidimensional Scaling (MDS) to adjust the topic weights for use in visualisation tools such as Dfr-Browser, pyLDAvis, and Topic Bubbles. These modules will not work properly if you do not perform scaling by running this cell. The generated scaling information is stored as topic_scaled.csv in the model's directory. By default, scaling files will be generated for the models you configured in Setup MALLET above. If you wish to specify which models to scale in this cell, replace num_topics with a list of topic desired topic numbers (e.g. [50, 100] ) in the code below.","title":"Scale Topics"},{"location":"modules/topic-modeling/#module-structure","text":"topic_modeling \u2523 scripts \u2503 \u2523 scale_topics.py \u2503 \u2523 timer.py \u2503 \u2523 mallet.py \u2503 \u2523 prepare_mallet_import.py \u2503 \u2523 slow.py \u2503 \u2523 timer.py \u2503 \u2517 we1s_standard_stoplist.txt \u2523 model_topics.ipynb \u2523 README.md","title":"Module Structure"},{"location":"modules/utilities/","text":"Utilities About This Module This module is really a container for miscellaneous \"helper\" notebooks that can be used to interact with the data in the project. These notebooks should be more or less self-documenting, so there is no user guide here. There are currently two notebooks: clear_caches.ipynb : Empties or deletes files and folders and clears the output Jupyter notebooks. zip_folder.ipynb : Create a zip or tar.gz archive of any folder in the project. Module Structure utilities \u2523 export_project.ipynb \u2523 README.md \u2517 zip_folder.ipynb","title":"utilities"},{"location":"modules/utilities/#utilities","text":"","title":"Utilities"},{"location":"modules/utilities/#about-this-module","text":"This module is really a container for miscellaneous \"helper\" notebooks that can be used to interact with the data in the project. These notebooks should be more or less self-documenting, so there is no user guide here. There are currently two notebooks: clear_caches.ipynb : Empties or deletes files and folders and clears the output Jupyter notebooks. zip_folder.ipynb : Create a zip or tar.gz archive of any folder in the project.","title":"About This Module"},{"location":"modules/utilities/#module-structure","text":"utilities \u2523 export_project.ipynb \u2523 README.md \u2517 zip_folder.ipynb","title":"Module Structure"}]}